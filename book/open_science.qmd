# Open science practices {#sec-openscience}

## Learning objectives {#sec-learning-objectives-6}

In this chapter, you will learn the reasons for practicing open science and some of the basic methodological techniques that we can use to facilitate an open science workflow.

In this chapter you will learn how to:

-   structure a project
-   manage a project workflow
-   capture a session or machine state
-   use dynamic reporting
-   ensure data and code retention

<!-- https://www.nature.com/articles/sdata201618 -->

## What is Open Science and why?

Open Science is a way of doing research that emphasizes public and free accessibility, reusability, and transparency at every stage of the scientific process. Instead of keeping data, methods, and results behind paywalls or locked on personal computers, Open Science seeks to make them available for others to examine, test, and build upon. In brief:

> “Open data and content can be freely used, modified, and shared by anyone for any purpose.” ([opendefinition.org](https://opendefinition.org/))

At its core, Open Science is grounded in *reproducibility*: the idea that scientific results should be verifiable by others using the same methods and data. Open science is enabled by *reusability* (as was introduced in @sec-reproducible), ensuring that outputs such as datasets, software, and workflows can be adapted for new purposes, speeding up discovery across disciplines.

Concretely, Open Science includes practices including open data, open source software, open access publishing, and open workflows and methods (see below). Beyond methods, Open Science represents a cultural shift. It challenges the traditional “publish or perish” system, where competition and secrecy often slow progress, and instead encourages collaboration, accountability, and broader participation. Several aspects motivate the cultural shift towards Open Science:

- Enables accountability, trust and leads to robust science.

Access to data and code enables tests and acts as a precise documentation of how results were obtained. This also reflects a response to the reproducibility crisis that has affected scientific studies and entire scientific fields. 

- Creates efficient infrastructure.
 
Making data freely accessible through the web enables fully reproducible workflows.

- Enhances impact.

Making your data and code accessible and reusable enhances its uptake by the community. This makes your work valuable and visible -- you create impact.

- Serves the public and transcends discrimination.

Since much research is publicly funded, its outputs should be accessible to the public and anyone, disregarding access to resources and funding.

It is important to acknowledge the granularity of what it takes to do good science -- science that may not just be *reproducible*, but also *replicable*, *robust*, and *generalisable*. These descriptors can be understood as different combinations of the data and/or the analysis being identical between studies (or not). Open access data and sofware are the basis for a study to be reproducible. The analysis (software) may be reusable and can be applied to new data, making findings replicable. The same data may also be analysed differently. If findings are similar, they are robust. If a pattern or a relationship is identified consistently with multiple different combinations of data and analytic approaches, findings are generalisable. Within the context of this course, we focus primarily on the practical aspects for reproducible science, i.e., ensuring that given the same data and code, the results will be (practically) identical.

![The reproducibility matrix by The Turing Way.](./figures/reproducible-matrix-turing-way.jpg){#fig-repromatrix width=60% fig.align='center'}

### Open data

Open data refers to the practice of making research datasets available so that others can examine, validate, and reuse them. When properly shared, data becomes a resource for the entire research community, enabling new insights and reducing duplication of effort. Yet, in practice, researchers often perceive barriers to making data open. In some cases, researchers must handle sensitive or confidential data, which cannot be openly released without consent and restrictions to openness. Finally, there are strategic considerations—sharing data may feel risky in competitive environments.

Despite these challenges, expectations and the culture around open data are shifting. Many journals and funding agencies now require authors to provide data availability statements in published articles, and some make data deposition in recognized repositories a condition of publication. To guide these practices, the *FAIR* principles  -- that data should be Findable, Accessible, Interoperable, and Reusable -- have become a global standard. FAIR does not mean all data must be open, but it emphasizes that well-documented and properly archived data maximizes its potential for reuse while protecting sensitive information.

### Open source software

Software is often as central to research as data, yet historically it has been less visible in scholarly outputs. Open source software addresses this gap by making code freely accessible for anyone to use, examine, and extend. A typical project might start as a tool written for a specific study, but once shared and adapted for more general applications, it can evolve into a library that supports many projects across a community. Also specialized software, such as a model for simulating a physical system, can become widely useful if written in a non-proprietary coding language and accompanied by sufficient documentation.

The benefits of open source are significant. Publicly available code enables reproducibility by letting others see exactly how analyses were performed. It also reduces dependence on proprietary systems, which may lock research into costly or opaque environments. Over time, well-maintained open source projects can attract contributions from the wider community, enhancing functionality and improving reliability. Version control platforms like *GitHub* make this collaborative development possible by offering tools for *issue tracking*, *pull requests*, and *continuous integration testing*. When paired with clear *licensing*, open source software not only supports transparency but also lays the foundation fair crediting and for cumulative progress.

### Open access publishing

One of the most visible aspects of Open Science is open access publishing, which aims to make research articles freely available to everyone. The motivation is straightforward: since much research is publicly funded, the public should not face paywalls to access it. However, the reality of traditional publishing has created a crisis. Commercial publishers often charge universities and libraries high subscription fees, while researchers -- who generate content, review manuscripts, and edit journals without pay -- receive little in return. The public ends up paying twice: once to fund the research through salaries and grants, and again through subscription costs.

Open access offers alternative models. Some scholarly societies, such as the *European Geosciences Union* and the *American Geophysical Union*, have pioneered non-profit publishing routes where revenues sustain the scientific community rather than shareholders. *Preprint* servers also play an increasingly important role, allowing researchers to freely share findings rapidly before peer review.

Within formal publishing, two main routes to open access exist. *Gold open access* makes articles immediately free to read on the publisher’s site, often supported by article processing charges (APCs) paid by authors or their institutions. *Green open access* relies on authors self-archiving preprints or accepted manuscripts in institutional or subject repositories, where they remain freely available even if the journal version is behind a paywall. Together, these models are reshaping the landscape of scholarly communication toward greater openness.

### Open workflows and methods

Beyond data, software, and publications, Open Science also encompasses the entire research workflow. Open workflows mean that the processes behind a study -- from data collection protocols to analysis scripts -- are documented and shared in ways that others can follow. This makes research more transparent and easier to reuse. For example, maintaining project-oriented workflows where data, code, and documentation are stored systematically helps others (and one’s future self) understand the work. In @sec-reproducible, you learned what it takes to implement reusable, project-oriented workflows. 

Version control systems like Git allow researchers to capture the history of their analysis workflows, while platforms like GitHub provide mechanisms for collaboration, peer review of code, and continuous integration testing to ensure reliability. Coupled with practices like versioning and tagged releases, these tools enable researchers to share stable snapshots of their work that can be cited and archived. Open workflows are also supported by *data management plans*, which assign clear roles and responsibilities, define storage and backup strategies, and address long-term preservation of outputs. More on such practices below in @sec-practices-open-science

In essence, open workflows transform research into a living, transparent process rather than a black box that only produces final results. This shift helps ensure that scientific claims can be checked, methods can be reused, and future work can build more efficiently on what has already been done.

## Practices for Open Science {#sec-practices-open-science}

### Open source software







<!-- ## Tutorial -->

<!-- The scientific method relies on repeated testing of a hypothesis. When dealing with data and formal analysis, one can reduce this problem to the question: could an independent scientist attain the same results given the described methodology, data and code? -->

<!-- Although this seems trivial, this issue has vexed the scientific community. These days, many scientific publications are based on complex analyses with often large data sets. More so, methods in publications are often insufficiently detailed to really capture the scope of an analysis. Even from a purely technical point of view, the reproducibility crisis or the inability to reproduce experimental results, is a complex problem. This is further compounded by social aspects and incentives. In recent decades, scientific research has seen a steady increase in speed due to the digitization of many fields and the commodification of science. -->

<!-- Although digitization has opened up new research possibilities, its potential for facilitating, accelerating, and advancing science are often not fully made use of. Historically, research and its output in the form of data and code has been confined to academic journals, data and code has often not been made available, and data were not shared or behind pay-walls. This limits the impact of science also in the public domain. (In many ways, this is still the case today, in year 2023 as we write). Digitization has made research output better visible and accessible, but practical obstacles and weak standards often prevent it from uptake, re-use, and further development by the wider community. -->

<!-- Open and reproducible science is a movement to make scientific research (output) widely accessible to the larger public, increase research transparency and enabling robust and verifiable science. Open science aims to be as open as possible about the whole scientific process, and as closed as desirable (e.g. privacy or security reasons). -->



<!-- The basics of open science coding and data practices rely on a number of simple concepts. The sections below describe a selection of the most important ones. Sticking to these principles and tools will increase the reproducibility of your work greatly. -->

<!-- ### Project structure -->

<!-- Reproducible science relies on a number of key components. Data and code management and the tracking of required meta-data is the first step in an open science workflow.  -->

<!-- In @sec-reproducible, you learned how to setup an R project. An R project gathers all components of your analysis in a single directory. Although current computers make it easy to "find" your files and are largely file location-agnostic, this is not the case in many research environments. Projects grow quickly, and often, the number of files will flood a single directory. Therefore, files need a precise and structured location. This structure allows you to determine both the function and order of a workflow without reading any code. -->

<!-- It is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structuring a project in one folder also makes projects portable. All parts reside in one location making it easy to create a *git* project from this location (see @sec-codemgmt), or just copy the project to a new drive. -->

<!-- An example data structure for raw data processing is given below and we provide an [R project template](https://github.com/geco-bern/R_proj_template) to work from and adjust through our [lab GitHub profile](https://github.com/geco-bern). A full description on using the template is provided in @sec-codemgmt. -->

<!-- ``` bash -->
<!-- data-raw/ -->
<!-- ├─ raw_data_product/ -->
<!-- ├─ 00_download_raw_data.R -->
<!-- ├─ 01_process_raw_data.R -->
<!-- ``` -->

<!-- ### Managing workflows -->

<!-- Although some code is agnostic to the order of execution, many projects are effectively *workflows*, where the output of one routine is required for the successful execution of the next routine. -->

<!-- In order to make sure that your future self, or a collaborator, understands the order in which things should be executed, it is best to number scripts accordingly. This is the most basic approach to managing workflows. -->

<!-- In the example below, all statistics code is stored in the `statistics` folder in an overall `analysis` folder (which also includes code for figures). All statistical analyses are numbered to ensure that the output of a first analysis is available to the subsequent one. -->

<!-- ``` bash -->
<!-- analysis/ -->
<!-- ├─ statistics/ -->
<!-- │  ├─ 00_randomforest_model.R -->
<!-- │  ├─ 01_randomforest_tuning.R -->
<!-- ├─ figures/ -->
<!-- │  ├─ global_model_results_map.R -->
<!-- │  ├─ complex_process_visualization.R -->
<!-- ``` -->

<!-- The code-chunk above is a visualisation of a folder (aka. directory) structure on your computer. The lines and indents denote folder levels. In this example, you have a folder `analysis` which holds two more folders `statistics` and `figures`, and in both sub-folders, you have different `*.R` files (`*` is a so-called "wild-card" which is a placeholder for any text). Note that different people may use different symbols to visualise folder structures but generally, folder levels are shown with indents, and files are identifiable by their suffixes. -->


<!-- #### Automating and visualizing workflows with targets -->

<!-- To sidestep some of the manual management in R you can use a dedicated pipeline tool like the {targets} package in R. The package learns how your pipeline fits together, skips tasks that are already up-to-date, and runs only the necessary computation. {targets} can also visualize the progress of your workflow. -->

<!-- ![A targets visualized workflow by rOpenSci.](https://books.ropensci.org/targets/man/figures/tar_watch.png){#fig-targets width=60% fig.align='center'} -->

<!-- Due to the added complexity of the {targets} package, we won't include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples [here](https://books.ropensci.org/targets/walkthrough.html). -->


<!-- ### Capturing your session state -->


<!-- ### Readable reporting using Rmarkdown -->

<!-- Within Rstudio, you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for *reporting*, i.e., writing your final document and presenting your analysis results. A Rmarkdown document consists of a header that specifies document properties (whether it should be rendered as an html page, a docx file or a pdf), and the actual content. You have encountered RMarkdown already in @sec-rmarkdown. -->

<!-- ### Project structure -->

<!-- In R projects, all files can be referenced relative to the top-most path of the project. When opening `your_project.Rproj` in RStudio, you can load data that is located in a sub-directory of the project directory `./data/` by `read.table("./data/some_data.csv")`. The use of *relative paths* and consistent directory structures across projects, enables that projects can easily be ported across computers and code adopted across projects. -->

<!-- ``` bash -->
<!-- project/ -->
<!-- ├─ your_project.Rproj -->
<!-- ├─ vignettes/ -->
<!-- │  ├─ your_dynamic_document.Rmd -->
<!-- ├─ data/ -->
<!-- │  ├─ some_data.csv -->
<!-- ``` -->

<!-- But why not use absolute paths to begin with? Portability! When I would run your `\*.Rmd` file with an absolute path on my computer, it would not render as the file `some_data.csv` would then be located at: `/my_computer/project/data/some_data.csv` -->

<!-- #### Limitations of notebooks -->

<!-- The file referencing issue and the common use of Rmarkdown, and notebooks in general, as a one size fits all solution, containing all aspects from data cleaning to reporting, implies some limitations. RMarkdown documents mix two cognitive tasks, writing text content (i.e. reporting) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa. -->

<!-- If your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your RMarkdown file contains more text than code, it probably is easier to collaborate on a true word processing file (or cloud-based solution). Notebooks, such as RMarkdown, are most suitable for communicating implementations, demonstrating functions, and reporting reproducible results. They can also be used like lab notes. They are less suited for code development.  -->

### Data retention

Coding practices and documenting all moving parts in a coding workflow is only one practical aspect of open science. An additional component is long-term data and code retention and versioning.

In @sec-codemgmt, you will learn more about *git* for code management and collaboration. Several online make use of *git* for providing web-based collaboration functionalities and remote storage of your repositories. Examples are [GitHub](https://github.com), [GitLab](https://gitlab.com), [Codeberg](https://codeberg.org), or [Bitbucket](https://bitbucket.org/). However, their remote storage service should only be considered an aid for collaboration, and not a place to store code into perpetuity. Furthermore, these services mostly have a limit to how much data can be stored in a repository (mostly \~2 GB). For small projects, data can be included in the repository itself. For larger projects and for making larger datasets accessible, this won't be possible.

To ensure long-term storage of code and data, outside of commercial for profit services (e.g., Dropbox, Google Drive etc), it is best to rely on public permanent repositories, such as [Zenodo](https://zenodo.org/). Zenodo is an effort by the European commission, but accessible to all, to facilitate archiving of science projects of all nature (code and data) up to 50 GB. In addition, Zenodo provides a citable digital object identifier or DOI. This allows data and code, even if not formally published in a journal, to be cited. Other noteworthy open science storage options include [Dryad](https://datadryad.org/stash) and the [Center for Open Science](https://osf.io/). 

The broad-purpose permanent data repositories mentioned above are not edited and are therefore not ideal for data discovery. In contrast, edited data repositories often have a specific thematic scope and different repositories are established in different research communities. Below you find a list of widely used data repositories, generalist and others, that provide manual or automated download access to their data. Note that this list contains some example and is far from extensive.

| Data type                                                                 | Website                             | Description                                                                                   | Download                          |
|----------------------|---------------|-------------------------------|---------------|
| Copernicus Climate Data Store                                             | <https://cds.climate.copernicus.eu> | Freely available climate data (reanalysis as well as future projections)                      | API                               |
| Oak Ridge National Laboratories Digital Active Archive Center (ORNL DAAC) | <https://daac.ornl.gov/>            | Environmental data of varying sources, either remote sensing, field work and or re-analysis.  | multiple APIs or manual downloads |
| Land Processes Digital Active Archive Center (LP DAAC)                    | <https://lpdaac.usgs.gov/>          | Remote sensing (analysis ready) data products.                                                | Login walled automated downloads  |
| Environmental Data Initiative                                             | <https://edirepository.org/>        | Generalist data repository for study data, with a strong focus on biology.                    | Manual download                   |
| Dryad                                                                     | <https://datadryad.org>             | Generalist data repository for study data, with a strong focus on biology.                    | Manual downloads                  |
| Zenodo                                                                    | <https://zenodo.org/>               | Generalist data repository for study data.                                                    | Manual downloads                  |
| Eurostat                                                                  | <https://ec.europa.eu/eurostat>     | Generalist data repository for EU wide (demographic) data.                                    | Manual downloads                  |
| Swiss Open Government data                                                | <https://opendata.swiss/en/>        | Generalist data repository from the Swiss Federal statistics office.                          | API or manual downloads           |


## Exercises

### External data {-}

You inherit a project folder which contains the following files.

```
~/project/
├─ survey.xlsx
├─ xls conversion.csv
├─ xls conversion (copy 1).csv
├─ Model-test_1.R
├─ Model-test-final.R
├─ Plots.R
├─ Figure 1.png
├─ test.png
├─ Rplot01.png
├─ Report.Rmd
├─ Report.html
├─ my_functions.R
```

What are your steps to make this project more reproducible? Write down how and why you would organize your project.

### A new project {-}

What are the basic steps to create a reproducible workflow from a file management perspective? Create your own R project using these principles and provide details the on steps involved and why they matter.

The project should be a reproducible workflow:

-   Download and plot a MODIS land cover map for Belgium using skills you learned in @sec-datavariety.

-   Write a function to count the occurrences of land cover classes in the map as a formal function using skills you learned in @sec-datawrangling.

-   Create a plot of the land cover map, see @sec-datavis.

-   Write a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow.


### Tracking the state of your project {-}

-   Track the packages you use in the project you created using {renv}.

-   Install any additional library and update the state of your project.

-   Create a simple {targets} project using the above workflow

    -   Make changes to the API download routine.

    -   Rerun the targets project.
