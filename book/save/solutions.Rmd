
### Magic Trick

```{r}
magic_trick <- function(original_value) {

  # Check if the argument for original_value is a numeric value
  if (!is.numeric(original_value)) stop("Error: Input is not a numeric value.")
  
  # Check if the argument for original_value is an integer
  if (round(original_value) != original_value) stop("Error: Input is not an integer.")
  
  # Check if original value is between 1 and 20
  if (original_value < 1 | original_value > 20) stop("Error: Input is outside of 1 and 20.")
  
  # Define temporary variable
  tmp <- original_value + 1
  tmp <- tmp * 2
  tmp <- tmp + 4
  tmp <- tmp / 2
  final_value <- tmp - original_value
  
  cat("The original value was: ", original_value, "\n",
      "The final value is: ", final_value, "\n")
}
```

Let's try out some false input arguments to see, if our function correctly captures them and reports back what is wrong:

```{r eval = FALSE}
magic_trick("7")
```
```
    ## Error in magic_trick("7") : Error: Input is not a numeric value.
    ## 2. stop("Error: Input is not a numeric value.")
    ## 1. magic_trick("7")
```

```{r eval = FALSE}
magic_trick(7.5)
```
```
    ## Error in magic_trick(7.5) : Error: Input is not an integer.
    ## 2. stop("Error: Input is not an integer.")
    ## 1. magic_trick("7")
```

```{r eval = FALSE}
magic_trick(77)
```

```
    ## Error in magic_trick(77) : Error: Input is outside of 1 and 20.
    ## 2. stop("Error: Input is outside of 1 and 20.")
    ## 1. magic_trick(77)
```

And now let us test, whether the magic trick works:

```{r}
magic_trick(1)
magic_trick(7)
magic_trick(11)
magic_trick(20)
```

### Code Clean-up

```{r eval=FALSE}
variable <- 1
vector   <- c(1, 2, 3, 4)
is_true <- TRUE
list(a = 1 , 
     b = "This is some really bad code")
my_new_function(
  argument_1 = "value 1", 
  argument_2 = "value 2", 
  argument_3 = NA
)
```

-   Clean up the code chunk below following the {tidyverse} style guide. Run the statements in your console - do they work? If not, why?

```{r eval=FALSE}
anothervariable = c(1)
seq <- vector(1,2 3 ,4)
is_true <- T
list(a=1, b = This is some really bad code}
var <- 7
my_new_function(this_is_a_very_long_argument = "value 1", and_this_is_another_very_long_argument = "value 2", short_but_still_long_argument = NA )
```

## Chapter 02 - Primers

### Loops using `for` and `while`

```{r}
# 1a. for-loop to compute sum from 1 - 100
sum <- 0
for (i in seq(100)) sum <- sum + i # for-loop iterating from 1 to 100
print(sum)


# 1b. while-loop to compute sum from 1 - 100
loop_status <- TRUE
counter <- 0
sum <- 0
while (loop_status) { # while-loop is repeated as long as loop_status is true
  counter <- counter + 1
  sum <- sum + counter
  
  if (counter == 100) loop_status <- FALSE
}
print(sum)
```

### Add up all numbers that are multiples of 3 and 7

```{r}
sum <- 0
for (i in seq(100)) {
	if (i %% 3 == 0 && i %% 7 == 0 ) {
		sum <- sum + i
	}	
}	
print(paste0("The sum is of multiples of 3 and 7 within 1-100 is: ", sum))
```

### Define and interpolate vector

```{r}
# Using a for-loop to identify first and last position of NA values
vec <- array(NA,c(100)) # Initiate vector of length 100 with NAs
vec[1:25] <- 6          # Fill up first 25 elements of 'vec' with value 6. 
vec[66:100] <- -20      # Fill up elements 66:100 with value -20.
for (i in 2:99) {       # for-loop iterating from 2 to 99
  # NOTE: The reason we loop between 2 and 99 is because we need to compare
  # neighboring positions to find the transition from entries with number to 
  # entries with NA (i-1, i, i + 1). Positions 0 and 101 are not defined 
  # in our vector and thus lead to an error when trying to access.
  
  # Find last defined value 
  # Requires: Current position i is not NA but the next one is
  if (!is.na(vec[i]) && is.na(vec[i+1])) last_non_na <- i

  # Find first defined value
  # Requires: Previous position is NA but not the current one
  if (!is.na(vec[i]) && is.na(vec[i-1])) first_non_na <- i
}

# Get the increment that is needed for interpolation
last_value  <- vec[last_non_na] # Last non-NA value
first_value <- vec[first_non_na] # First non-NA value
delta <- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x

for (i in 2:length(vec)) { # If loop iterating from 2 to length of vec
  # If position is NA, replace it by previous value plus increment
  if (is.na(vec[i])) vec[i] <- vec[i-1] + delta
}

# Create plot
# Intrinsic function 'plot' requires the following arguments:
# 1st argument: vecor of x-values
# 2nd argument: vecor of y-values (must be of same length as x-value vector)
plot(seq(length(vec)), 
     vec,
     type = "l",
     main = "Interpolated vector using for-loop")


# Using a while-loop to identify first and last position of NA values
vec <- array(NA,c(100)) # Initiate vector of length 100 with NAs
vec[1:25] <- 6 # Fill up first 25 elements of 'vec' with value 6. 
vec[66:100] <- -20 # Fill up elements 66:100 with value -20.
last_non_na <- 1
is_defined <- TRUE

while (is_defined){ # Search last element defined before gap and store its position
	if (is.na(vec[last_non_na+1])){
		is_defined <- FALSE
	} else {
		last_non_na <- last_non_na+1	
	}
}

# One may omit the boolean variable 'defined' and directly write:
last_non_na <- 1
while (!is.na(vec[last_non_na+1])) last_non_na <- last_non_na + 1

# Search first element defined after gap and store its position
first_non_na <- last_non_na + 1
while (is.na(vec[first_non_na])) first_non_na <- first_non_na + 1	

# Get the increment that is needed for interpolation
last_value  <- vec[last_non_na] # Last non-NA value
first_value <- vec[first_non_na] # First non-NA value
delta <- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x

for (i in 2:length(vec)) { # If loop iterating from 2 to length of vec
  # If position is NA, replace it by previous value plus increment
  if (is.na(vec[i])) vec[i] <- vec[i-1] + delta
}

# Create plot
plot(seq(length(vec)), 
     vec,
     type = "l",
     main = "Interpolated vector using while-loop")
```

### Find the sum of the values per column




## Chapter 03 - Data wrangling

```{r include=FALSE}
# Load packages
library(tidyverse) 
library(lubridate)  # not part of the automatic load of tidyverse
```

<!-- ## Encoding elevation -->

<!-- ```{r} -->
<!-- base::load("data/siteinfo_fluxnet2015.rda")  # loads siteinfo_fluxnet2015 -->

<!-- siteinfo_fluxnet2015 |> -->
<!--   mutate(m_asl = elv, -->
<!--          m_asl = ifelse(m_asl < 0, paste"", m_asl)) # meters above sea level -->
<!-- ``` -->

### Tidyverse exercises
```{r message=FALSE}
# a.
dplyr::starwars |> dplyr::filter(skin_color == "pale", homeworld == "Naboo" | homeworld == "Ryloth") |> nrow()   
# b.
dplyr::starwars |> unnest_wider(starships) |> filter_at(vars(contains("...")), all_vars(!is.na(.)))  

# c.
dplyr::starwars |> unnest(starships) |> filter(films == "Return of the Jedi") |> top_n(-1, height) |> select(name, starships) 
```

### Aggregation of data in tidy style

```{r}
# Load and wrangle the hhdf dataset
hhdf <- read_csv("data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv") |>
  dplyr::select(
    starts_with("TIMESTAMP"),
    ends_with("_F"),
    GPP_NT_VUT_REF,
    NEE_VUT_REF_QC,
    starts_with("SWC_F_MDS_"),
    -contains("JSB"),
    NIGHT
  )

# Get different statistics for VPD_F
hhdf |>  
  mutate(date_time = lubridate::ymd_hm(TIMESTAMP_START),
         date      = lubridate::date(date_time)) |>
  group_by(date) |> 
  summarise(VPD_F_mean   = mean(VPD_F),
            VPD_F_median = median(VPD_F),
            VPD_F_min    = min(VPD_F),
            VPD_F_max    = max(VPD_F),
            VPD_F_sd     = sd(VPD_F)) |> 
  head()
```


### Temporal pattern of gap-filled data
```{r}
# Create the two datasets
hhdf_low_quality  <- hhdf |> filter(NEE_VUT_REF_QC != 0)
hhdf_high_quality <- hhdf |> filter(NEE_VUT_REF_QC == 0)

# Calculate means
mean_full_df      <- mean(hhdf$GPP_NT_VUT_REF)
mean_low_quality  <- mean(hhdf_low_quality$GPP_NT_VUT_REF)
mean_high_quality <- mean(hhdf_high_quality$GPP_NT_VUT_REF)

cat("\n GPP mean values of: full --- low quality --- high quality datasets \n",
    mean_full_df, "---", mean_low_quality, "---", mean_high_quality)

# Relationship between data quality and time
# Save hour of day (hod) in separate variable
hhdf |>
  
  # Turn numeric time into time-object
  mutate(date = lubridate::ymd_hm(TIMESTAMP_START),
         hod  = lubridate::hour(date)) |> 
  dplyr::group_by(hod) |>
  nest() |>
  mutate(QC_meas = purrr::map_dbl(data, ~. |> dplyr::filter(NEE_VUT_REF_QC == 0) |> nrow()),
         QC_fill = purrr::map_dbl(data, ~. |> dplyr::filter(NEE_VUT_REF_QC != 0) |> nrow())) |>
  pivot_longer(cols = c("QC_meas", "QC_fill"),
               names_to = "qc_flag",
               values_to = "qc_value") |>
  mutate(qc_flag = as.factor(qc_flag)) |>
  ggplot() +
  aes(x = hod, y = qc_value) +
  geom_line() +
  geom_col() +
  facet_wrap(~qc_flag) +
  labs(title = "Temporal patterns of gap-filled GPP data",
       subtitle = "Gap-filled data applies more often to night-time measurements!",
       x = "Hour of Day",
       y = "Count of quality flags")
```

## Chapter 04 - Data visualisation

### Identifying Outliers

```{r}
# 1. Identify Outliers
# Load packages
library(tidyverse) 
library(lubridate)  # not part of the automatic load of tidyverse
# read half-hourly data
hhdf <- read_csv("data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv") |> 
  
  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 

  # interpret timestamp as a date-time object
  mutate_at(vars(starts_with("TIMESTAMP_")), ymd_hm)

# aggregate to daily
ddf <- hhdf |> 
  
    # create a date-only object
    mutate(date = as_date(TIMESTAMP_START)) |> 
  
    # aggregate
    group_by(date) |> 
    summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE),
              PPFD_IN = mean(PPFD_IN, na.rm = TRUE),
              .groups = 'drop' # not mandatory
    )

# fit linear regression
linmod <- lm(GPP_NT_VUT_REF ~ PPFD_IN, data = ddf)

# get box plot statistics for determining "outlying" points
out_boxplot <- boxplot.stats(linmod$residuals)

# record the row numbers of outlying points based on the output list element 'out'
# row numbers are the names of elements in out_boxplot$out, provided as strings. 
# convert them to integers.
idx_outlying <- names(out_boxplot$out) |> as.integer()

# 2. Remove Outliers
# In base-R, this could be done as:
ddf$GPP_NT_VUT_REF_clean <- ddf$GPP_NT_VUT_REF
ddf$GPP_NT_VUT_REF_clean[idx_outlying] <- NA

# In tidyverse style:
ddf <- ddf |> 
  mutate(rownumber = row_number()) |>    # could also do: mutate(rownumber = 1:nrow(.))
  mutate(GPP_NT_VUT_REF_clean = ifelse(rownumber %in% idx_outlying, NA, GPP_NT_VUT_REF))

## 3. Create scatterplot
ddf |> 
  ggplot(aes(x = PPFD_IN)) + 
  geom_point(aes(y = GPP_NT_VUT_REF), color = "red") +  # first, plot all points including outliers in one color
  geom_point(aes(y = GPP_NT_VUT_REF_clean), color = "black")  # then overplot cleaned data in black so that remaining red points are outliers
```


### Diurnal and seasonal cycles
```{r}
## a. Half-hourly dataset
hhdf_meanseason <- 
  hhdf |> 
  mutate(hour_day = hour(TIMESTAMP_START),
         day_year = yday(TIMESTAMP_START)) |> 
  group_by(hour_day, day_year) |> 
  summarise(gpp = mean(GPP_NT_VUT_REF, na.rm = TRUE))

## b. Raster plot
hhdf_meanseason |> 
  ggplot(aes(x = hour_day, y = day_year, fill = gpp)) +
  geom_raster()

## c. Make raster plot publishable - This is up to your judgement. Below is an example. 
library(viridisLite)
hhdf_meanseason |> 
  
  # as above
  ggplot(aes(x = hour_day, y = day_year, fill = gpp)) +
  geom_raster() +
  
  # use a color scale that works also for color-blind people
  scale_fill_viridis_c(option = "magma") +
  
  ## change theme (axes style)
  theme_classic() +
  
  # adjust the aspect ratio of the plotting region
  coord_fixed(ratio=0.2) +
  
  # labels of each mapping axis, \n is a line break
  labs(x = "Hour of day", 
       y = "Day of year", 
       fill = expression(paste("GPP \ngC m"^-2, "s"^-1))) +
  
  # avoid adding distance from the lowest values to the axes
  scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0))
```

## Chapter 05 - Data variety

### Files and file formats

#### Reading and writing human readable files {-}

The below code shows how to read in the different demo data sets (CSV files). You will note that they all need separate settings, and that a given file extension isn't necessarily a reflection of the content the file. Inspection of your read in data is therefore key.

```{r}
# read in the first demo
demo_01 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_1.csv",
  sep = ",",
  header = TRUE
  )

# read in second demo
demo_02 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_2.csv",
  sep = " ",
  header = TRUE
  )

demo_03 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_3.csv",
  sep = ";",
  comment.char = "|",
  header = TRUE,
  )
```

All the demo data sets are equal, except for their formatting. We can test if the content is identical by using the `identical()` function in R.

```{r}
# compare 1 with 2
identical(demo_01, demo_02)

# compare 2 with 3
identical(demo_02, demo_03)

# Given transitive properties, demo_01 is identical to demo_03
```

You can combine the three datasets using the {dplyr} `bind_rows()` function.
```{r}
# combining all demo datasets
demo_all <- dplyr::bind_rows(demo_01, demo_02, demo_03)

# writing the data to a temporary CSV file
write.table(
  demo_all, 
  file = file.path(tempdir(), "tmp_csv_file.csv"),
  col.names = TRUE,
  row.names = FALSE,
  sep = ","
)

# or...
write.csv(
  demo_all,
  file.path(tempdir(), "tmp_csv_file.csv"),
  row.names = FALSE
)

# read in the previous CSV file
demo_all_new <-read.table(
  file.path(tempdir(), "tmp_csv_file.csv"),
  header = TRUE,
  sep = ","
)

# writing the data to a JSON file
jsonlite::write_json(demo_all_new, path = "./my_json_file.json")
```

#### Reading and writing binary files {-}

1. It is a NetCDF file
2. the {terra} library (see Table xyz)
3. Temperature data for Switzerland at noon on 2021-01-01

```{r eval=FALSE}
# read unknown netcdf file using the {terra} library
library(terra)
unknown_netcdf <- terra::rast("https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc")

# print the meta-data by calling the variable
unknown_netcdf

# visually plot the data
terra::plot(unknown_netcdf)
```

4. 

```{r eval=FALSE}
# write the data as a geotiff (other options are possible as well in writeRaster)
terra::writeRaster(
  unknown_netcdf,
  filename = "./test.tif",
  overwrite = TRUE
  )
```

```{r eval=FALSE}
# read unknown tif file using the {terra} library
library(terra)
unknown_tif <- terra::rast("https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.tif")

# print the meta-data by calling the variable
unknown_tif

# visually plot the data
terra::plot(unknown_tif)

# Are they exactly the same
terra::plot(unknown_tif - unknown_netcdf)

# or...
identical(unknown_netcdf, unknown_tif)
```

1. Looks similar to the NetCDF data, however one pixel is different as shown by taking the difference between both datasets.

## Chapter 06 - Open Science

### External data

The project data is stored in one folder without folders to sort data from code to give it structure.

The project can be re-organized using a simple project structure as such:

``` bash
~/project/
├─ data/
     ├─ 00_convert_data.R
     ├─ survey.xlsx # the original
     ├─ survey.csv # from (xls conversion (copy 1).csv)
├─ R/
     ├─ my_functions.R
├─ analysis/
     ├─ 00_model_fits.R # from Model-test-final.R
     ├─ 01_model_plots.R # from Plots.R
├─ vignettes/
     ├─ Report.Rmd
├─ manuscript/
     ├─ Report.html
     ├─ Figure 1.png
```

Note that duplicate files are removed, code to cleanup data is numbered and stored with the data, functions which are accessible for analysis are stored in the R folder, Rmarkdown files are stored in the vignettes folder and the results of the full analysis is stored in a manuscript folder. Some variations on naming is possible.

### A new project



## Chapter 07 - Code management

There are no exercises that require solutions.

## Chapter 08 - Regression and classification

### Warm-up Exercises
#### Loops and Ifs

```{r}
A <- matrix(c(6, 7, 3, NA, 15, 6, 7, 
              8, 9, 12, 6, 11, NA, 3, 
              9, 4, 7, 3, 21, NA, 6, 
              7, 19, 6, NA, 15, 8, 10),
            nrow = 4, byrow = TRUE)
B <- c(8, 4, 12, 9, 15, 6)

summ <- data.frame()

for (i in 1:nrow(A)){
  
  for (j in 1:ncol(A)){
    
    if (is.na(A[i,j])){
      A[i,j] <- max(B)
    }
  }
  
  B <- B[-which(B == max(B))] # update the B vector removing the biggest values
  summ <- bind_rows(summ, data.frame(row_number = i, avg = mean(B)))
}
  
summ
```

#### Stepwise regression

```{r message=FALSE, warning=FALSE}
vec_files <- list.files("data", pattern = "_FLUXNET2015_FULLSET_DD_", full.names = TRUE)
list_df <- purrr:::map(as.list(vec_files), ~readr::read_csv(.))
names(list_df) <- vec_files

clean_fluxnet_dd <- function(df){
    
    df |>
        
        ## select only the variables we're interested in
        select(starts_with("TIMESTAMP"),
               ends_with("_F"),
               ends_with("_F_MDS"),
               # ends_with("_ERA"),
               starts_with("_SWC_F_MDS"),
               CO2_F_MDS,
               PPFD_IN,
               GPP_NT_VUT_REF,
               NEE_VUT_REF_QC,
               USTAR,
               -starts_with("G_"),
               -starts_with("LE_"),
               -starts_with("H_"),
               -contains("JSB")
        ) |>
        
        ## convert to a nice date object
        mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>
        
        ## not setting heavily gapfilled data to zero
        
        ## set all -9999 to NA
        dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
    
        # drop NAs
        drop_na() |> 
    
        ## filter bad data (at least 80% must be measured or good quality gapfilled)
        mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF)) |> 
        
        ## drop QC variables (no longer needed), except NEE_VUT_REF_QC
        select(-ends_with("_QC"))
    
}

df <- purrr::map(list_df, ~clean_fluxnet_dd(.)) |> 
    dplyr::bind_rows(.id = "siteid") |> 
    dplyr::mutate(siteid = str_sub(siteid, start = 10, end = 15))

readr::write_csv(df, "data/df_for_stepwise_regression.csv")
```


```{r}
## specify target variable (as above)
target <- 'GPP_NT_VUT_REF'

## determine predictors as all except date, siteid and target
preds <- df |> 
  dplyr::select(-target, -TIMESTAMP, -siteid) |> 
  names()

# This is the vector of candidate predictors to be added in the model. To begin with, consider all as candidates.
preds_candidate <- preds 

# predictors retained in the model from the previous step. To begin with, is empty.
preds_retained <- c()

## work with lists as much as possible (more flexible!)
df_metrics <- data.frame()

## outer loop for k predictors
for (k_index in 1:length(preds)){
  
  # rsq_candidates <- c()
  df_rsq_candidates <- data.frame()
  linmod_candidates <- list()
  
  ## inner loop for single additional predictor
  for (ipred in preds_candidate){
    
    # variable vector (new variable + retained variables) used in regression
    pred_add <- c(preds_retained, ipred)
    
    # define formulate with newly-added predictor
    forml  <- as.formula(paste( target, '~', paste(pred_add, collapse = '+')))
    
    # fit linear model
    fit_lin <- lm(forml, data = df)
    
    # add model object to list, and name the element according to the added variable
    linmod_candidates[[ ipred ]] <- fit_lin
    
    # record metrics for all candidates
    rsq <- summary(fit_lin)[["r.squared"]]
    df_rsq_candidates <- bind_rows(df_rsq_candidates, data.frame(pred = ipred, rsq = rsq))  # when storing R2 in a data frame
    # rsq_candidates <- c(rsq_candidates,  rsq)  # when storing R2 as a vector
  }
  
  ## get name of candidate predictor that achieved the highest R2.
  pred_add <- 
    df_rsq_candidates |>  # when storing R2 in a data frame
    arrange(desc(rsq)) |> 
    slice(1) |> 
    pull(pred) |> 
    as.character()
  
  # pred_add <- preds_candidate[ which.max(rsq_candidates) ]   # when storing R2 as a vector
  
  ## add best predictors to retained predictors 
  preds_retained <- c(preds_retained, pred_add)
  
  # record AIC and BIC and adjusted-R2 of the respective model
  df_metrics <- df_metrics |> 
    bind_rows(
      data.frame( pred = pred_add,
                  rsq = summary(linmod_candidates[[ pred_add ]])[["r.squared"]],
                  adj_rsq = summary(linmod_candidates[[ pred_add ]])[["adj.r.squared"]],
                  aic = AIC(linmod_candidates[[ pred_add ]]),
                  bic = BIC(linmod_candidates[[ pred_add ]])
      )
    )
  

  # remove the selected variable from the candidate variable list
  preds_candidate <- preds_candidate[-which(preds_candidate == pred_add)]
  # preds_candidate <- setdiff(preds_candidate,pred_add)  # alternative
  
  # Printing information on model development
  cat("\n Next Best Model: GPP_NT_VUT_REF ~ ", paste(preds_retained, collapse = " + "))
}

data.frame(df_metrics$pred) # order in which variables enter the model

df_metrics |> 
  arrange(desc(rsq)) |> 
  knitr::kable()
```

```{r}
library(ggplot2)

df_metrics$pred <-  factor(df_metrics$pred, levels = df_metrics$pred)

ggplot() +
  geom_point(data = df_metrics, aes(x = pred, y = rsq)) +
  geom_point(data = filter(df_metrics, rsq == max(rsq)), aes(x = pred, y = rsq), color = "red") +
  labs(title = expression(italic(R)^2)) + 
  coord_flip()

ggplot() +
  geom_point(data = df_metrics, aes(x = pred, y = adj_rsq)) +
  geom_point(data = filter(df_metrics, adj_rsq == max(adj_rsq)), aes(x = pred, y = adj_rsq), color = "red") +
  labs(title = expression(paste("Adjusted-", italic(R)^2))) + 
  coord_flip()

ggplot() +
  geom_point(data = df_metrics, aes(x = pred, y = aic)) +
  geom_point(data = filter(df_metrics, aic == min(aic)), aes(x = pred, y = aic), color = "red") +
  labs(title = "AIC")+ 
  coord_flip()

ggplot() +
  geom_point(data = df_metrics, aes(x = pred, y = bic)) +
  geom_point(data = filter(df_metrics, bic == min(bic)), aes(x = pred, y = bic), color = "red") +
  labs(title = "BIC")+ 
  coord_flip()
```

#### Finding best predictor

```{r}
df <- read_csv("data/df_for_stepwise_regression.csv")

## specify target variable
target <- 'GPP_NT_VUT_REF'

## determine predictors as all except site ID, timestamp, and the target (should be 14)
preds <- df |> 
  dplyr::select(-target, -siteid, -TIMESTAMP) |> 
  names()

## initialise an empty data frame (necessary, because otherwise we cannot use bind_rows() below)
df_rsq <- data.frame()
# rsq_list <- c()  # alternative for vector

for (var in preds){
  
  ## create formula dynamically
  forml <- as.formula(paste(target, "~", var))
  
  ## fit linear model
  fit_lin <- lm(forml, data = df)
  
  ## extract R2 from linear model
  rsq <- summary(fit_lin)[["r.squared"]]
  
  ## add a row to the data frame that holds the results
  df_rsq <- bind_rows(df_rsq, data.frame(pred = var, rsq = rsq))
  
  # rsq_list <- c(rsq_list,rsq)  # alternative with vector
}

## print a table arrange by best rsq at the top
df_rsq |> arrange(-rsq) |> knitr::kable()

## alternative: determine the first variable to enter into our model
# preds[which.max(rsq_list)]

## use the data frame that holds the results for plotting
df_rsq |> 
  ggplot(aes(x = reorder(pred, rsq), y = rsq)) +
  geom_bar(stat = "identity") + 
  labs(y = expression(italic(R)^2), x = "Variable") +
  coord_flip()
```

## Chapter 09 - Supervised ML I

There are no exercises with solutions for this Chapter.

## Chapter 10 - Supervised ML II

### Implementing k-fold cross-validation by hand

## Chapter 11 - Random Forests

### Tuning hyperparameters of a Random Forest model