[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Geodata Science",
    "section": "",
    "text": "Preface\nThe target of this book are people interested in applying data science methods for research. Methods, example data sets, and prediction challenges are chosen to make the book most relatable to scientists and students in Geography and Environmental Sciences. No prior knowledge of coding is required. Respective essentials are briefly introduced as primers. The focus of this book is not on the theoretical basis of the methods. Other “classical” statistics courses serve this purpose. Instead, this book introduces essential concepts, methods, and tools for applied data science in Geography and Environmental Sciences with an emphasis on covering a wide breadth. It is written with a hands-on approach using the R programming language and should enable an intuitive understanding of concepts with only a minimal reliance on mathematical language. Worked examples are provided for typical steps of data science applications in Geography and Environmental Sciences. The aim of this book is to teach the diverse set of skills needed as a basis for data-intensive research in academia and outside.\nWe also use this book as a reference and on-boarding resource for group members of Geocomputation and Earth Observation (GECO), at the Institute of Geography, University of Bern.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Applied Geodata Science",
    "section": "About this book",
    "text": "About this book\nThis book serves as the basis for the series of courses in Applied Geodata Science, taught at the Institute of Geography, University of Bern. The starting point of this book were the tutorials edited by Benjamin Stocker, Loïc Pellissier, and Joshua Payne for the course Environmental Systems Data Science (D-USYS, ETH Zürich). The present book was written as a collaborative effort led by Benjamin Stocker, with contributions by Pepa Arán and Koen Hufkens, and exercises by Pascal Schneider.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Applied Geodata Science",
    "section": "Links",
    "text": "Links\nBrowse the source code\nReport an issue",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Geodata Science",
    "section": "License",
    "text": "License\nImages and other materials used here were made available under non-restrictive licenses. Original sources are attributed. Content without attribution is our own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing or other, please contact us or report an issue. Any feedback, positive or negative, is welcome.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-cite-this-book",
    "href": "index.html#how-to-cite-this-book",
    "title": "Applied Geodata Science",
    "section": "How to cite this book",
    "text": "How to cite this book\nBenjamin Stocker, Koen Hufkens, Pepa Arán, & Pascal Schneider. (2023). Applied Geodata Science (v1.0). Zenodo.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Applied Geodata Science?\nData science is interdisciplinary by nature. It sits at the intersection between domain expertise, Statistics and Mathematics knowledge, and coding skills. Data science generates new insights for applications in different fields by combining these three realms (Fig. Figure 1.1). Combining only two of the three realms falls short of what data science is (Conway, 2013).\nDealing with data requires coding (but not a degree in computer science). Coding skills are essential for file and data manipulation and for thinking algorithmically.\nBasic knowledge in Statistics and Mathematics are needed for extracting insights from data and for applying appropriate statistical methods. An overview of methods, a general familiarity, and an intuitive understanding of the basics are more important for most data science projects than having a PhD in Statistics.\nStatistics plus data yields machine learning, but not “data science”. In data science, questions and hypotheses are motivated by the scientific endeavor in different domains or by applications in the public or private sectors. To emphasize the distinctively applied and domain-oriented approach to data science of this course, we call it Applied Geodata Science.\nOf course, empirical research has always relied on data. The essential ingredient of a course in (Applied Geo-) data science is that it emphasizes the methodological aspects that are unique and critical for data-intensive research in Geography and Environmental Sciences, and for putting Open Science into practice.\nThis course is also supposed to teach you how to stay out of the “danger zone” - where data is handled and models are fitted with a blind eye to fundamental assumptions and relations. The aim of data science projects is to yield credible (“trustworthy”) and robust results.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-applied-geodata-science",
    "href": "intro.html#what-is-applied-geodata-science",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: The Venn diagram of data science. Adapted from Conway, 2013.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-data-science-workflow",
    "href": "intro.html#the-data-science-workflow",
    "title": "1  Introduction",
    "section": "1.2 The data science workflow",
    "text": "1.2 The data science workflow\nThe red thread of this course is the data science workflow (Fig. Figure 1.2). Applied (geo-) data science projects typically start with research questions and hypotheses, and some data at hand, and (ideally) end with an answer to the research questions and the communication of results in textual, visual, and reproducible forms. What lies in between is not a linear process, but a cycle. One has to “understand” the data in order to identify appropriate analyses for answering the research questions. Before we’ve visualized the data, we don’t know how to transform it. And before we’ve modeled it, we don’t know the most appropriate visualization. In practice, we approach answers to our research questions gradually, through repeated cycles of exploratory data analysis - repeated cycles of transforming the data, visualizing it, and modelling relationships. More often than not, the exploratory data analysis generates insights about missing pieces in the data puzzle that we’re trying to solve. In such cases, the data collection and modelling task may have to be re-defined (dashed line in Fig. Figure 1.2), and the exploratory data analysis cycle re-initiated.\n\n\n\n\n\n\nFigure 1.2: The data science workflow. Figure adapted from: Wickham and Grolemund R for Data Science\n\n\n\nAs we work our way through repeated cycles of exploratory data analysis, we take decisions based on our data analysis, modelling, and visualizations. And we write code. The final conclusions we draw, the answers to research questions we find, and the results we communicate rest on the combination of all steps of our data processing, analysis, and visualization. Simply put, it rests on the reproducibility (and legibility) of our code (encapsulated by ‘Program’ in Fig. Figure 1.2).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-now",
    "href": "intro.html#why-now",
    "title": "1  Introduction",
    "section": "1.3 Why now?",
    "text": "1.3 Why now?\nThree general developments set the stage for this course. First, Geography and Environmental Sciences (as many other realms of today’s world) have entered a data-rich era (Chapters Chapter 6). Second, machine learning algorithms have revolutionized the way we can extract information from large volumes of data (this Chapter and Chapters Chapter 11 - Chapter 12). Third, Open Science principles (Chapter Chapter 7) - essential for inclusive research, boundless progress, and for diffusing science to society - are becoming a prerequisite for getting research funded and published. The skill set required to make use of the potentials of a data-rich world is diverse and is often not taught as part of the curriculum in the natural sciences (as of year 2023). This course fills this open space.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#a-new-modelling-paradigm",
    "href": "intro.html#a-new-modelling-paradigm",
    "title": "1  Introduction",
    "section": "1.4 A new modelling paradigm",
    "text": "1.4 A new modelling paradigm\nWhat is ‘modelling’? Models are an essential part of the scientific endeavor. They are used for describing the world, explaining observed phenomena, and for making predictions that can be tested with data. Models are thus a device for translating hypotheses of how the world operates into a form that can be confronted with how the world is observed.\nModels can be more or less explicit and more or less quantitative. Models can come in the form of vague mental notions that underpin our view of the world and our interpretation of observations. Towards the more specific end of this spectrum, models can be visualizations. For example a visualization of how elements in a system are connected. At the arguably most explicit and quantitative end of the spectrum are models that rely on mathematical descriptions of how elements of a system are connected and how processes operate. Examples of such models include General Circulation Models of the climate system or models used for Numerical Weather Prediction. Such models are often referred to as mechanistic models.\nA further distinction within mechanistic models can be made between dynamic models that describe a temporal evolution of a system (e.g., the dynamics of the atmosphere and the ocean in a General Circulation Model) and “static” models (e.g., a model for estimating the power generation of a solar photovoltaics station). In a dynamic model, we need to specify an initial state and the model (in many cases given additional inputs) predicts the evolution of the system from that. In a static model, the prediction can be described as a function of a set of inputs, without temporal dependencies between the inputs and the model prediction.\nOften, mechanistic and empirical models (or, here used as synonym, statistical models) are distinguished. Empirical models can be viewed as somewhere closer towards the less explicit end of the spectrum described above. In mechanistic models, the mathematical descriptions of relationships are informed by theory or by independently determined relationships (e.g., laboratory measurements of metabolic rates of an enzyme). In contrast, empirical models rely on no, or only a very limited amount of a priori knowledge that is built into the model formulation. However, it should be noted that mechanistic models often also rely on empirical or statistical descriptions for individual components (e.g., the parametrisation of convection in a climate model), and statistical models may, in some cases, also be viewed as a representation of mechanisms that reflects our theoretical understanding. For example, depending on whether a relationship between two variables is linear or saturating by nature, we would chose a different structure of an empirical model. An specific example is the light use efficiency model (Monteith, 1972) that linearly relates vegetation productivity to the amount of absorbed solar radiation. It simply has the form of a bivariate linear regression model. Vice-versa, traditional statistical models also rely on assumptions regarding the data generating process(es) and the resulting distribution of the data.\nSupervised machine learning models can be regarded as empirical models that are even more “assumption free” than traditional statistical models. In contrast to mechanistic models where rules and hypotheses are explicitly and mathematically encoded, and in contrast to statistical models where assumptions of the data distribution are made for specifying the model, machine learning approaches modelling from the flip side: from the data to the insight (Breiman, 2001). Rules are not encoded by a human, but discovered by the machine. Machine learning models learn from patterns in the data for making new predictions, rather than relying on theory and a priori knowledge of the system. In that sense, machine learning follows a new modelling paradigm. The learning aspect in machine learning refers to the automatic search process and the guidance of the model fitting by some feedback signal (loss function) that are employed in machine learning algorithms (see also Chapter Chapter 11).\nThe aspect of “patterns in the data” is key here. Often, these patterns are fuzzy. Rule-based algorithms have a limited capacity for dealing with such problems. Symbolic artificial intelligence is based on rules and underlies, for example, a computer playing chess (Chollet & Allaire, 2018). However, where rules cannot be encoded from the outset, symbolic artificial intelligence has reached its limits. A breakthrough in learning from fuzzy patterns in the data has been enabled by deep learning. Through multiple layers of abstraction of the data, deep learning models identify underlying, abstract, relationships and use them for prediction. Deep learning has been extremely successful in solving problems, e.g., in image classification, speech recognition, or language translation.\nHowever, the abstraction comes at the cost of interpretability. Deep learning models and machine learning models in general are used with an emphasis on prediction and have seen particularly wide adoption in fields where a false prediction has acceptable consequences (An inappropriate book recommendation based on your previous purchases is not grave.) (Knüsel et al., 2019). The model itself remains a black box and its utility for hypothesis testing is limited. This challenge has spurred the field of interpretable machine learning, where solutions are sought for uncovering the black box and probe the model for its trustworthiness.\nChapters Chapter 9 up to Chapter 12 lead into the world of machine learning and introduce the essential steps of the modelling workflow without delving into deep learning. Together with its preceeding chapters, this completes the toolbox required for making the first data scientific steps for applications in Geography and Environmental Sciences. This may be only just the beginning…",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#reading-and-link-collection",
    "href": "intro.html#reading-and-link-collection",
    "title": "1  Introduction",
    "section": "1.5 Reading and link collection",
    "text": "1.5 Reading and link collection\nFoundations\n\nLeo Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author).” Statist. Sci. 16 (3) 199 - 231, 2001. https://doi.org/10.1214/ss/1009213726\n\nA paper describing the paradigm shift in statistical modelling - from traditional approaches to machine learning. Written in accessible language by the inventor of the Random Forest algorithm.\n\nMarquet, P. A., Allen, A. P., Brown, J. H., Dunne, J. A., Enquist, B. J., Gillooly, J. F., Gowaty, P. A., Green, J. L., Harte, J., Hubbell, S. P., O’Dwyer, J., Okie, J. G., Ostling, A., Ritchie, M., Storch, D., & West, G. B.: On Theory in Ecology. BioScience, 64(8), 701–710, 2014. https://doi.org/10.1093/biosci/biu098\n\nA key statement of this paper summarises its content: “Some have suggested that theories are irrelevant in the big data era—that correlations are sufficient to build a vigorous science […]. We disagree.”\n\n\nData wrangling\n\nHadley Wickham and Garrett Grolemund: “R for Data Science”, https://r4ds.had.co.nz/\n\nA comprehensive resource for data analysis (and visualisation) using R tidyverse. Covers contents of Chapters Chapter 2, Chapter 3, and Chapter 4 - but in more depth.\n\nMax Kuhn and Kjell Johnson: “Feature Engineering and Selection: A Practical Approach for Predictive Models”, http://www.feat.engineering/\n\nA practical guide to all steps of the data science workflow, with particularly valuable chapters on data wrangling and feature engineering. This free online book is not accompanied by code and is thus useful for any data science practitioner, irrespective of their preferred programming language.\n\n\nData visualisation\n\nClaus O. Wilke: “Fundamentals of Data Visualization”, https://clauswilke.com/dataviz/\n\nA comprehensive resource for data visualisation - not specific to any programming language, but specific about the grammar of graphics. Covers concepts of Chapter Chapter 5 - but in more depth. The go-to resource for the implementation of data visualisation using the {ggplot2} R library is Wickham and Grolemund (see above).\n\nCrameri, F., Shephard, G.E. & Heron, P.J. The misuse of colour in science communication. Nature Communications 11, 5444 (2020). https://doi.org/10.1038/s41467-020-19160-7\n\nA systematic approach to why some color scales are better than others.\n\nhttps://exts.ggplot2.tidyverse.org/gallery/\n\nAn overview of {ggplot2} extensions for special visualisations and solutions for publication-ready graphs.\n\n\nMachine learning\n\nBradley Boehmke and Brandon Greenwell: “Hands-On Machine Learning with R”, https://bradleyboehmke.github.io/HOML/\n\nA great entry point for machine learning in R. It demonstrates concepts and a range of algorithms of varying complexity - from linear regression to Random Forest - with examples in R. This book served as an inspiration and starting point for the (in some ways reduced) contents of this course, covered in Chapters Chapter 9 - Chapter 12.\n\nPichler, M., & Hartig, F. (2023). Machine learning and deep learning—A review for ecologists. Methods in Ecology and Evolution, 00, 1– 23. https://doi.org/10.1111/2041-210X.14061\n\nA review paper that provides an instructive overview of machine learning and deep learning for a general readership (not just scientists in Ecology and Evolution). The paper explains also the fundamental assumptions of different methodological approaches and provides an intuitive understanding of the sometimes surprising power of machine learning algorithms.\n\nChollet & Allaire “Deep learning with R”, Manning Publications, Accessed February 17, 2023. https://www.manning.com/books/deep-learning-with-r.\n\nThis is the next step after you’ve studied Applied Geodata Science I. It introduces machine learning with deep neural networks using the {keras} machine learning library (with its wrapper in R).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "2  Getting started",
    "section": "",
    "text": "2.1 Learning objectives\nChapter lead author: Pepa Arán\nThis chapter provides a start at the very beginning of your journey in Applied Geodata Science with two main aims. First, it introduces the very basics for readers with no experience using R. You will be able to:\nSecond, the tutorial of this chapter prepares you for your work on the remainder of this course by setting up all the necessary infrastructure. Also experienced R users should follow respective tutorial sections: Section 2.2.6, Section 2.2.7, and Section 2.2.8). You will:\nCompleting these points are essential before we go deeper into project management topics in Chapter 7 and Chapter 8. Chapter 3 will focus on how to code.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#learning-objectives",
    "href": "getting_started.html#learning-objectives",
    "title": "2  Getting started",
    "section": "",
    "text": "Work with R and RStudio.\nKnow basic R objects and classes.\nUnderstand how R interacts with files in your computer.\n\n\n\nOrganize your workspace for efficient data science projects.\nHave installed git and all R packages used throughout the book.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#tutorial",
    "href": "getting_started.html#tutorial",
    "title": "2  Getting started",
    "section": "2.2 Tutorial",
    "text": "2.2 Tutorial\n\n2.2.1 Working with R and RStudio\nR is a free, open-source programming language and software environment for statistical computing and graphics. It is widely used, not only among statisticians and data miners for developing statistical software, but also by scientist in various domains for data analysis, visualisation, and modelling. RStudio is an integrated development environment (IDE) that provides a user-friendly “center stage” for your work in R (and Python, see here).\n\n2.2.1.1 Installing R and RStudio\nTo use R and RStudio, you will first need to download and install them on your computer.\n\nTo install R, go to the CRAN website and download the latest version of R for your operating system. Once the download is complete, follow the on-screen installation instructions for your operating system to install R.\nTo install RStudio, go to the RStudio website and download the latest version of RStudio for your operating system. Once the download is complete, follow the installation instructions for your operating system to install RStudio.\n\n\n\n2.2.1.2 The RStudio interface\nRStudio provides a user-friendly interface for writing, running, and debugging R code. When you open RStudio, you will see the following:\n\n\n\n\n\n\nFigure 2.1: RStudio interface.\n\n\n\nThe interface is divided into four main panels:\n\nThe source editor is where you can write, edit, and save your R code.\nThe console is where you can enter R commands and see the output.\nThe environment panel shows you the objects (variables, data frames, etc.) that are currently in your R session, as well as their values.\nThe files, plots, help, etc. panel shows you the files, plots, and other items that are currently in your R workspace, as well as help and documentation for R functions and packages. We will cover this in more detail later in this course.\n\n\n\n2.2.1.3 Running R code\nOnce you have both programs installed, you can open RStudio and begin a new R session. To run R code using R Studio, follow these steps:\n\nIn the source editor panel, type your R code.\nTo run the code, you can either press the Run button or use the keyboard shortcut Ctrl + Enter (Windows) or Command + Enter (Mac).\nThe code will be executed in the console panel, and any output will be displayed there.\nAlternatively, you can directly type single-statement R commands in the console and run them by pressing Enter.\n\nFor example, let’s say you want to calculate the sum of the numbers 1, 2, and 3. You can write the following code in the console or in the source editor:\n\n# Calculate the sum of 1, 2, and 3\n1 + 2 + 3\n\n[1] 6\n\n\nIf you’ve entered it in the console, press Enter. If you’ve entered it in the source editor, you can press the Run button or use the keyboard shortcut to run the code. The output will be displayed in the console:\n    &gt; 1 + 2 + 3\n    [1] 6\n\n\n2.2.1.4 Base R operations\nThe R {base} package contains the basic functions which let R function as a programming language: arithmetic, input/output, basic programming support, etc. Its contents are always available when you start an R session. Here we introduce the main binary operators, which work on vectors, matrices and scalars.\nArithmetic operators:\n\n+ addition\n- subtraction\n* multiplication\n/ division\n^ or ** exponentiation\n%% modulo operator (returns remainder of a division)\n\nLogical operators:\n\n&gt; greater than\n&gt;= greater than or equal to\n== exactly equal to\n&lt; less than\n&lt;= less than or equal to\n!= not equal\n\n\n\n\n2.2.2 R objects\nIn addition to running single statements in the R console, the output of a statement can be saved as a new object. There are many kinds of R objects, some of which are covered here and in future chapters.\n\n2.2.2.1 Types of data\nFirst, we will introduce the different types of data that one can encounter. We can classify variables according to what values they take.\n\nNumerical: These variables can be measured quantitatively and their value is a number.\n\nContinuous: We say that a variable is continuous when it can take an infinite number of real values within an interval. One could consider unbounded variables (height above sea level) or restricted variables, like positive variables (weight of a person) or an interval (a proportion between 0 and 1).\nDiscrete: When the variable can only take a finite number of values in an interval, we say it is discrete. A common example is count data, like the population of a city.\n\nCategorical: The values are characteristics that cannot be quantified.\n\nBinary: These variables have two possible values: TRUE or FALSE (a variable indicating whether the person has siblings or not).\nNominal: They describe a name, label, or category without a natural order (for example, the name of a person).\nOrdinal: Like their name indicates, ordinal variables are categorical and follow a natural order. For example, “terrible”, “bad”, “neutral”, “good”, “great”. A numerical variable can sometimes be discretized and put into categories, like dividing a person’s age into age groups (bins) “toddler”, “child”, “teenager”, “adult”.\n\n\nNext, we will see how these different types of variables can be treated in R.\n\n\n2.2.2.2 Variables and classes\nIn R, a variable is a named location in memory that stores a value. To create a variable, you simply assign a value to a name using the &lt;- operator (or the = operator, which has an equivalent role when assigning values to a variable, but &lt;- is preferred). For example:\n\nmy_variable &lt;- 5\n\nThis code creates a variable called my_variable and assigns the value 5 to it. You can access the value of a variable or any other object by simply referring to its name, like this:\n\nmy_variable\n\n[1] 5\n\n\nWhen you run this code, the value of my_variable will be printed to the console. Running print(my_variable) is an alternative syntax, using the print() function.\nIn R, every object and value has a class that determines how it is stored and how it behaves. For example, the 5 in our example above is a number, so its class is numeric. To find out the class of a value or a variable, you can use the class() function, like this:\n\nclass(5)\n\n[1] \"numeric\"\n\nclass(my_variable)\n\n[1] \"numeric\"\n\n\nThe most basic classes are:\n\nnumeric (num) - any real number, e.g. 2.375\ninteger (int) - integer numbers, e.g. 2\ncharacter (chr) - any string, e.g., \"fluxes\"\nlogical (logi) - binary, i.e., either TRUE or FALSE.\nfactor (Factor) - categorical data, the variable can only be one of a defined number of options, e.g., one of C3, C4, or CAM (the three pathways of photosynthesis). Factors may also be given an order.\nfunction - a set of statements organized to perform a specific task, for example mean()\n\nBy default, any number is coerced as \"numeric\". So if you want an integer value to have class \"integer\", you need to specify it like this:\n\nmy_variable &lt;- as.integer(5)\nclass(my_variable)\n\n[1] \"integer\"\n\n\nSometimes, you need to convert the class of an object, for example turning an \"integer\" number into a \"character\". You can do so as follows:\n\nmy_variable &lt;- as.character(my_variable)\nmy_variable\n\n[1] \"5\"\n\nclass(my_variable)\n\n[1] \"character\"\n\n\nNote that now, the values are in quotes \"5\". This way, R interprets it as a text and you will not be able to do any numeric calculations with it anymore.\n\n\n2.2.2.3 Vectors\nA vector in R is a sequence of data elements of the same class. Vectors can be created with the c() function, which stands for concatenate, i.e., to link together in a series or chain. For example, the following code creates a numeric vector:\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nTo access the elements of a vector, you can use the square bracket notation. For example, the following code retrieves the second element of the vector x:\n\nx[2]\n\n[1] 2\n\n\nYou can also use the square bracket notation to extract a sub-vector from a larger vector. For example, you can extract the second to fourth elements of the vector x:\n\nx[2:4]\n\n[1] 2 3 4\n\n\nAnother useful property of vectors in R is that they can be easily combined using arithmetic operators. For example, adding the elements of two vectors x and y element-wise:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(4, 5, 6)\nx + y\n\n[1] 5 7 9\n\n\nR also supports vectors of other classes, for example character vectors. Since all elements must be of the same class, the most general class will be adopted. The following code concatenates the vectors x and y, followed by new character elements:\n\nz &lt;- c(x, y, \"seven\", \"eight\")\nz\n\n[1] \"1\"     \"2\"     \"3\"     \"4\"     \"5\"     \"6\"     \"seven\" \"eight\"\n\nclass(z)\n\n[1] \"character\"\n\n\nOperations on vectors are performed element-wise. For example, if we ask what numbers in x are greater than 2, we obtain a vector of logical values (and class \"logical\"):\n\nx &gt; 2\n\n[1] FALSE FALSE  TRUE\n\n\nVectors that contain sequences of numbers are often needed in programming. They are easily created in R, e.g., by:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nMore flexibility is offered by the seq() function:\n\nseq(from = 0, to = 10, by = 2)\n\n[1]  0  2  4  6  8 10\n\n\nOften, we need to evaluate multiple elements in a vector. We’ve learned that the operator &gt; tests whether the value left to it is greater than the value on its right and returns a logical. We can subset a vector based on a vector of equal length that contains logicals.\n\nx &gt; 1\n\n[1] FALSE  TRUE  TRUE\n\nx\n\n[1] 1 2 3\n\nx[x &gt; 1]\n\n[1] 2 3\n\n\nWe can also determine the indices (positions of elements in the vector) that evaluate to TRUE, or that have the lowest value:\n\nwhich(x &gt; 1)\n\n[1] 2 3\n\nwhich.min(x)\n\n[1] 1\n\n\nElements can be dropped from vectors by referring to indices:\n\ny[-2]\n\n[1] 4 6\n\n\n\n\n2.2.2.4 Lists\nLists are R objects, of class \"list\". They are a bit like vectors, but more flexible. They allow us to store different types of data, even if they are of different lengths or of different classes. They are created with the function list() and can be named or not. Here is an example where each element of the list is named.\n\nmylist &lt;- list(\n    temperatures = c(2.234, 1.987, 4.345), \n    my_favourite_function = mean, \n    my_favourite_course = \"Applied Geodata Science\"\n    )\n\nSimilar to vectors, we can extract elements from lists, either by index [[1]] or by the name using [[\"temperatures\"]] or $temperatures. Note the double [[]] here, indicating an element of a list as opposed to [] indicating an element of a vector. To get the entire vector of temperatures, do either of the three:\n\nmylist[[1]]\n\n[1] 2.234 1.987 4.345\n\nmylist[[\"temperatures\"]]\n\n[1] 2.234 1.987 4.345\n\nmylist$temperatures\n\n[1] 2.234 1.987 4.345\n\n\nNote below how, if we index the list like we would index a vector, a list with just one element would be returned, rather than the element itself. [ is used to subset a list (and a list is returned). In contrast, [[ or $ extract a single element from a list. A thorough explanation of these differences is given here and here.\n\nmylist[1]  # returns a subset of the list as a new list\n\n$temperatures\n[1] 2.234 1.987 4.345\n\nmylist[[1]]  # extracts the first element of the list (a vector)\n\n[1] 2.234 1.987 4.345\n\n\nTo get the first temperature value, which is an element of the vector (at the same time an element of the list), we can run:\n\nmylist[[\"temperatures\"]][1]\n\n[1] 2.234\n\n\nYou can also append elements to the list (either way is possible):\n\nmylist[[\"my_second_favourite_function\"]] &lt;- median\nmylist$my_second_favourite_function &lt;- median\n\nThis was a very condensed introduction to vectors and lists. A more complete introduction is given here.\n\n\n2.2.2.5 Data frames\nA data frame, an object of class \"data.frame\", is essentially a table, consisting of named columns and rows. A data frame can be created as follows:\n\ndf &lt;- data.frame(name = c(\"Maria\", \"Peter\", \"Alex\", \"Charlie\"),\n                 age = c(13, 56, 30, 42),\n                 is_married = c(FALSE, TRUE, TRUE, FALSE))\ndf\n\n     name age is_married\n1   Maria  13      FALSE\n2   Peter  56       TRUE\n3    Alex  30       TRUE\n4 Charlie  42      FALSE\n\n\nA data frame can also be understood as a list of vectors of equal length, whereby each vector vector makes up a column and each of these vectors (columns) contains values of the same type. This notion makes it also evident that the elements of a data frame can be accessed the same way like we access elements of lists. To get the vector corresponding to the column named age, we can do:\n\ndf$age\n\n[1] 13 56 30 42\n\n\nData frames can be also be treated as a matrix. Note that the first index refers to rows and the second to columns. For example:\n\ndf[, 1]   # first column (returns a vector)\n\n[1] \"Maria\"   \"Peter\"   \"Alex\"    \"Charlie\"\n\ndf[2, ]   # second row (returns a data frame)\n\n   name age is_married\n2 Peter  56       TRUE\n\ndf[2,2]   # age of Peter (returns a scalar)\n\n[1] 56\n\n\nThe method of selecting parts of a data frame by index is quite flexible. For example, we may require the information in the third column for the first three rows. Putting a colon between two numbers, e.g. [1:3,], indicates we want to select the rows numbers starting at the first and ending with the second number. So here [1:3,] will give us rows one, two and three. This can be combined with subsetting for the other dimension as well:\n\ndf[1:3, 3]\n\n[1] FALSE  TRUE  TRUE\n\n\nTo reduce the data frame to fewer columns/rows that are not contiguous, the function c() is used. This outputs the data frame reduced to the selected row or column numbers inside c(). Another method is to select the columns by column names, i.e. giving as input a string vector with the name of each column we want to select. For example, the following commands give the same output:\n\ndf[, c(1,3)]                   # select by column index\n\n     name is_married\n1   Maria      FALSE\n2   Peter       TRUE\n3    Alex       TRUE\n4 Charlie      FALSE\n\ndf[, c(\"name\", \"is_married\")]  # select by column name\n\n     name is_married\n1   Maria      FALSE\n2   Peter       TRUE\n3    Alex       TRUE\n4 Charlie      FALSE\n\n\nThere are several base R functions to help you understand the structure of a data frame. Here is a non-exhaustive list of of them:\n\nSize\n\ndim() - Returns the dimensions of an object (here: number of rows and columns).\nnrow() - Returns the number of rows of an object.\nncol() - Returns the number of columns of an object.\n\nContent\n\nhead() - Returns the first 6 rows.\ntail() - Returns the last 6 rows.\nView() - Opens a window in the source panel in RStudio where you can look at the entire data set in the form of a table.\n\nNames\n\nnames() - Returns the column names (for data.frame objects it is synonymous to colnames()).\nrownames() - Returns the row names.\n\n\nFor example, the data frame df has 4 rows and 3 columns:\n\ndim(df)\n\n[1] 4 3\n\n\nThere are many more things you can do with data frames. Since they are central to analyzing data with R, we have dedicated all of Chapter Chapter 4 to teach you how to work with data frames in a tidy way with the {tidyverse} collection of packages.\n\n\n2.2.2.6 Functions\nR functions can be applied to an object (or several objects) and return another object. For example, the mean() function can take a numeric vector as input and output the mean of its elements.\n\nmean(df$age)\n\n[1] 35.25\n\n\nFunctions are also R objects and have class \"function\". Writing your own functions is an essential part of good programming and will be introduced in Chapter Chapter 3.\n\n\n2.2.2.7 Missing values\nR has two representations for missing values: NA and NULL. Similar objects also exist in other programming languages.\nNA is an identifier to mark missing data and stands for not available. You will encounter this when reading data into a data frame, and some of its cells show NA because that value is missing. Also, if you ask for the fourth element of a vector of length 3, R returns NA.\n\nx[4]\n\n[1] NA\n\n\nIn general, operations on vectors that contain at least one NA value return NA. For example:\n\nmean(c(1, 2, NA))\n\n[1] NA\n\n\nTo remove all missing values in the function evaluation, the common argument to set in the respective function call is na.rm. By default, it is usually set to FALSE, but we can do:\n\nmean(c(1, 2, NA), na.rm = TRUE)\n\n[1] 1.5\n\n\nFurthermore, NA counts as an element in vectors. A variable assigned just NA would have length 1 (of class \"logical\") and the vector above has length 3, as can be determined using the length() function, and has class \"numeric\".\nWhether a value is missing can be tested by:\n\nis.na(c(1, 2, NA))\n\n[1] FALSE FALSE  TRUE\n\n\nBy contrast, NULL is the R null object or empty space. You can also assign NULL to a variable, which will then have length zero because it is empty. Functions may return NULL when no output was defined, or if an error occurred.\n\n\n2.2.2.8 Read and save objects\nThe function save() allows to save multiple R objects of any form as a single .RData file. This is how the environment of your R session is saved. This is how we would save several R objects:\n\nsave(df, df_small, file = \"./data/data_frames.RData\")\n\nTo tell the function where the data is located, pass the data’s path as an argument. You can either use an absolute path, starting from C:/ on a Windows computer or ~/ on a Mac or Linux. Or, alternatively, you can provide a relative path, where ./ points to the present working directory (like above) and ../ is one level up, or ../../ is two levels up, etc. We recommend that you work with R projects and use relative paths, because the working directory is set to the root directory of the R project and relative paths will also work on another person’s computer, helping with reproducibility.\nIn the example above, we provided a relative path, starting at the current working directory (./). The file data_frames.RData sits in a sub-directory of the current working directory, called data/. What is the current working directory? Unfortunately, the answer is “it depends…”. You’ll learn more about this in Section Section 2.2.7.1.\n.RData files are read into your environment using the load() function. This function loads the objects with the name that they were saved with.\n\nload(\"./data/data_frames.RData\")\n\nAlternatively, the function saveRDS() allows you save single R objects that can then be read into R with a specific (potentially new) variable name. This is more transparent than using save() and load(), and thus is preferred, as it gives the user more control over variable names.\n\nsaveRDS(df_small, file = \"./data/df_small.rds\")\ndf_small_2 &lt;- readRDS(\"./data/df_small.rds\")\n\nsave() and saveRDS() create binary files that are fast to write and read, but only intelligible to R (and not to the human eye or another program). Such files are commonly identified by the suffix .rds. It is recommended to name the .rds files according to the single object they contain. When publishing and sharing data, follow Open Science principles (Chapter Chapter 7) and avoid file formats that are not readable across different platforms and programming languages. We will learn more about human-readable tabular data files like CSV in Chapter Chapter 4 and other open source binary file formats in Chapter Chapter 6.\n\n\n\n2.2.3 R environment\nThe set of objects (variables, data frames, etc.) defined during an R session are referred to as the environment. You can view the objects in RStudio in the environment panel in R Studio, grouped as Data, Values and Functions.\nAfter closing an existing R session (e.g., after quitting RStudio), the environment defined by the used during that session will not be saved automatically and will not be available in your next R session. You should avoid saving your entire environment. Rather, save individual objects into files and read them in explicitly in the next R session, as described above. This gives you control, transparency, and ensures better reproducibility.\n\n\n2.2.4 Libraries\nPackages, also called libraries, are collections of R functions, data, and complied code in a well-defined format. R comes with a standard set of packages (including {base} R, {utils}, {stats}…) and other packages targeted for specific applications are available for download and installation. Once installed, you need to load them each time you start a new R session to use them.\nFor example, the {tidyverse} package is used for data wrangling and will be covered in this course. This is a special package which loads many other packages in the background (like {readr}, {ggplot2}, etc.). You can install a new package as follows:\n\ninstall.packages(\"tidyverse\")\n\nThen, you can load it with the following code. Note that now the name of the package is not in quotation marks.\n\nlibrary(tidyverse)\n\nYou can now use the functions and features provided by the {tidyverse} package in your R scripts.\n\nImagine that you have loaded two packages named {lib1} and {lib2} with the library() function. Now, all the functions in those two packages are available for use in R. But if both of them have a function called fun() and you run the command fun(my_vector), how does R know from which package that function comes? Normally, the package loaded last will “mask” the previous packages, such that their functions (if names are repeated) are used by R.\n\n\nIf you want to specify from which library to take a function, you can use the :: notation. So in our imaginary example we would use lib1::fun(). Furthermore, using the :: notation allows to access a function without the package not being loaded. In general, it’s a good practice to specify the package from which a function comes with ::.\n\nAt any time, you can see a list of your installed packages on the source panel with the following command:\n\nlibrary()\n\nAnd a list of the packages currently loaded:\n\nsearch()\n\n [1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\" \n [4] \"package:grDevices\" \"package:datasets\"  \"renv:shims\"       \n [7] \"package:utils\"     \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\nThis information can also be found on the Packages panel in RStudio. The loaded packages are shown with a tick mark.\n\n\n2.2.5 R scripts\nUsually, multiple statements are needed to get, e.g., from reading data into R to final numbers and figures that make up a further analysis. Together, these multiple statements constitute a workflow. It is essential that all workflows that underlie results of publications are reproducible, that is, that another person can replicate your results using your code and certain data.\nTo make a workflow reproducible, the sequence of statements that you needed to carry out your analysis and produce outputs can be saved as an R script. A script is a text file named with the suffix .R to indicate that it is executable by R. It contains a sequence of R commands, which you can be executed, line by line, starting from the top.\nTo create a new script in RStudio, go to the File menu and select New File &gt; R Script. This will open a new script file in the source editor. You can then type your R code in the script file and save it to your computer.\nTo run a script, you can either use the Source button in the source editor or use the keyboard shortcut Ctrl + Shift + Enter (Windows) or Command + Shift + Enter (Mac). This will run all of the commands in the script file, in the order they are written, in the console. Alternatively, you can type into the console:\n&gt; source(\"my_r_script.R\")\nNote that, to be able to run the code above, the file my_r_script.R must be in your current working directory. You must always specify the path to the file, also when sourcing code.\n\n\n2.2.6 R Markdown\nR Markdown files are an enhanced version of scripts. They combine formatted text and executable code chunks. They can either be compiled (knitted) into an HTML or PDF output, where code chunks are executed upon compilation and visualization outputs are directly placed into the output, or they can be run like a script entirely or each code chunk separately. Rmarkdown is ideal for reporting, i.e., writing your final document presenting your analysis results. When opened in RStudio, RMarkdown files appear in the Editor like this:\n\n\n\n\n\n\nFigure 2.2: R Markdown document opened in the source panel.\n\n\n\nAs shown in Figure 2.2, an RMarkdown file consists of a header that specifies the document properties, such as how it should be rendered (as an html page, a docx file or a pdf).\n---\ntitle: \"Simple global map\"\nauthor: Alex Humboldt\noutput: html_document\n---\nBelow the header follow the contents as either text or code chunks. Text is formatted using the Markdown syntax. Nice brief guides are provided here or here. For example, a top-level section title is specified by # and a title of a section one level lower by ##.\nCode chunks that contain executable R code are opened by a line ```{r}.\nThe document can be rendered by calling rmarkdown::render() on the command line or hitting the Knit button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer). The RMarkdown source file shown in Figure 2.2 is rendered to:\n\n\n\n\n\n\nFigure 2.3: Rendered HTML output.\n\n\n\nNote that the code chunk produced a figure as an output which was placed directly in the rendered html. This demonstrates the usefulness of RMarkdown as notebooks to document entire workflows and make their outputs reproducible.\nTo create a new RMarkdown file, select from the drop-down menu in the top-left corner of RStudio as shown below:\n\n\n\n\n\n\nFigure 2.4: Create a new RMarkdown file.\n\n\n\n\n\n2.2.7 Workspace management\nWorkspace management is crucial for efficient data science projects. Two aspects are essential for workspace management: The structure of directories and the types of files they contain; and code management and version control. To address these two points, we will briefly introduce you to R projects and git.\n\n2.2.7.1 R projects\nThink of all files that are linked to a certain data science workflow as belonging together into one project. A project is therefore a directory that contains the code and the files it produces. Different types of files go in different sub-directories with descriptive names. To keep an order and make your code useful also for others, always keep files of the same type in sub-directories that are always named the same way across projects. When creating a new project, you will find the same overall structure. A common structure is:\n\nKeep .R files that implement a user-defined function (see Chapter 3) in a sub-directory ./R/, and .R files that implement a workflow in a sub-directory ./analysis/. Workflow implementations with a view to communication should be implemented in RMarkdown files.\nRMarkdown files are commonly kept in a sub-directory ./vignettes/ or in the main (top-level) project directory.\nKeep figures produced by your code in a sub-directory ./figures/\nKeep data files produced by your code in a sub-directory ./data/\nData obtained from external sources (e.g., published datasets, data shared by collaborators) and potentially used across multiple projects should be kept outside a project directory. Set up your own directory of data obtained from external sources, for example in your home as ~/data/\n\nTo read and write from/to files you should use relative paths (relative to the project’s top-level directory), like any of the two equivalent following options:\n&gt; source(\"./R/my_r_script.R\")\n&gt; source(\"R/my_r_script.R\")\n… and not:\n&gt; source(\"~/my_project/my_r_script.R\")\nNote that when a path is specified, e.g,. in a script or RMarkdown file that sits itself in a sub-directory, then the effective current working directory is that sub-directory, and not the top-level project directory.\n\nHint: Use here::here() to obtain the path to the top-level. Paths can then be specified relative to that, irrespective of where the code lives that uses that path. For example, a file in sub-directory data/ can be read by read.csv(paste0(here::here(), \"/data/a_file.rds\")).\n\nR sessions in RStudio that are connected to such project directories are R projects. To create a new R project, go to the File menu and select New Project…. This will open the New Project dialog, where you can choose where to save your project and what type of project to create. The current project that you are working on is shown on the upper right corner of the RStudio interface. Here you can also switch between existing projects or create a new one.\n\n\n\n\n\n\nFigure 2.5: R project menu.\n\n\n\nWhen starting a new project, named project_name, a file project_name.Rproj is created in the top-level project directory. It stores information about your the session (settings, open files, etc.), automatically enables useful features in RStudio for easy package, website, or book building, lets you manage code version control (see next sub-section), and optionally (not recommended) stores the environment of your current session. It is not recommended to save and restore entire environments. Instead read and write objects from and into files to guarantee control, transparency, and reproducibility. When you want to continue working on an existing R project, you can start a new session by clicking on your project_name.Rproj file. This restores\nIn Chapter 7 and Chapter 8, we will learn more about efficient code management.\n\n\n2.2.7.2 git\ngit is a version control system that keeps track of all changes on your code, who made it, and when. It allows you to go back to previous changes, and lets you control when to iterate from one state of the code to the next. The basic unit of git is a git repository - all the code contained in a project directory. That’s why R projects work well with git. git can also keep track of changes in data, so long as the data is small and in text format (see Chapter 6).\ngit also lets you sync your local copy of a git repository with git service in the cloud, for example GitHub. This allows you to manage the same repository across computers and collaborate with multiple partners on the same repository.\nWe will learn more about git in Chapter 8. For now, follow instructions in Section Section 2.2.8 to set yourself up with installing git and other required stuff on your computer and with a GitHub account. You will use this later in this course (and maybe throughout your career).\n\n\n\n2.2.8 Setup\n\n2.2.8.1 Packages\nYou either have installed R and RStudio at the start of this chapter or you had them in your computer already. If you belong to the second group, make sure that you update both R and RStudio to get the latest versions. Working with the latest version of R, RStudio and packages helps to avoid errors due to dependency conflicts. Now, let’s install all the required packages for this course and load them.\n\nuse_pkgs &lt;-  c(\"dplyr\", \"tidyr\", \"readr\", \"lubridate\", \"stringr\", \"purrr\",\n              \"ggplot2\", \"tidyverse\", \"visdat\", \"terra\", \"hexbin\", \"jsonlite\",\n              \"MODISTools\", \"forcats\", \"yardstick\", \"recipes\", \"caret\",\n              \"broom\", \"skimr\", \"cowplot\", \"scico\", \"hwsdr\", \"usethis\",\n              \"renv\", \"rsample\", \"modelr\", \"rmarkdown\", \"rpart\",\n              \"rpart.plot\", \"ranger\", \"sessioninfo\", \"ncdf4\")\n\nnew_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, \"Package\"])]\nif (length(new_pkgs) &gt; 0) install.packages(new_pkgs,repos = \"http://cran.us.r-project.org\")\ninvisible(lapply(use_pkgs, require, character.only = TRUE))\n\nIf the installation failed, there should be an error message “Installation of package … had non-zero exit status”. If so, check with teaching assistance.\n\n\n2.2.8.2 Other libraries and applications\nSome R packages depend on one another (for example, all depend on {base} R), but they can also depend on software external to the R ecosystem. For Chapter 6, we will need the {ncdf4} package to work with netCDF files in R. NetCDF (network Common Data Form) is a file format for storing multidimensional scientific data (variables) such as temperature, humidity, pressure, wind speed, and direction. The package {ncdf4} depends on software that is not available as an R package, the netCDF command-line tools, which you need to install additionally on your computer.\nTo install the netCDF command-line tools, follow these instructions:\n\nFor MacOS users, via homebrew:\n\nYou first need to install the package manager Homebrew. Copy-paste the following code into the terminal: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\".\nType into terminal brew --version.\n\nIf version number is shown, you are good to go.\nIf nothing pops up, restart Mac and type in again. If still nothing shows, check with teaching assistance.\n\nType in brew install netcdf following this.\nRestart RStudio if it was open during the steps above. Enter install.packages(\"ncdf4\") and see if it installs it correctly.\n\nIf installation failed, there should be a message “Installation of package … had non-zero exit status”. If so, check with teaching assistance.\n\n\n\n\nNote: The programm Terminal allows you to interact with your Mac through the command line and is installed automatically. You can open it through the Finder if you go to Applications &gt; Utilities &gt; Terminal.\n\n\nFor MacOS users, via MacPorts:\n\nInstall xcode via the Terminal by typing in xcode-select --install (if not installed already).\nThen, install the package manager Homebrew via the terminal code /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\".\nThen, install netcdf via the terminal code brew install netcdf.\nIt is possible that you still cannot install {terra} because you are missing gdal. If so, run brew install gdal in the terminal.\n\nFor Linux users:\n\nSince the package is pre-installed in Linux, just type sudo apt install gdal-bin libgdal-dev in the terminal.\n\nFor Windows users:\n\nDownload the “netCDF-4 64-bit (Windows)” .exe file on the netCDF website.\nOnce downloaded, click on the .exe file and follow the installation dialog.\n\n\nYou can check if the installation was successful by running the following code in the RStudio console:\n\nterra::rast(\"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc\")\n\nclass       : SpatRaster \ndimensions  : 41, 71, 1  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : 4.95, 12.05, 43.95, 48.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : demo_data.nc \nvarname     : t2m (2 metre temperature) \nname        : t2m \nunit        :   K \ntime        : 2022-01-01 12:00:00 UTC \n\n\nOr, to be independent of R and the {terra} package, you can test it by running the following code in your bash terminal (on Mac or Linux):\n# download file demo_data.nc\ncurl --output demo_data.nc https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc\n\n# show header of file\nncdump -h demo_data.nc\n\n\n2.2.8.3 git\nIn this section, you will install git, create a GitHub account and connect it to RStudio. This should set up your laptop so that you can do version control of your code (with an easy to use interface) and upload changes to an online copy of your R project (on GitHub) directly from RStudio. Follow these instructions:\n\nIf you don’t have one already, create a GitHub account on https://github.com and sign in. We encourage you to use a personal email (not your student email) so you can keep using this account for years to come, thus compiling your “code knowledge base”.\nNext, follow the Git download and installation instructions for your OS outlined here.\nOnce Git is installed, reboot RStudio. Set up RStudio for version control by going to Tools &gt; Global Options &gt; Git/SVN and tick “Enable version control interface…”. The path to your Git installation should be correct automatically.\n\nIf it says “(Not Found)”, try the following:\n\nFor Windows, it’s probably C:\\\\Program Files\\\\Git\\\\bin\\\\git.exe.\nFor MacOS and Linux, you can type which git in the Terminal and it will show the installation location.\nIf you get an error after trying this, ask the teaching assistance.\n\n\n\nConnect your GitHub account with Git locally, using SSH keys (which are cryptographic authentication credentials). You can do this from RStudio following the steps:\n\nGo to Tools &gt; Global Options &gt; Git/SVN and select Create SSH Key…. Create an SSH key with the default options (you can ignore the passphrase and click “Create”). This will create a new SSH in your default ~/.ssh folder.\n\nNext, click on View public key (above your new SSH key) and copy the public key (all the text). \nGo back to your browser and open your GitHub account. At the top right, open your profile’s menu and choose Settings &gt; SSH and GPG Keys. Then select Add new SSH key. Give your SSH connection a name, for example “My laptop” and paste the public key that you copied from RStudio. If you have problems, check out these instructions to add a new SSH key to your GitHub account.\nFinally, you can test your SSH connection following these instructions. Note that, at the top of these websites, there’s an option to choose your OS because the steps vary depending on the type of computer your work with.\n\n\n\n\n2.2.8.4 Preparation for your report\nYour performance assessment for Applied Geodata Science will be in the form of a report. You will approach the writing of this report as if you were working on a clean, structured and reproducible data science project. This section is meant to guide you through the preparation of your report workspace, such that it’s in the form that you will submit it. Following the next steps, you will set up a git repository containing an R project and an R Markdown file.\n\nLog into GitHub in your browser and create a new repository. Name it agds_report_yourname, give it a short description and set it to be public, so we can read it for the evaluation. Include a README file (it’s a sort of menu which you’ll edit later), a .gitignore template for R (select in the drop-down menu) and a Creative Commons license (for code sharing rights, etc).\n\nNow that you have a GitHub repository, open it and you should see a green button saying Code. Go to SSH and copy that url. You will use it to clone the repository.\n\nOpen RStudio. Create a new R project (opening the New Project dialog) from a GitHub repository. Paste the repository’s url, give it a name (by default the same as the repository) and select in which folder you want to keep it locally (for example the base directory ~). This will clone all the files from GitHub onto the project directory (i.e. folder) in your computer.\n\nWith this, you’re set up. If your R project agds_report_yourname is open, you should see a Git panel in RStudio (next to Environment) and also be able to use Git in the Terminal panel in RStudio. As mentioned above, we will start working with these tools in a few weeks and they will be a crucial part of the course.\nYou’ll see a detailed explanation of the git workflows in Chapter 8. For now, add your solutions to each report exercise into a separate RMarkdown file, placed in a subdirectory named ./vignettes/. The first Report Exercise will be the one for Chapter 4 where you will be asked to implement your solutions in a file called re_tidy.Rmd. At that stage, your repository should look like this:\n\n\nIn total you should have two R projects related to this course: a local project containing solutions to general exercises (for yourself, created in the exercises below) and the report project + git repository (that you just created and will share with us at the end of the course). Your report project should not include the solutions to general exercises.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#exercises",
    "href": "getting_started.html#exercises",
    "title": "2  Getting started",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\nDimensions of a circle\n\nGiven the radius of a circle r, write a few lines of code that calculates its area and its circumference. Run your code with different values assigned to r.\nPrint the solution as text.\n\n\nHint: Enter pi in your console.\n\n\nHint: Entering print(\"agds\") in your console returns \"agds\". Combining (concatenating) multiple strings into a single one can be done using paste().\n\n\n\nSequence of numbers\nGenerate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5.\n\nHint: Consult the manual of the function seq() by entering ?seq in your terminal.\n\n\n\nGauss sum\nRumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution?\nGauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R.\n\n\nMagic trick algorithm\nDefine a variable named x that contains an integer value and perform the following operations in sequence:\n\nRedefine x by adding 1.\nDouble the resulting number, over-writing x.\nAdd 4 to x and save the result as x.\nRedefine x as half of the previous value of x.\nSubtract the originally chosen arbitrary number from x.\n\nPrint x. Restart the algorithm defined above by choosing a new arbitrary natural number.\n\n\nVectors\nPrint the object datasets::rivers and consult the manual of this object.\n\nWhat is the class of the object?\nWhat is the length of the object?\nCalculate the mean, median, minimum, maximum, and the 33%-quantile across all values.\n\n\nHint: If you don’t know how to solve a problem, help yourself on the internet.\n\n\n\nData frames\nPrint the object datasets::quakes and consult the manual of this object.\n\nDetermine the dimensions of the data frame using the respective function in R.\nExtract the vector of values in the data frame that contain information about the Richter Magnitude.\nDetermine the value largest value in the vector of event magnitudes.\nDetermine the geographic position of the epicenter of the largest event.\n\n\n\nWorkspace\nCreate a new R project and create sub-directories in a meaningful way (as described in this Chapter). Create an RMarkdown file in your new project which implements your solutions to above exercises. Give the file a title, implement some structure in the document, and write some text explaining what your code does.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "programming_primers.html",
    "href": "programming_primers.html",
    "title": "3  Programming primers",
    "section": "",
    "text": "3.1 Learning objectives\nChapter lead author: Pepa Aran\nAfter you’ve gone over the lecture and solved the exercises, you should be able to:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming primers</span>"
    ]
  },
  {
    "objectID": "programming_primers.html#learning-objectives",
    "href": "programming_primers.html#learning-objectives",
    "title": "3  Programming primers",
    "section": "",
    "text": "Use loops, conditional statements and functions in your code\nWrite clean, stylized and structured code\nLook for help",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming primers</span>"
    ]
  },
  {
    "objectID": "programming_primers.html#tutorial",
    "href": "programming_primers.html#tutorial",
    "title": "3  Programming primers",
    "section": "3.2 Tutorial",
    "text": "3.2 Tutorial\n\n3.2.1 Programming basics\nIn this section, we will review the most basic programming elements (conditional statements, loops, functions…) for the R syntax.\n\n3.2.1.1 Conditional statements\nIn cases where we want certain statements to be executed or not, depending on a criterion, we can use conditional statements if, else if, and else. Conditionals are an essential feature of programming and available in all languages. The R syntax for conditional statements looks like this:\n\nif (temp &lt; 0.0){\n  is_frozen &lt;- TRUE\n}\n\nThe evaluation of the criterion inside the round brackets (here (temp &lt; 0.0)) has to return either TRUE or FALSE. Whenever the statement between brackets is TRUE, the chunk of code between the subsequent curly brackets is executed. You can also write a conditional that covers all possibilities, like this:\n\ntemp &lt;- -0.5\nif (temp &lt; 0.0){\n  is_frozen &lt;- TRUE\n} else {\n  is_frozen &lt;- FALSE\n}\n\nWhen the temperature is below 0, the first chunk of code is executed. Whenever it is greater or equal that 0 (i.e. the condition returns FALSE) the second chunk of code is evaluated.\nYou can also write more than two conditions, covering several cases:\n\nis_frozen &lt;- FALSE\njust_cold &lt;- FALSE\nif (temp &lt; 0.0){\n  is_frozen &lt;- TRUE\n} else if (temp &lt; 10){\n  just_cold &lt;- TRUE\n}\n\n\nNote: In the code chunks above, an indentation was used to highlight which parts go together, which makes the code easy to understand. Indentations are not evaluated by R per se (unlike in other programming languages, e.g., Matlab, Python), but help to make the code easier to read.\n\n\n\n3.2.1.2 Loops\nLoops are essential for solving many common tasks. for and while loops let us repeatedly execute the same set of commands, while changing an index or counter variable to take a sequence of different values. The following example calculates the sum of elements in the vector vec_temp by iteratively adding them together.\n\nvec_temp &lt;- seq(10)  # equivalent to 1:10\ntemp_sum &lt;- 0        # initialize sum\nfor (idx in seq(length(vec_temp))){\n  temp_sum &lt;- temp_sum + vec_temp[idx]\n}\ntemp_sum\n\n[1] 55\n\n\nOf course, this is equivalent to just using the sum() function.\n\nsum(vec_temp)\n\nInstead of directly telling R how many iterations it should do we can also define a condition and use a while-loop. As long as the condition is TRUE, R will continue iterating. As soon as it is FALSE, R stops the loop. The following lines of code do the same operation as the for loop above. What is different? What is the same?\n\nidx = 1           # initialize counter\ntemp_sum &lt;- 0     # initialize sum\nwhile (idx &lt;= 10){\n  temp_sum &lt;- temp_sum + vec_temp[idx]\n  idx = idx + 1\n}\ntemp_sum\n\n\n\n3.2.1.3 Functions\nOften, analyses require many steps and your scripts may get excessively long. An important aspect of good programming is to avoid duplicating code. If the same sequence of multiple statements or functions are to be applied repeatedly, then it is usually advisable to bundle them into a new function and apply this single function to each object. This also has the advantage that if some requirement or variable name changes, it has to be edited only in one place. A further advantage of writing functions is that you can give the function an intuitively understandable name, so that your code reads like a sequence of orders given to a human.\nFor example, the following code, converting temperature values provided in Fahrenheit to degrees Celsius, could be turned into a function.\n\n# not advisable\ntemp_soil &lt;- (temp_soil - 32) * 5 / 9\ntemp_air  &lt;- (temp_air  - 32) * 5 / 9\ntemp_leaf &lt;- (temp_leaf - 32) * 5 / 9\n\nFunctions are a set of instructions encapsulated within curly brackets ({}) that generate a desired outcome. Functions contain four main elements:\n\nThey start with a name to describe their purpose,\nthen they need arguments, which are a list of the objects being input,\nenclosed by curly brackets function(x){ ... } for the code making up the body of the function,\nand lastly, within the body, a return statement indicating the output of the function.\n\nBelow, we define our own function f2c():\n\n# advisable\nf2c &lt;- function(temp_f){\n  temp_c &lt;- (temp_f - 32) * 5 / 9\n  return(temp_c)\n}\n\ntemp_soil &lt;- f2c(temp_soil)\ntemp_air  &lt;- f2c(temp_air)\ntemp_leaf &lt;- f2c(temp_leaf)\n\nFunctions are essential for efficient programming. Functions have their own environment, which means that variables inside functions are only defined and usable within that function and are not saved to the global environment. Functions restrict the scope of the domain in which variables are defined. Information flows inside the function only through its arguments, and flows out of the function only through its returned variable.\nFunctions (particularly long ones) can be written to separate source files with a suffix .R and saved in your ./R directory - “written” as in copy and paste the function text as in the code chunk above into a text file with .R suffix. Preferably, the file has the same name as the function. We can save the previous function in a script ./R/f2c.R and load it later by running source(\"./R/f2c\"). It’s good practice to keep one file per function, unless a function calls another function that is called nowhere else. In that case, the “sub-ordinate” function can be placed inside the same .R file.\n\n\n\n3.2.2 Style your code\nNice code is clean, readable, consistent, and extensible (easily modified or adapted). Ugly code works, but is hard to work with. There is no right or wrong about coding style, but certain aspects make it easier to read and use code. Here are a few points to consider.\n\n3.2.2.1 Spaces and breaks\nAdding enough white spaces and line breaks in the right locations greatly helps the legibility of code. Cramming variables, operators, and brackets without spaces leaves an unintelligible sequence of characters and it will not be clear what parts go together. Therefore, consider the following points:\n\nUse spaces around operators (=, +, -, &lt;-, &gt;, etc.).\nUse &lt;-, not =, for allocating a value to a variable.\nCode inside curly brackets should be indented (recommended: two white spaces at the beginning of each line for each indentation level - don’t use tabs).\n\nFor example:\n\nif (temp &gt; 5.0){\n  growth_temp &lt;- growth_temp + temp  \n}\n\n\n\n3.2.2.2 Variable naming\nIt is recommended to use concise and descriptive variable names. Different variable naming styles are being used. In this course, we use lowercase letters, and underscores (_) to separate words within a variable name (_). Avoid (.) as they are reserved for certain types of objects in R. Also, avoid naming your objects with names of common functions and variables since your re-definition will mask already defined object names.\nFor example, df_daily is a data frame with data at a daily resolution. Or clean_daily is a function that cleans daily data. Note that a verb is used as a name for a function and an underscore (_) is used to separate words.\nIt is also recommended to avoid variable names consisting of only one character. Single-letter names make it practically impossible to search for that variable.\n\n# Good\nday_01\n\n# Bad\nDayOne\nday.one\nfirst_day_of_the_month\ndjm1\n\n# Very bad\nmean &lt;- function(x) sum(x)/length(x) # mean() itself is already a function\nT &lt;- FALSE # T is an abbreviation of TRUE\nc &lt;- 10 # c() is used to create a vector (example &lt;- c(1, 2, 3))\n\n\n\n3.2.2.3 Script style\nLoad libraries at the very beginning of a script, followed, by reading data or functions from files. Functions should be defined in separate .R files, unless they are only a few lines long. Then, place the sequence of statements. The name of the script should be short and concise and indicate what the script does.\nUse comments to describe in human-readable text what the code does. Comments are all that appears to the right of a # and are code parts that are not interpreted by R and not executed. Adding comments in the code greatly helps you and others to read your code, understand what it does, modify it, and resolve errors (bugs).\nTo visually separate parts of a script, use commented lines (e.g., #----). The RStudio text editor automatically recognizes ---- added to the right end of a commented line and interprets it as a block of content which can be navigated by using the document (Outline button in the top right corner of the editor panel).\nAvoid reading entire workspace environments (e.g., load(old_environment.RData)), deleting environments rm(list=ls()), loading hidden dependencies (e.g., .Rprofile), or changing the working directory (setwd(\"~/other_dir/\") as part of a script.\nNote that information about the author of the script, its creation date, and modifications, etc. should not be added as commented text in the file. In Chapter 8, we will learn about the code versioning control system git, which keeps track of all this as meta information associated with files.\nA good and comprehensive best practices guide is given by the tidyverse style guide.\n\n\n\n3.2.3 Where to find help\nThe material covered in this course will give you a solid basis for your future projects. Even more so, it provides you with code examples that you can adapt to your own purposes. Naturally, you will face problems we did not cover in the course and you will learn more as you go. Different approaches to getting help can be taken for different types of problems and questions.\n\n3.2.3.1 Within R\n“I know the name of a function that might help solve the problem but I do not know how to use it.” Typing a ? in front of the function will open the documentation of the function, giving information about a function’s purpose and method, arguments, the returned object, and examples. You have learned a few things about plots but you may not know how to make a boxplot:\n\n?graphics::boxplot\n\nRunning the above code will open the information on making boxplots in R.\n“There must be a function that does task X but I do not know which one.” Typing ?? will call the function help.search(). Maybe you want to save a plot as a JPEG but you do not know how:\n\n??jpeg\n\nNote that it only looks through your installed packages.\n\n\n3.2.3.2 Online\nTo search in the entire library of R go to the website rdocumentation.org or turn to a search engine of your choice. It will send you to the appropriate function documentation or a helpful forum where someone has already asked a similar question. Most of the time you will end up on stackoverflow.com, a forum where most questions have already been answered.\n\n\n3.2.3.3 Error messages\nIf you do not understand the error message, start by searching for it on the web. Be aware that this is not always useful as developers rely on the error catching provided by R. To be more specific, add the name of the function and package you are using, to get a more detailed answer.\n\n\n3.2.3.4 Worked examples\nWorked examples are implementations of certain workflows that may serve as a template for your own purpose. It is often simpler to adjust existing code to fulfill your purpose than to write it from scratch. Vignettes are provided for many packages and serve as example workflows that demonstrate the utility of package functions. You can type …\n\nvignette(\"caret\", package = \"caret\")\n\n… to get information about how to use the {caret} package in an easily digestible format. (You will learn more about caret in Chapter 10 and Chapter 11). Several blogs serve similar purposes and are a great entry point to learn about new topics. Examples are the Posit Blog (Posit is the company developing and maintaining RStudio and several R packages), R-bloggers, R-Ladies, etc.\n\n\n3.2.3.5 Asking for help\nIf you cannot find a solution online, start by asking your friends and colleagues. Someone with more experience than you might be able and willing to help you. When asking for help it is important to think about how you state the problem. The key to receiving help is to make it as easy as possible to understand the issue you are facing. Try to reduce what does not work to a simple example. Reproduce a problem with a simple data frame instead of one with thousands of rows. Generalize it in a way that people who do not do research in your field can understand the problem. If you are asking a question online in a forum include the output of sessionInfo() (it provides information about the R version, packages your using,…) and other information that can be helpful to understand the problem. Stackoverflow has its own guidelines on how to ask a good question, which you should follow. Here’s a great template you should use for R-specific question. If your question is well crafted and has not been answered before you can sometimes get an answer within 5 minutes.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming primers</span>"
    ]
  },
  {
    "objectID": "programming_primers.html#exercises",
    "href": "programming_primers.html#exercises",
    "title": "3  Programming primers",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\nGauss variations\nUse a for loop to compute the sum of all natural numbers from 1 to 100. Print the result to the screen. Repeat this exercise but use a while loop.\nAdd up all numbers between 1 and 100 that are at the same time a multiple of 3 and a multiple of 7. Print the result to the screen in the form of: The sum of multiples of 3 and 7 within 1-100 is: {your result}.\n\n\nNested loops\nGiven a matrix mymat and a vector myvec (see below), implement the following algorithm:\n\nStart with the first row in mymat.\nFill all missing values in the current row of mymat with the maximum value in myvec.\nDrop the maximum value from myvec.\nProceed to the next row of mymat and repeat steps 2-4.\n\nmymat and myvec are defined as:\n\nmymat &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, \n              NA, 9, 12, 6, 11, NA, 3, \n              9, 4, 7, 3, 21, NA, 6, \n              rep(NA, 7)),\n            nrow = 4, byrow = TRUE)\nmyvec &lt;- c(8, 4, 12, 9, 15, 6)\n\n\n\nInterpolation\nDefine a vector \\(\\vec{v}\\) of length 100. Define the vector so that \\(v_i = 6\\), for \\(i = 1 : 25\\) and \\(v_i = -20\\), for \\(i = 66 : 100\\). Remaining elements are to be defined as ‘missing’. Linearly interpolate missing values that are not defined. Plot the values of \\(\\vec{v}\\) using plot(vec).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming primers</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "4  Data wrangling",
    "section": "",
    "text": "4.1 Learning objectives\nChapter lead author: Benjamin Stocker\nIn this chapter you will learn how to manipluate and transform data, a curcial part of the data science workflow.\nYou will learn how to:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#learning-objectives",
    "href": "data_wrangling.html#learning-objectives",
    "title": "4  Data wrangling",
    "section": "",
    "text": "read and transform tabulated datas\nunderstand the ‘tidy’ data concept\nselect variables\nAggregate data\nhandle bad and/or missing data",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#setup",
    "href": "data_wrangling.html#setup",
    "title": "4  Data wrangling",
    "section": "4.2 Setup",
    "text": "4.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#tutorial",
    "href": "data_wrangling.html#tutorial",
    "title": "4  Data wrangling",
    "section": "4.3 Tutorial",
    "text": "4.3 Tutorial\nExploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them in a transparent fashion and efficiently - both in terms of robustness against coding errors (“bugs”) and in terms of code execution speed.\nWe refer to data wrangling here to encompass the steps for preparing the data set prior to modelling - including, the combination of variables from different data sources, the removal of bad data, and the aggregation of data to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample).\nIn contrast, pre-processing refers to the additional steps that are either required by the the specific machine learning algorithm used with the data (e.g., centering and scaling for K-Nearest Neighbors or Neural Networks), the gap-filling of variables, or the transformation of variables guided by the resulting improvement of the predictive power of the machine learning model. Pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. We will introduce and discuss data pre-processing in Chapter 10.\n\n4.3.1 Example data\nThe example data used in this chapter are parallel time series of (gaseous) CO\\(_2\\) and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are obtained with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations.\nThe data are provided at half-hourly resolution for the site CH-Lae, located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees in the measured forest are deciduous). The dataset is generated and formatted following standard protocols (FLUXNET2015). For more information of the variables in the dataset, see the FLUXNET2015 website and Pastorello et al., 2020 for a comprehensive documentation of variable definitions and methods.\nFor our demonstrations, the following variables are the most relevant:\n\nTIMESTAMP_START: Hour and day of the start of the measurement period for which the respective row’s data are representative. Provided in a format of “YYYYMMDDhhmm”.\nTIMESTAMP_END: Hour and day of the end of the measurement period for which the respective row’s data are representative. Provided in a format of “YYYYMMDDhhmm”.\nTA_* (°C): Air temperature.\nSW_IN_* (W m\\(^{-2}\\)): Shortwave incoming radiation\nLW_IN_* (W m\\(^{-2}\\)): Longwave incoming radiation\nVPD_* (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure)\nPA_* (kPa): Atmospheric pressure\nP_* (mm): Precipitation\nWS_* (m \\(^{-1}\\)): Wind speed\nSWC_* (%): Volumetric soil water content\nGPP_* (\\(\\mu\\)mol CO\\(_2\\) m\\(^{-2}\\) s\\(^{-1}\\)): Gross primary production (the ecosystem-level gross CO\\(_2\\) uptake flux driven by photosynthesis)\n*_QC: Quality control information for the variable *. Important for us: NEE_*_QC is the quality control information for the net ecosystem CO\\(_2\\) exchange flux (NEE_*) and for GPP derived from the corresponding NEE estimate (GPP_*). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor.\n\nSuffixes _* indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables TA_* contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in Pastorello et al., 2020.\n\n\n4.3.2 Tidyverse\nThe tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output.\n\nFrom this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R {tidyverse} here and we heavily rely on this dialect of the R language throughout the remainder of this course.\n\n\n4.3.3 Reading tabular data\nTabular data are organised in rows and columns. R data frames are tabular data. As introduced in Chapter 2, each column can be regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance or data record - for example a record of simultaneously taken measurements of different variables, along with some attributes and meta information (e.g., the date). In Chapter 6, you will be introduced to other types of data.\nThe most common format for tabular data are CSV (comma-separated-values), typically indicated by the file name suffix .csv. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, .xlsx). The first row in a CSV file typically specifies the name of the variable provided in the respective column.\nLet’s get started with working with our example data set and read it into R, as the variable half_hourly_fluxes. Note that the naming of variables can be important for keeping code legible. Chose intuitively understandable names that describe what the object represents (as done here).\nTo import the data into the R environment, we use the function read_csv() from the {readr} package (part of tidyverse). In other R code, you will also encounter the base R read.csv() function. However, read_csv() is much faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. For example, tibbles generate a nicely readable output when printing the object as is done below.\n\nhalf_hourly_fluxes &lt;- readr::read_csv(\"./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv\")\nhalf_hourly_fluxes\n\n# A tibble: 52,608 × 235\n   TIMESTAMP_START TIMESTAMP_END TA_F_MDS TA_F_MDS_QC TA_ERA  TA_F TA_F_QC\n             &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    200401010000  200401010030    -9999       -9999  -2.22 -2.22       2\n 2    200401010030  200401010100    -9999       -9999  -2.25 -2.25       2\n 3    200401010100  200401010130    -9999       -9999  -2.28 -2.28       2\n 4    200401010130  200401010200    -9999       -9999  -2.50 -2.50       2\n 5    200401010200  200401010230    -9999       -9999  -2.72 -2.72       2\n 6    200401010230  200401010300    -9999       -9999  -2.94 -2.94       2\n 7    200401010300  200401010330    -9999       -9999  -3.17 -3.17       2\n 8    200401010330  200401010400    -9999       -9999  -3.39 -3.39       2\n 9    200401010400  200401010430    -9999       -9999  -3.61 -3.61       2\n10    200401010430  200401010500    -9999       -9999  -3.59 -3.59       2\n# ℹ 52,598 more rows\n# ℹ 228 more variables: SW_IN_POT &lt;dbl&gt;, SW_IN_F_MDS &lt;dbl&gt;,\n#   SW_IN_F_MDS_QC &lt;dbl&gt;, SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, SW_IN_F_QC &lt;dbl&gt;,\n#   LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, LW_IN_F &lt;dbl&gt;,\n#   LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, LW_IN_JSB_ERA &lt;dbl&gt;,\n#   LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, VPD_F_MDS &lt;dbl&gt;,\n#   VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, …\n\n\n\nTo reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here.\n\nSince the file is properly formatted, with variable names given in the first line of the file, the function read_csv() identifies them correctly as column names and interprets values in each column as values of a consistent type (as numeric &lt;dbl&gt;). The file is also automatically machine-readable because it has no merged cells and only one value per cell.\n\n\n4.3.4 Variable selection\nFor our further data exploration, we will reduce the data frame we are working with and select a reduced set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data are large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in Pastorello et al., 2020):\n\nAll variables with names starting with TIMESTAMP)\nAll meteorological variables derived following the “final gap-filled method”, as indicated with names ending with _F.\nGPP estimates that are based on the nighttime decomposition method, using the “most representative” of different gap-filling versions, after having applied the variable u-star filtering method (GPP_NT_VUT_REF) and the corresponding quality control information (NEE_VUT_REF_QC)\nSoil water measured at different depths (variables starting with SWC_F_MDS_)\nDo not use any radiation variables derived with the “JSBACH” algorithm (not with a name that contains the string JSB)\nFlag indicating whether a time step is at night (NIGHT)\n\nThis is implemented by:\n\nhalf_hourly_fluxes &lt;- select(\n  half_hourly_fluxes,\n  starts_with(\"TIMESTAMP\"),\n  ends_with(\"_F\"),\n  GPP_NT_VUT_REF,\n  NEE_VUT_REF_QC,\n  starts_with(\"SWC_F_MDS_\"),\n  -contains(\"JSB\"),\n  NIGHT\n  )\n\nThis reduces our dataset from 235 available variables to 20 variables. As you can see, select() is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with starts_with(), ends_with(), contains(), matches(), etc. Using these functions within select() can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (-) is added in front of a column name or one of the mentioned functions within select(), then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the select() function call. You can find the complete reference for selecting variables here.\n\n\n4.3.5 Time objects\nThe automatic interpretation of the variables TIMESTAMP_START and TIMESTAMP_END by the function read_csv() is not optimal:\n\nclass(half_hourly_fluxes$TIMESTAMP_START[[1]])\n\n[1] \"numeric\"\n\nas.character(half_hourly_fluxes$TIMESTAMP_START[[1]])\n\n[1] \"200401010000\"\n\n\nAs we can see, it is considered by R as a numeric variable with 12 digits (“double-precision”, occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The {lubridate} package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use ymd_hm() to convert them to actual date-time objects.\n\ndates &lt;- ymd_hm(half_hourly_fluxes$TIMESTAMP_START)\ndates[1]\n\n[1] \"2004-01-01 UTC\"\n\n\nWorking with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by:\n\nnextday &lt;- dates + days(1)\nnextday[1]\n\n[1] \"2004-01-02 UTC\"\n\n\nThe following returns the month of each date object:\n\nmonth(dates[1])\n\n[1] 1\n\n\nThe number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the {tidyverse} here, and a complete reference of the {lubridate} package is available here.\n\n\n4.3.6 Variable (re-) definition\nSince read_csv() did not interpret the TIMESTAMP_* variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by:\n\nhalf_hourly_fluxes$TIMESTAMP_START &lt;- ymd_hm(half_hourly_fluxes$TIMESTAMP_START)\n\nModifying existing or creating new variables (columns) in a data frame is done in the {tidyverse} using the function mutate(). The equivalent statement is:\n\nhalf_hourly_fluxes &lt;- mutate(\n  half_hourly_fluxes,\n  TIMESTAMP_START = ymd_hm(TIMESTAMP_START)\n  )\n\n\nNote: Avoid using whitespaces (‘Leerzeichen’) to name columns in a dataframe because it can cause troubles down the line. Instead, use _ to separate words in one name.\n\nIn the code chunk above, the function mutate() is from the tidyverse package {dplyr}. It takes a dataframe as its first argument (here half_hourly_fluxes) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form:\n\nhalf_hourly_fluxes &lt;- half_hourly_fluxes |&gt; \n  mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START))\n\nHere, the pipe operator |&gt; is used. It “pipes” the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course.\n\nNote: The pipe operator is so popular that has been recently included in the latest versions of base R (version 4.1.0 and beyond). This is the |&gt; pipe we just introduced. Nevertheless, you may encounter the %&gt;% operator, which is the original pipe from the {magrittr} package (part of the {tidyverse}).\n\n\nNote: If you cannot update R and have a version older than 4.1.0, just use the magrittr pipe %&gt;% throughout. This can happen if you have an older Macbook that can’t operate the latest Operating System version.\n\nMutating both our timestamp variables could be written as mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END)). Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using across():\n\nhalf_hourly_fluxes &lt;- half_hourly_fluxes |&gt; \n  mutate(across(starts_with(\"TIMESTAMP_\"), ymd_hm))\n\nWe will encounter more ways to use mutate later in this tutorial. A complete reference to mutate() is available here.\nIf you only want to change the name of a variable, but not modify its values, you can do so with the {dplyr} function rename().\n\n\n4.3.7 Axes of variation\nTabular data are two-dimensional (rows \\(\\times\\) columns), but not all two-dimensional data are tabular. For example, raster data are a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the volcano data (provided as an example dataset in R) is a matrix - each column contains the same variable, and no variable names are provided.\n\nvolcano[1:5, 1:5]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  100  100  101  101  101\n[2,]  101  101  102  102  102\n[3,]  102  102  103  103  103\n[4,]  103  103  104  104  104\n[5,]  104  104  105  105  105\n\n\nIn the volcano dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The volcano data are not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given as arrays with more than two dimensions. Typically, this is longitude, latitude, and time, and sometimes a vertical dimension representing, for example, elevation. Such data are multi-dimensional and, as such, not tabular.\nTabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Most environmental data are structured, that is, values of “nearby” observations tend to be more similar than values of “distant” observations. Here, “nearby” and “distant” may refer to a spatial distance, but not necessarily so. Structure in data arises from similarity of the subjects generating the data (e.g., evapotranspiration over two croplands may be more similar than evapotranspiration over a forest), or from temporal proximity. In biological data, there may be a genetic structure arising from evolutionary relatedness (Roberts et al., 2016). Note also that temporal proximity is more complex than than being governed by a single dimension - time. In environmental data, time is often expressed through periodically varying conditions (the diurnal and seasonal cycles). It’s often critical to understand and account for the structure in data when analysing it and using it for model fitting. Challenges are posed when structure is not apparent or not known.\nNote also that some structures are hierarchical. For example, data may be structured by postal codes within cantons; or by hours within a day within a year. Biological data may be generated by species within genera within families. Data from experiments is typically structured as samples within treatments. You see, structure in data is rather the rule than the exception.\nOur example data contains values recorded at each half-hourly time interval over the course of eleven years (check by nrow(half_hourly_fluxes)/(2*24*365)). The data are recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple axes of variation in the data. For example, over the course of one day (2*24 rows in our data), the shortwave incoming radiation SW_IN_F varies over a typical diurnal cycle:\n\nplot(\n  half_hourly_fluxes[1:(2*24),]$TIMESTAMP_START,\n  half_hourly_fluxes[1:(2*24),]$SW_IN_F,\n  type = \"l\"\n  )\n\n\n\n\n\n\n\n\n\nNote: plot() is the very basic of plotting data. In Chapter 5, you will get introduced to additional methods for visualising data. The argument type = \"l\" indicates that we want a line plot, rather than points.\n\nOver the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer:\n\nplot(\n  half_hourly_fluxes[1:(365*2*24),]$TIMESTAMP_START,\n  half_hourly_fluxes[1:(365*2*24),]$SW_IN_F,\n  type = \"l\"\n  )\n\n\n\n\n\n\n\n\nAll data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these “axes of variation” are not reflected by the structure of the data frame object itself and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns TIMESTAMP_* contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by:\n\nhalf_hourly_fluxes |&gt;\n  mutate(year = year(TIMESTAMP_START),\n         month = month(TIMESTAMP_START),\n         doy = yday(TIMESTAMP_START)     # day of year\n         ) |&gt;\n  select(TIMESTAMP_START, TIMESTAMP_END, year, month, doy)  # for displaying\n\n# A tibble: 52,608 × 5\n   TIMESTAMP_START     TIMESTAMP_END        year month   doy\n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2004-01-01 00:00:00 2004-01-01 00:30:00  2004     1     1\n 2 2004-01-01 00:30:00 2004-01-01 01:00:00  2004     1     1\n 3 2004-01-01 01:00:00 2004-01-01 01:30:00  2004     1     1\n 4 2004-01-01 01:30:00 2004-01-01 02:00:00  2004     1     1\n 5 2004-01-01 02:00:00 2004-01-01 02:30:00  2004     1     1\n 6 2004-01-01 02:30:00 2004-01-01 03:00:00  2004     1     1\n 7 2004-01-01 03:00:00 2004-01-01 03:30:00  2004     1     1\n 8 2004-01-01 03:30:00 2004-01-01 04:00:00  2004     1     1\n 9 2004-01-01 04:00:00 2004-01-01 04:30:00  2004     1     1\n10 2004-01-01 04:30:00 2004-01-01 05:00:00  2004     1     1\n# ℹ 52,598 more rows\n\n\nNote that we used mutate() here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function.\n\n\n4.3.8 Tidy data\nData comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in some remotely tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same.\n\nco2_concentration\n\n# A tibble: 36 × 3\n    year month co2_concentration\n   &lt;int&gt; &lt;ord&gt;             &lt;dbl&gt;\n 1  1959 Jan                315.\n 2  1959 Feb                316.\n 3  1959 Mar                316.\n 4  1959 Apr                318.\n 5  1959 May                318.\n 6  1959 Jun                318 \n 7  1959 Jul                316.\n 8  1959 Aug                315.\n 9  1959 Sep                314.\n10  1959 Oct                313.\n# ℹ 26 more rows\n\nco2_concentration_monthly\n\n# A tibble: 3 × 13\n   year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1959  315.  316.  316.  318.  318.  318   316.  315.  314.  313.  315.  315.\n2  1960  316.  317.  317.  319.  320.  319.  318.  316.  314   314.  315.  316.\n3  1961  317.  318.  318.  319.  320.  320.  318.  317.  315.  315.  316.  317.\n\nco2_concentration_yearly\n\n# A tibble: 12 × 4\n   month `1959` `1960` `1961`\n   &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan     315.   316.   317.\n 2 Feb     316.   317.   318.\n 3 Mar     316.   317.   318.\n 4 Apr     318.   319.   319.\n 5 May     318.   320.   320.\n 6 Jun     318    319.   320.\n 7 Jul     316.   318.   318.\n 8 Aug     315.   316.   317.\n 9 Sep     314.   314    315.\n10 Oct     313.   314.   315.\n11 Nov     315.   315.   316.\n12 Dec     315.   316.   317.\n\n\nThere are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to tidy data. In essence, quoting Wickham and Grolemund (2017), data are tidy if:\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\n\n\n\n\n\n\nFigure 4.1: Rules for tidy data. Figure from Wickham and Grolemund (2017).\n\n\n\nThe {tidyr} package provides powerful functions to make data tidy. In the examples above, co2_concentration_monthly and and co2_concentration_yearly are not tidy. In co2_concentration_monthly, the same variable (CO2 concentration) appears in multiple columns. Organising columns by months leads to a “wide” table format. We can convert it to a “long” format by:\n\nco2_concentration_monthly |&gt; \n  pivot_longer(cols = 2:13, names_to = \"month\", values_to = \"co2\")\n\n# A tibble: 36 × 3\n    year month   co2\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1  1959 Jan    315.\n 2  1959 Feb    316.\n 3  1959 Mar    316.\n 4  1959 Apr    318.\n 5  1959 May    318.\n 6  1959 Jun    318 \n 7  1959 Jul    316.\n 8  1959 Aug    315.\n 9  1959 Sep    314.\n10  1959 Oct    313.\n# ℹ 26 more rows\n\n\nThis corresponds to the format of co2_concentration and is tidy. A long format of data frames is required to visualise data using the plotting functions of the {ggplot2} package which will be introduced in Chapter 5.\nEither way, for certain applications, it may be advantageous to work with a wide format. We can convert from a long to a wide format by:\n\nco2_concentration |&gt; \n  pivot_wider(names_from = year, values_from = co2_concentration)\n\n# A tibble: 12 × 4\n   month `1959` `1960` `1961`\n   &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan     315.   316.   317.\n 2 Feb     316.   317.   318.\n 3 Mar     316.   317.   318.\n 4 Apr     318.   319.   319.\n 5 May     318.   320.   320.\n 6 Jun     318    319.   320.\n 7 Jul     316.   318.   318.\n 8 Aug     315.   316.   317.\n 9 Sep     314.   314    315.\n10 Oct     313.   314.   315.\n11 Nov     315.   315.   316.\n12 Dec     315.   316.   317.\n\n\nWhen seeking, for example, the average CO2 concentration for each month, you may be tempted to work with a wide data frame and treat it as a matrix to calculate a mean by rows. You can do so, but then, you leave the tidyverse. This will complicate your life. You’ll learn how to perform tidy data aggregation below.\nThe concept of tidy data can even be taken further by understanding a “value” as any object type, e.g., a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below.\n\n\n4.3.9 Aggregating data\nAggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all \\(N\\) rows in a data frame (\\(N\\times M\\)), calculating the sum for each of the \\(M\\) columns. This returns a data frame (\\(1 \\times M\\)) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within \\(G\\) groups of rows. This yields a data frame (\\(G \\times M\\)) with the number of rows corresponding to the number of groups.\nLet’s say we want to calculate the mean of half-hourly shortwave radiation within each day. We thus have \\(N\\) half-hourly time steps in \\(G\\) days. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or “axis of variation”), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the mean() function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the {dplyr} functions group_by() and summarise(). The entire aggregation workflow is implemented by the following code:\n\ndaily_fluxes &lt;- half_hourly_fluxes |&gt;  \n  mutate(date = as_date(TIMESTAMP_START)) |&gt;  # converts the ymd_hm-formatted date-time object to a date-only object (ymd)\n  group_by(date) |&gt; \n  summarise(SW_IN_F = mean(SW_IN_F))\n\nThe seasonal course can now be more clearly be visualized with the data aggregated to daily values.\n\nplot(daily_fluxes[1:365,]$date, daily_fluxes[1:365,]$SW_IN_F, type = \"l\")\n\n\n\n\n\n\n\n\nWe can also apply multiple aggregation functions to different variables simultaneously. In the example below, we aggregate half-hourly data to daily data by…\n\ntaking the daily mean GPP\ncounting the number of half-hourly data points by day\ncounting the number of measured (not gap-filled) data points\ntaking the mean shortwave radiation\n\nFinally, we calculate the fraction of measured underlying half-hourly data from which the aggregation is calculated and we save the daily data frame as a CSV file for later use.\n\ndaily_fluxes &lt;- half_hourly_fluxes |&gt; \n    mutate(date = as_date(TIMESTAMP_START)) |&gt;   # converts time object to a date object\n    group_by(date) |&gt; \n    summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE),\n              n_datapoints = n(), # counts the number of observations per day\n              n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data)\n              SW_IN_F = mean(SW_IN_F, na.rm = TRUE),  # we will use this later\n              .groups = 'drop' # to un-group the resulting data frame\n              ) |&gt; \n    mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations\nwrite_csv(daily_fluxes, file = \"data/daily_fluxes.csv\")\ndaily_fluxes\n\n# A tibble: 1,096 × 6\n   date       GPP_NT_VUT_REF n_datapoints n_measured SW_IN_F f_measured\n   &lt;date&gt;              &lt;dbl&gt;        &lt;int&gt;      &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 2004-01-01        -0.0138           48          0    38.1          0\n 2 2004-01-02         0.768            48          0    23.9          0\n 3 2004-01-03         0.673            48          0    54.1          0\n 4 2004-01-04        -0.322            48          0    41.7          0\n 5 2004-01-05         0.841            48          0    17.4          0\n 6 2004-01-06         1.22             48          0    40.5          0\n 7 2004-01-07         0.215            48          0    31.6          0\n 8 2004-01-08         1.11             48          0    58.4          0\n 9 2004-01-09         1.44             48          0    11.9          0\n10 2004-01-10         0.364            48          0    27.6          0\n# ℹ 1,086 more rows\n\n\nNote that above, we specified the argument .groups = 'drop' to “un-group” the resulting data frame. The same can also be achieved by a separate function call ungroup() after the summarise() step.\nMore info on how to group values using summarise functions here, or a summary on the inputs the function group_by() and summarise() take.\nAggregating is related to nesting performed by the {tidyr} function nest():\n\nhalf_hourly_fluxes |&gt;  \n  mutate(date = as_date(TIMESTAMP_START)) |&gt;\n  group_by(date) |&gt; \n  nest()\n\n# A tibble: 1,096 × 2\n# Groups:   date [1,096]\n   date       data              \n   &lt;date&gt;     &lt;list&gt;            \n 1 2004-01-01 &lt;tibble [48 × 20]&gt;\n 2 2004-01-02 &lt;tibble [48 × 20]&gt;\n 3 2004-01-03 &lt;tibble [48 × 20]&gt;\n 4 2004-01-04 &lt;tibble [48 × 20]&gt;\n 5 2004-01-05 &lt;tibble [48 × 20]&gt;\n 6 2004-01-06 &lt;tibble [48 × 20]&gt;\n 7 2004-01-07 &lt;tibble [48 × 20]&gt;\n 8 2004-01-08 &lt;tibble [48 × 20]&gt;\n 9 2004-01-09 &lt;tibble [48 × 20]&gt;\n10 2004-01-10 &lt;tibble [48 × 20]&gt;\n# ℹ 1,086 more rows\n\n\nHere, the data frame has one row per date and therefore the same number of rows as the data frame daily_fluxes, but the data itself is not reduced by a summarising function. Instead, the data are kept at the half-hourly level, but it’s nested inside the new column data, which now contains a list of half-hourly data frames for each day. This is just a brief perspective of what nesting is about. More is explained in Section Section 4.4. More comprehensive tutorials on nesting and functional programming are provided in Altman, Behrman and Wickham (2021) or in Wickham and Grolemund (2017), Chapter 21.\n\n\n4.3.10 Data cleaning\nData cleaning is often a time-consuming task and decisions taken during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of “bad” data, and the gap-filling of missing or removed data. An excellent resource for further reading is the Quartz Guide to Bad Data which provides an overview of how to deal with different types of bad data.\n\n4.3.10.1 Cleaning formats\nAs a general principle, we want to have machine readable data. Key for achieving machine-readability is that a cell should only contain one value of one type. Hence, for example, character strings should be kept in separate columns (as separate variables) from numeric data. Character strings can impose particular challenges for achieving machine-readability. Typically, they encode categorical or ordinal information, but are prone to spelling inconsistencies or errors that undermine the ordering or categorization. Here are typical examples for challenges working with character strings and lessons for avoiding problems:\n\nOften, character strings encode the units of a measurement, and entries may be c(\"kg m-2\", \"kg/m2\", \"Kg / m2\", \"1000 g m-2\") . They are all equivalent, but “the machine” treats them as non-identical. To clean such data, one may compile a lookup-table to identify equivalent (but not identical) strings. Much better is to specify a consistent treatment of units before data collection.\nEven if the data are clean and contain a consistently spelled categorical variable in the form of a character string, R doesn’t necessarily treat it as categorical. For certain downstream steps of the workflow, it may be necessary to transform such a variable to one of type factor. For example, as entries of an unordered categorical variable, we have unique(df$gender) = c(\"female\", \"male\", \"non-binary\"). To treat them as categorical and not just mere character strings, we would have to do:\n\n\ndf &lt;- df |&gt; dplyr::mutate(gender = as.factor(gender))\n\n\nCharacter strings may encode ordinal information. For example, entries specify quality control information and are one of c(\"good quality\", \"fair quality, \"poor quality\"). A challenge could be that the spelling is inconsistent (c(\"Good quality\", \"good quality\", …)). Using integers (positive natural numbers) instead of character strings avoids such challenges and enforces an order. The quality control variable NEE_VUT_REF_QC in our example dataset half_hourly_fluxes follows this approach:\n\n\nunique(half_hourly_fluxes$NEE_VUT_REF_QC)\n\n[1] 3 2 1 0\n\n\n\nAn entry like &gt;10 m is not a friend of a data scientist. Here, we have three pieces of information: &gt; as in “greater than”, 10, and m indicating the units. A machine-readable format would be obtained by creating separate columns for each piece of information. The &gt; should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises).\n\n\nString manipulations are usually required for cleaning data. The Section 4.4.2 below demonstrates some simple examples.\nNote that a majority of machine learning algorithms and other statistical model types require all data to be numeric. Methods exist to convert categorical data into numeric data, as we will learn later. We re-visit data cleaning in the form of data pre-processing as part of the modelling workflow in Chapter 10.\n\n\n4.3.10.2 Bad data\nData may be “bad” for different reasons, including sensor error, human error, a data point representing a different population, or unsuitable measurement conditions. In this sense, data are “bad” if they don’t represent what they are assumed by the user to represent. Its presence in analyses and modelling may undermine the model skill or even lead to spurious results. A goal of data cleaning typically is to remove bad data. But how to detect them? And how safe is it to remove them?\nA diversity of processes may generate bad data and it is often not possible to formulate rules and criteria for their identification a priori. Therefore, an understanding of the data and the data generation processes is important for the identification and treatment of bad data. Often, such an understanding is gained by repeated exploratory data analysis cycles, involving the visualization, transformation, and analysis of the data.\nIdeally, information about the quality of the data are provided as part of the dataset. Also other meta-information (e.g., sensor type, human recording the data, environmental conditions during the data collection) may be valuable for data cleaning purposes. In our example dataset, the column with suffices _QC provide such information (see ‘Example data’ section above) and an example for their use in data cleaning is given further below.\nBad data may come in the form of outliers, which are commonly defined based on their value with respect to the distribution of all values of the same variable in a dataset. Hence, their identification most commonly relies on quantifying their distance from the center of the variable’s empirical distribution. The default boxplot() plotting function in R (which we will learn about more in Chapter 5) shows the median (bold line in the center), the upper and lower quartiles (corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , given by the upper and lower edge of the box plot) and the range of \\(( Q_1 - 1.5 (Q_3 - Q_1), Q_3 + 1.5 (Q_3 - Q_1))\\). Any point outside this range is plotted by a circle and labeled an “outlier”. However, this definition is very restrictive and may lead to a false labeling of outliers, in particular if they are drawn from a distribution with a fat tail or from asymmetrical distributions.\nOutliers may also be identified via multivariate distributions. We will re-visit such methods later, in Chapter 9. For certain applications, outliers or anomalies may be the target of the investigation, not the noise in the data. This has spurred the field of anomaly detection which relies on machine learning algorithms for determining whether a value is anomalous, given a set of covariates.\nSensor error or algorithm error may generate spurious values, identified, for example when a continuous variable attains the numerically identical value with a spuriously high frequency.\n\nhalf_hourly_fluxes$GPP_NT_VUT_REF |&gt; \n  table() |&gt; \n  sort(decreasing = TRUE) |&gt; \n  head()\n\n\n 5.18422  3.54996   1.3107 -5.57199 0.984756  2.49444 \n      32       22       19       18       17       17 \n\n\nThe probability of a certain numeric value of a continuous variable to appear twice in a dataset is practically zero. Here, several values appear multiple times - the value 5.18422 even 32 times! This must be bad data.\nOther processes may lead to spurious trends or drift in the data, for example caused by sensor degradation. Spurious step changes or change points in time series or in (multivariate) regressions may be related to the replacement or deplacement of the measuring device. Different methods and R libraries help identifying such cases (see for example this tutorial). Solutions have to be found for the remediation of such spurious patterns in the data on a case-by-case basis.\n\n\n4.3.10.3 Handling missing data\nThe question about when data are “bad” and whether to remove it is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. In practice, this means that the removal of data and entire rows should be avoided and implemented only at the very last step if necessary (e.g., when passing the data into a model fitting function). Instead, information about whether data are bad should be kept in a separate, categorical, variable (a quality control variable, like *_QC variables in our example data half_hourly_fluxes).\nData may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of informative missingness (Kuhn and Johnson, 2019) and its information can be used for modelling. For categorical data, we may replace such data with \"none\" (instead of NA). Some machine learning algorithms (mainly tree-based methods, e.g., Random Forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand.\nMost machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This generally leads to a loss of information contained in the remaining variables. Methods exist to impute missing values in order to avoid this information loss. However, the gain of data imputation has to be traded off against effects of associating the available variables with the imputed (knowingly wrong) values, and effects of data leakage have to be considered. Data imputation as part of the modelling process will be dealt with in Chapter 10.\nIn our example dataset, some values of SWC_F_MDS_* are given as -9999.\n\nhalf_hourly_fluxes |&gt; \n  select(TIMESTAMP_START, starts_with(\"SWC_F_MDS_\")) |&gt; \n  head()\n\n# A tibble: 6 × 9\n  TIMESTAMP_START     SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 SWC_F_MDS_4\n  &lt;dttm&gt;                    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 2004-01-01 00:00:00       -9999       -9999       -9999       -9999\n2 2004-01-01 00:30:00       -9999       -9999       -9999       -9999\n3 2004-01-01 01:00:00       -9999       -9999       -9999       -9999\n4 2004-01-01 01:30:00       -9999       -9999       -9999       -9999\n5 2004-01-01 02:00:00       -9999       -9999       -9999       -9999\n6 2004-01-01 02:30:00       -9999       -9999       -9999       -9999\n# ℹ 4 more variables: SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;,\n#   SWC_F_MDS_3_QC &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;\n\n\nWhen reading the documentation of this specific dataset, we learn that -9999 is the code for missing data. The {dplyr} functions help us to clarify these missing values by mutating across all numeric variables and overwrite entries with NA if they hold a -9999.\n\nhalf_hourly_fluxes &lt;- half_hourly_fluxes |&gt;  \n  mutate(across(where(is.numeric), ~na_if(., -9999)))\n\nhalf_hourly_fluxes |&gt; \n  select(TIMESTAMP_START, starts_with(\"SWC_F_MDS_\")) |&gt; \n  head()\n\n# A tibble: 6 × 9\n  TIMESTAMP_START     SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 SWC_F_MDS_4\n  &lt;dttm&gt;                    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 2004-01-01 00:00:00          NA          NA          NA          NA\n2 2004-01-01 00:30:00          NA          NA          NA          NA\n3 2004-01-01 01:00:00          NA          NA          NA          NA\n4 2004-01-01 01:30:00          NA          NA          NA          NA\n5 2004-01-01 02:00:00          NA          NA          NA          NA\n6 2004-01-01 02:30:00          NA          NA          NA          NA\n# ℹ 4 more variables: SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;,\n#   SWC_F_MDS_3_QC &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;\n\n\nThis lets us visualise the data and its gaps with vis_miss() from the {visdat} package. Visualising missing data can be informative for making decisions about dropping rows with missing data versus removing predictors from the analysis (which would imply too much data removal).\n\nvisdat::vis_miss(\n  half_hourly_fluxes |&gt; slice(1:10000),\n  cluster = FALSE, \n  warn_large_data = FALSE\n  )\n\n\n\n\n\n\n\n\nFor many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. The {dplyr} function used for such tasks is filter(). As argument, it takes the expressions that specify the criterion for filtering using logical operators (&gt;, &gt;=, &lt;, ==, !-, ..., see Chapter 2). Multiple filtering criteria can be combined with logical (boolean) operators:\n\n&: logical AND\n|: logical OR\n! logical NOT\n\nFor example, if we wanted only those rows in our data where NEE is based on measured or good quality gap-filled NEE data, we write:\n\nhalf_hourly_fluxes |&gt;\n  filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1)\n\nFor evaluating multiple OR operations simultaneously, we can write alternatively and equivalently:\n\nhalf_hourly_fluxes |&gt;\n  filter(NEE_VUT_REF_QC %in% c(0,1))\n\nNote that filter() removes entire rows. In some cases this is undesired and it is preferred to replace bad quality values with NA. It is important to note that specifying a value as missing is information itself. Dropping an entire row leads to the loss of this information. For cases where we do not want to drop entire rows when applying filter(), we can just replace certain values with NA. In our case, where we want to retain only data where NEE is based on actual measurements or good quality gap-filling, we can do this by:\n\nhalf_hourly_fluxes |&gt; \n    mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA))\n\nIf we decide to drop a row containing NA in any of the variables later during the workflow, we can do this, for example using the useful {tidyr} function drop_na().\n\nhalf_hourly_fluxes |&gt; \n  drop_na()\n\nAn excellent source for a more comprehensive introduction to missing data handling is given in Kuhn and Johnson (2019).\n\n\n\n4.3.11 Writing data to CSV\nAfter having applied some data reduction and cleaning steps above, let’s save the data frame in the form of a CSV file for use in later chapters.\n\nwrite_csv(half_hourly_fluxes, file = \"data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv\")\n\n\nNote: Making a file publicly available as a .rds or .RData file (explained in Chapter 2) violates the open science principles (introduced in Chapter 7). These two formats make it very easy to save R objects related to your analysis project, but are not adequate to save data. Therefore, whenever possible, save your data in a format that is readable across platforms without requiring proprietary software. Hence use write_csv() from {readr} whenever possible. We will encounter other non-proprietary formats that let you save and share more complex data structures in Chapter 6.\n\n\n\n4.3.12 Combining relational data\nOften, data are spread across multiple files and tables and need to be combined for the planned analysis. In the simplest case, data frames have an identical number of columns, arranged in the same order, and we can “stack” them along rows:\n\nco2_conc_subset_1\n\n# A tibble: 6 × 4\n  month `1959` `1960` `1961`\n  &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Jan     315.   316.   317.\n2 Feb     316.   317.   318.\n3 Mar     316.   317.   318.\n4 Apr     318.   319.   319.\n5 May     318.   320.   320.\n6 Jun     318    319.   320.\n\nco2_conc_subset_2\n\n# A tibble: 6 × 4\n  month `1959` `1960` `1961`\n  &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Jul     316.   318.   318.\n2 Aug     315.   316.   317.\n3 Sep     314.   314    315.\n4 Oct     313.   314.   315.\n5 Nov     315.   315.   316.\n6 Dec     315.   316.   317.\n\nbind_rows(co2_conc_subset_1, co2_conc_subset_2)\n\n# A tibble: 12 × 4\n   month `1959` `1960` `1961`\n   &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan     315.   316.   317.\n 2 Feb     316.   317.   318.\n 3 Mar     316.   317.   318.\n 4 Apr     318.   319.   319.\n 5 May     318.   320.   320.\n 6 Jun     318    319.   320.\n 7 Jul     316.   318.   318.\n 8 Aug     315.   316.   317.\n 9 Sep     314.   314    315.\n10 Oct     313.   314.   315.\n11 Nov     315.   315.   316.\n12 Dec     315.   316.   317.\n\n\nIn other cases, data frames may have an identical set of rows (and arranged in the same order) and we can “stack” them along columns.\n\nco2_conc_subset_3\n\n# A tibble: 12 × 3\n   month `1959` `1960`\n   &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan     315.   316.\n 2 Feb     316.   317.\n 3 Mar     316.   317.\n 4 Apr     318.   319.\n 5 May     318.   320.\n 6 Jun     318    319.\n 7 Jul     316.   318.\n 8 Aug     315.   316.\n 9 Sep     314.   314 \n10 Oct     313.   314.\n11 Nov     315.   315.\n12 Dec     315.   316.\n\nco2_conc_subset_4\n\n# A tibble: 12 × 2\n   month `1961`\n   &lt;ord&gt;  &lt;dbl&gt;\n 1 Jan     317.\n 2 Feb     318.\n 3 Mar     318.\n 4 Apr     319.\n 5 May     320.\n 6 Jun     320.\n 7 Jul     318.\n 8 Aug     317.\n 9 Sep     315.\n10 Oct     315.\n11 Nov     316.\n12 Dec     317.\n\nbind_cols(co2_conc_subset_3, co2_conc_subset_4)\n\nNew names:\n• `month` -&gt; `month...1`\n• `month` -&gt; `month...4`\n\n\n# A tibble: 12 × 5\n   month...1 `1959` `1960` month...4 `1961`\n   &lt;ord&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;ord&gt;      &lt;dbl&gt;\n 1 Jan         315.   316. Jan         317.\n 2 Feb         316.   317. Feb         318.\n 3 Mar         316.   317. Mar         318.\n 4 Apr         318.   319. Apr         319.\n 5 May         318.   320. May         320.\n 6 Jun         318    319. Jun         320.\n 7 Jul         316.   318. Jul         318.\n 8 Aug         315.   316. Aug         317.\n 9 Sep         314.   314  Sep         315.\n10 Oct         313.   314. Oct         315.\n11 Nov         315.   315. Nov         316.\n12 Dec         315.   316. Dec         317.\n\n\nBut beware! In particular the stacking along columns (bind_cols()) is very error-prone and should be avoided. Since a tidy data frame regards each row as an instance of associated measurements, the rows of the two data frames and their order must match exactly. Otherwise, an error is raised or (even worse) rows get associated when they shouldn’t be. In such cases, where information about a common set of observations is distributed across multiple data objects, we are dealing with relational data. The key for their combination (or “merging”) is the join variable - the column that is present in both data frames and which contains values along which the merging of the two data frames is performed. In our example from above, this is month, and we can use the {dplyr} function left_join().\n\nco2_conc_subset_3 |&gt;\n  slice(sample(1:n(), replace = FALSE)) |&gt;  # re-shuffling rows\n  left_join(co2_conc_subset_4, by = \"month\") |&gt; \n  arrange(month)  # sort in ascending order\n\n# A tibble: 12 × 4\n   month `1959` `1960` `1961`\n   &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan     315.   316.   317.\n 2 Feb     316.   317.   318.\n 3 Mar     316.   317.   318.\n 4 Apr     318.   319.   319.\n 5 May     318.   320.   320.\n 6 Jun     318    319.   320.\n 7 Jul     316.   318.   318.\n 8 Aug     315.   316.   317.\n 9 Sep     314.   314    315.\n10 Oct     313.   314.   315.\n11 Nov     315.   315.   316.\n12 Dec     315.   316.   317.\n\n\nNote that here, we first re-shuffled (permuted) the rows of df6 for demonstration purposes, and arranged the output data frame again by month - an ordinal variable. left_join() is not compromised by the order of the rows, but instead relies on the join variable, specified by the argument by = \"month\", for associating (merging, joining) the two data frames. In some cases, multiple columns may act as the joining variables in their combination (for example by = c(\"year\", \"month\")).\nOther variants of *_join() are available as described here.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#sec-extramaterialwrangling",
    "href": "data_wrangling.html#sec-extramaterialwrangling",
    "title": "4  Data wrangling",
    "section": "4.4 Extra material",
    "text": "4.4 Extra material\n\n4.4.1 Functional programming I\nAbove, we read a CSV table into R and applied several data transformation steps. In practice, we often have to apply the same data transformation steps repeatedly over a set of similar objects. This extra material section outlines an example workflow for demonstrating how to efficiently work with lists of similar objects - in particular, lists of data frames.\nOur aim is to read a set of files into R data frames and apply transformation steps to each data frame separately. Here, we will work with daily data, not half-hourly data. The daily data contains largely identical variables with consistent naming and units as in the half-hourly data (description above). Let’s start by creating a list of paths that point to the files with daily data. They are all located in the directory \"./data\" and share a certain string of characters in their file names \"_FLUXNET2015_FULLSET_DD_\".\n\nvec_files &lt;- list.files(\"./data\", pattern = \"_FLUXNET2015_FULLSET_DD_\", full.names = TRUE)\nprint(vec_files)\n\n[1] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[2] \"./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv\"\n[3] \"./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv\"\n[4] \"./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv\"\n\n\n\nTo reproduce this code chunk, you can download the files FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv from here and read it from the local path where the file is stored on your machine.\n\nvec_files is now a vector of three files paths as character strings. To read in the three files and combine the three data frames (list_df below) into a list of data frames, we could use a for loop:\n\nlist_df &lt;- list()\nfor (ifil in vec_files){\n  list_df[[ifil]] &lt;- read_csv(ifil)\n}\n\nRepeatedly applying a function (here read_csv()) over a list similar objects is facilitated by the map*() family of functions from the {purrr} package. An (almost) equivalent statement is:\n\nlist_df &lt;- purrr::map(as.list(vec_files), ~read_csv(.))\n\nHere, purrr::map() applies the function read_csv() to elements of a list. Hence, we first have to convert the vector vec_files to a list. A list is always the first argument within the purrr::map() function. Note two new symbols (~ and .). The ~ always goes before the function that is repeatedly applied (or “mapped”) to elements of the list. The . indicates where the elements of the list would go if spelled out (e.g., here, read_csv(.) would be read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") for the first iteration). The output of purrr::map() is again a list. There are many variants of the function purrr::map() that each have a specific use. A complete reference for all {purrr} functions is available here. A useful and more extensive tutorial on {purrr} is available here.\nThe above purrr::map() call does not return a named list as our for loop created. But we can give each element of the returned list of data frames different names by:\n\nnames(list_df) &lt;- vec_files  # this makes it a named list\n\nNext, we will apply a similar data cleaning procedure to this data set as we did above for half-hourly data. To do so, we “package” the individual cleaning steps into a function …\n\n# function definition\nclean_data_dd &lt;- function(df){\n\n  df &lt;- df |&gt;\n\n    # select only the variables we are interested in\n    dplyr::select(\n      TIMESTAMP,\n      ends_with(\"_F\"),\n      GPP_NT_VUT_REF,\n      NEE_VUT_REF_QC,\n      starts_with(\"SWC_F_MDS_\"),\n      -contains(\"JSB\")) |&gt; \n\n    # convert to a nice date object\n    dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt;\n\n    # set all -9999 to NA\n    dplyr::mutate(across(where(is.numeric), ~na_if(., -9999)))\n  \n  return(df)\n}\n\n… and apply this function to each data frame within our list of data frames:\n\nlist_df &lt;- purrr::map(list_df, ~clean_data_dd(.))\n\nHaving different data frames as elements of a list may be impractical. Since we read in similarly formatted files and selected always the same variables in each data frame, all elements of the list of data frames list_df share the same columns. This suggests that we can collapse our list of data frames and “stack” data frames along rows. As described above, this can be done using bind_rows() and we can automatically create a new column \"siteid\" in the stacked data frame that takes the name of the corresponding list element.\n\ndaily_fluxes_allsites &lt;- bind_rows(list_df, .id = \"siteid\")\ndaily_fluxes_allsites\n\n# A tibble: 23,011 × 21\n   siteid              TIMESTAMP    TA_F SW_IN_F LW_IN_F VPD_F  PA_F   P_F  WS_F\n   &lt;chr&gt;               &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 ./data/FLX_CH-Dav_… 1997-01-01 -4.57     77.4    223. 0.565  82.6   0.4 0.559\n 2 ./data/FLX_CH-Dav_… 1997-01-02 -3.34     45.6    235. 0.978  82.9   0   1.11 \n 3 ./data/FLX_CH-Dav_… 1997-01-03  0.278    74.1    239. 2.24   82.4   0   2.03 \n 4 ./data/FLX_CH-Dav_… 1997-01-04 -1.88     58.1    250. 1.38   81.7   1.8 1.92 \n 5 ./data/FLX_CH-Dav_… 1997-01-05 -4.96     80.8    248. 1.16   82.3   0   0.407\n 6 ./data/FLX_CH-Dav_… 1997-01-06 -4.48     59.6    237. 0.838  82.7   0   0.466\n 7 ./data/FLX_CH-Dav_… 1997-01-07 -3.15     45.5    234. 1.33   82.9   0   1.03 \n 8 ./data/FLX_CH-Dav_… 1997-01-08 -2.45     76.7    222. 1.87   82.7   0   1.95 \n 9 ./data/FLX_CH-Dav_… 1997-01-09 -2.43     47.6    251. 1.44   82.2   0   0.785\n10 ./data/FLX_CH-Dav_… 1997-01-10 -3.09     39.6    242. 0.776  82.8   0   1.25 \n# ℹ 23,001 more rows\n# ℹ 12 more variables: GPP_NT_VUT_REF &lt;dbl&gt;, NEE_VUT_REF_QC &lt;dbl&gt;,\n#   SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;,\n#   SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;,\n#   SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;,\n#   SWC_F_MDS_5_QC &lt;dbl&gt;\n\n\nA visualisation of missing data indicates that soil water content data (SWC_F_MDS_*) are often missing.\n\n# create a subset of the data\ndaily_fluxex_subset &lt;- daily_fluxes_allsites |&gt;\n  slice(1:10000)\n\n# visualize missing data\nvisdat::vis_miss(\n  daily_fluxex_subset,\n  cluster = FALSE,\n  warn_large_data = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n4.4.2 Strings\nThe column siteid currently contains strings specifying the full paths of the files that were read in earlier. The next task is to extract the site name from these strings. The file names follow a clear pattern (this also highlights why naming files wisely can often make life a lot simpler).\n\ndaily_fluxes_allsites$siteid |&gt; head()\n\n[1] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[2] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[3] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[4] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[5] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n[6] \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n\n\nThe paths each start with the subdirectory where they are located (\"./data/\"), then \"FLX_\", followed by the site name (the first three entries of the table containing data from all sites are for the site \"CH-Dav\"), and then some more specifications, including the years that respective files’ data cover.\nThe {stringr} package (part of tidyverse) offers a set of functions for working with strings. Wikham and Grolemund (2017) provide a more comprehensive introduction to working with strings. Here, we would like to extract the six characters, starting at position 12. The function str_sub() does that job.\n\nvec_sites &lt;- str_sub(vec_files, start = 12, end = 17)\nhead(vec_sites)\n\n[1] \"CH-Dav\" \"CH-Lae\" \"FI-Hyy\" \"FR-Pue\"\n\n\nWe can use this function to mutate all values of column \"siteid\", overwriting it with just these six characters.\n\ndaily_fluxes_allsites &lt;- daily_fluxes_allsites |&gt;\n  mutate(siteid = str_sub(siteid, start = 12, end = 17))\n\ndaily_fluxes_allsites\n\n# A tibble: 23,011 × 21\n   siteid TIMESTAMP    TA_F SW_IN_F LW_IN_F VPD_F  PA_F   P_F  WS_F\n   &lt;chr&gt;  &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 CH-Dav 1997-01-01 -4.57     77.4    223. 0.565  82.6   0.4 0.559\n 2 CH-Dav 1997-01-02 -3.34     45.6    235. 0.978  82.9   0   1.11 \n 3 CH-Dav 1997-01-03  0.278    74.1    239. 2.24   82.4   0   2.03 \n 4 CH-Dav 1997-01-04 -1.88     58.1    250. 1.38   81.7   1.8 1.92 \n 5 CH-Dav 1997-01-05 -4.96     80.8    248. 1.16   82.3   0   0.407\n 6 CH-Dav 1997-01-06 -4.48     59.6    237. 0.838  82.7   0   0.466\n 7 CH-Dav 1997-01-07 -3.15     45.5    234. 1.33   82.9   0   1.03 \n 8 CH-Dav 1997-01-08 -2.45     76.7    222. 1.87   82.7   0   1.95 \n 9 CH-Dav 1997-01-09 -2.43     47.6    251. 1.44   82.2   0   0.785\n10 CH-Dav 1997-01-10 -3.09     39.6    242. 0.776  82.8   0   1.25 \n# ℹ 23,001 more rows\n# ℹ 12 more variables: GPP_NT_VUT_REF &lt;dbl&gt;, NEE_VUT_REF_QC &lt;dbl&gt;,\n#   SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;,\n#   SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;,\n#   SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;,\n#   SWC_F_MDS_5_QC &lt;dbl&gt;\n\n\n\n\n4.4.3 Functional programming II\nFunctions can be applied to a list of objects of any type. Therefore, purrr::map() is a powerful approach to “iterating” over multiple instances of the same object type and can be used for all sorts of tasks. In the following, list elements are data frames of daily data and the function lm() fits a linear regression model of GPP versus shortwave radiation to each sites’ data. We’ll learn more about fitting statistical models in R in Chapter 9.\n\nlist_linmod &lt;- purrr::map(list_df, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))\n\nNote how the . indicates where the elements of list_df go when evaluating the lm() function. This returns a list of linear model objects (the type of objects returned by the lm() function call).\nWe can spin the functional programming concept further and apply (or map) the summary() function to the lm-model objects to get a list of useful statistics and metrics, and then further extract the element \"r.squared\" from that list as:\n\nlist_linmod |&gt;\n  purrr::map(summary) |&gt;  # apply the summary() function to each list element\n  map_dbl(\"r.squared\")    # extract R-squared from the list generated by summary()\n\n./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv \n                                                 0.4201802 \n./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv \n                                                 0.5074248 \n./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv \n                                                 0.6415685 \n./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv \n                                                 0.3772839 \n\n\nmap_dbl() is a variant of the purrr::map() function that returns not a list, but a vector of numeric values of class “double” (hence, the name _dbl). Note further, that providing a character (\"r.squared\") as an argument instead of an (unquoted) function name, purrr::map() extracts the correspondingly named list element, instead of applying a function to a list element.\nWhen writing code for an analysis, it’s useful, if not essential, to understand the objects we’re working with, understand its type and shape, and make sense of the results of simple print &lt;object&gt; statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive (variables along columns, observations along rows, values in cells). Here, we’re dealing with a list of linear model objects. Can such a list fit into the paradigm of tidy data frames?\nYes, they can. Think of the linear model objects as ‘values’. Values don’t necessarily have to be scalars, but they can be of any type (class).\n\ntibble::tibble(\n  siteid = vec_sites,\n  linmod = list_linmod\n  )\n\n# A tibble: 4 × 2\n  siteid linmod      \n  &lt;chr&gt;  &lt;named list&gt;\n1 CH-Dav &lt;lm&gt;        \n2 CH-Lae &lt;lm&gt;        \n3 FI-Hyy &lt;lm&gt;        \n4 FR-Pue &lt;lm&gt;        \n\n\nThe fact that cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may even contain another data frame. In such a case, we say that the data frame is no longer flat, but nested.\nThe following creates a nested data frame, where the column data is defined by the list of data frames read from files above (list_df).\n\ntibble::tibble(\n  siteid = vec_sites,\n  data = list_df\n  )\n\n# A tibble: 4 × 2\n  siteid data                 \n  &lt;chr&gt;  &lt;named list&gt;         \n1 CH-Dav &lt;tibble [6,574 × 16]&gt;\n2 CH-Lae &lt;tibble [4,018 × 18]&gt;\n3 FI-Hyy &lt;tibble [6,940 × 20]&gt;\n4 FR-Pue &lt;tibble [5,479 × 10]&gt;\n\n\nWe can achieve the same result by directly nesting the flat data frame holding all sites’ data (daily_fluxes_allsites). This is done by combining the group_by(), which we have encountered above when aggregating using summarise(), with the function nest() from the {tidyr} package.\n\ndaily_fluxes_allsites |&gt;\n  group_by(siteid) |&gt;\n  nest()\n\n# A tibble: 4 × 2\n# Groups:   siteid [4]\n  siteid data                 \n  &lt;chr&gt;  &lt;list&gt;               \n1 CH-Dav &lt;tibble [6,574 × 20]&gt;\n2 CH-Lae &lt;tibble [4,018 × 20]&gt;\n3 FI-Hyy &lt;tibble [6,940 × 20]&gt;\n4 FR-Pue &lt;tibble [5,479 × 20]&gt;\n\n\nThe function nest() names the nested data column automatically \"data\".\nThis structure is very useful. For example, for applying functions over sites’ data frames separately (and not over the entire data frame). By combining purrr::map() and mutate(), we can fit linear models on each site’s data frame individually in one go.\n\ndaily_fluxes_allsites |&gt;\n  group_by(siteid) |&gt;\n  nest() |&gt;\n  dplyr::mutate(linmod = purrr::map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .)))\n\nThis approach is extremely powerful and lets you stick to working with tidy data frames and use the rows-dimension flexibly. Here, rows are sites and no longer time steps, while the nested data frames in column \"data\" have time steps along their rows. The power of nesting is also to facilitate complex aggregation steps over a specified dimension (or axis of variation, here given by siteid), where the aggregating function is not limited to taking a vector as input and returning a scalar, as is the case for applications of summarise() (see above).\nCombining the steps described above into a single workflow, we have:\n\ndaily_fluxes_allsites_nested &lt;- daily_fluxes_allsites |&gt;\n  group_by(siteid) |&gt;\n  nest() |&gt;\n  dplyr::mutate(linmod = purrr::map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) |&gt;\n  dplyr::mutate(summ = purrr::map(linmod, ~summary(.))) |&gt;\n  dplyr::mutate(rsq = map_dbl(summ, \"r.squared\")) |&gt;\n  arrange(desc(rsq))  # to arrange output, with highest r-squared on top\n\ndaily_fluxes_allsites_nested\n\n# A tibble: 4 × 5\n# Groups:   siteid [4]\n  siteid data                  linmod summ         rsq\n  &lt;chr&gt;  &lt;list&gt;                &lt;list&gt; &lt;list&gt;     &lt;dbl&gt;\n1 FI-Hyy &lt;tibble [6,940 × 20]&gt; &lt;lm&gt;   &lt;smmry.lm&gt; 0.642\n2 CH-Lae &lt;tibble [4,018 × 20]&gt; &lt;lm&gt;   &lt;smmry.lm&gt; 0.507\n3 CH-Dav &lt;tibble [6,574 × 20]&gt; &lt;lm&gt;   &lt;smmry.lm&gt; 0.420\n4 FR-Pue &lt;tibble [5,479 × 20]&gt; &lt;lm&gt;   &lt;smmry.lm&gt; 0.377\n\n\nThis code is a demonstration of the power of tidy and nested data frames and for the clarity of the {tidyverse} syntax.\nNesting is useful also for avoiding value duplication when joining relational data objects. Above, we nested time series data objects (where time steps and sites are both organised along rows) by sites and got a data frame where only sites are organised along rows, while time steps are nested inside the column \"data\". This now fits the structure of a relational data object (siteinfo_fluxnet2015) containing site-specific meta information (also with only sites along rows).\n\nbase::load(\"data/siteinfo_fluxnet2015.rda\")  # loads siteinfo_fluxnet2015\n\nJoining the nested data frame with site meta information results in a substantially smaller and much handier data frame compared to an alternative, where the site meta information is joined into the un-nested (daily) data frame, and therefore duplicated for each day within sites.\n\ndaily_fluxes_allsites_nested_joined &lt;- siteinfo_fluxnet2015 |&gt;\n  rename(siteid = sitename) |&gt;\n  right_join(\n    select(daily_fluxes_allsites_nested, -linmod, -summ, -rsq),\n    by = \"siteid\"\n  )\n\ndaily_fluxes_allsites_joined &lt;- siteinfo_fluxnet2015 |&gt;\n  rename(siteid = sitename) |&gt;\n  right_join(\n    daily_fluxes_allsites,\n    by = \"siteid\"\n  )\n\nprint(paste(\"Flat and joined:\", \n            format(object.size(daily_fluxes_allsites_joined),  \n                   units = \"auto\", \n                   standard = \"SI\")))\n\n[1] \"Flat and joined: 5.8 MB\"\n\nprint(paste(\"Nested and joined:\", \n            format(object.size(daily_fluxes_allsites_nested_joined),  \n                   units = \"auto\", \n                   standard = \"SI\")))\n\n[1] \"Nested and joined: 3.7 MB\"\n\n# save for later use\nwrite_rds(\n  daily_fluxes_allsites_nested_joined,\n  file = \"data/daily_fluxes_allsites_nested_joined.rds\"\n  )",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#exerciseswrangling",
    "href": "data_wrangling.html#exerciseswrangling",
    "title": "4  Data wrangling",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nHint: For all exercises remember the resources we provided on finding help in section Section 3.2.3.\n\n\nStar wars\n{dplyr} comes with a toy dataset dplyr::starwars (just type it into the console to see its content). Have a look at the dataset with View(). Play around with the dataset to get familiar with the {tidyverse} coding style. Use (possibly among others) the functions dplyr::filter(), dplyr::arrange(), dplyr::pull(), dplyr::select(), dplyr::desc() and dplyr::slice() to answer the following questions:\n\nHow many pale characters come from the planets Ryloth and Naboo?\nWho is the oldest among the tallest thirty characters?\nWhat is the name of the smallest character and their starship in “Return of the Jedi”\n\n\nHint: Use unnest() to expand columns that contain lists inside cells. The expansion of such columns creates additional rows in the data frame if the cell contained a list with more than one element.\n\n\n\nAggregating\nYou have learned about aggregating in the {tidyverse}. Let’s put it in practice.\n\nReuse the code in the tutorial to read, reduce, and aggregate the half_hourly_fluxes dataset to the daily scale, calculating the following metrics across half-hourly VPD_F values within each day: mean, 25% quantile, and 75% quantile.\nRetain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles.\nCalculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD.\n\n\n\nPatterns in data quality\nThe uncleaned dataset FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv holds half-hourly data that is sometimes of poor quality. Investigate whether NEE data quality is randomly spread across hours in a day by calculating the proportion of (i) actually measured data, (ii) good quality gap-filled data, (iii) medium quality data, and (iv) poor quality data within each hour-of-day (24 hours per day).\n\nHint: summarise(total = n()) aggregates by counting the number of values.\n\n\nHint: summarise(count_0 = sum(x == 0)) aggregates by counting the number of values for which the evaluation is TRUE.\n\nInterpret your findings: Are the proportions evenly spread across hours in a day?\nPerform an aggregation of the half-hourly GPP data (variable GPP_NT_VUT_REF) to daily means of the unmodified data read from file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv, and from cleaned data where only measured (not gap-filled) data is kept. This yields two data frames with daily GPP data. Calculate the overall mean GPP for the two data frames (across all days in the data frame). Are the overall mean GPP values equal? If not, why?",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#sec-retidy",
    "href": "data_wrangling.html#sec-retidy",
    "title": "4  Data wrangling",
    "section": "4.6 Report Exercise",
    "text": "4.6 Report Exercise\n\nAnalyzing changes in soil organic matter during elevated CO\\(_2\\) experiments\nOpen Science requires that data underlying published research articles is made available upon publication of the article. A separate aspect is the format of the shared data. Is it provided in an open-access data format? How easy is it to use the data for your own analyses? In this exercise, you will encounter data that was made freely available, but not in an open access format. Although it is “nice-looking” data, you will encounter that it is not tidy.\nIn this exercise, you will investigate the data published by Groeningen et al. (2014), where they looked at how soil carbon stocks may change due to the anthropogenic increase in \\(CO_2\\). They gathered data on changes in the soil organic matter content from experiments where ecosystems were exposed to elevated CO\\(_2\\) concentrations and your task is to have a high-level look at this dataset. So, perform the following steps:\n\nDownload the data file (.xlsx) from the Supplementary Material of the following paper: Groenigen, Kees Jan van, Xuan Qi, Craig W. Osenberg, Yiqi Luo, and Bruce A. Hungate. “Faster Decomposition Under Increased Atmospheric CO2 Limits Soil Carbon Storage.” Science 344, no. 6183 (May 2, 2014): 508–9. https://doi.org/10.1126/science.1249534.\nManually clean the data in the tab “Database S1” and save it as a CSV file that can be read into R.\n\n“Database S1” contains data of soil organic carbon measurements in experiments, where ecosystems are exposed to ambient (low) and elevated (high) CO\\(_2\\) concentrations. The mean soil organic carbon of multiple samples (“n”) is recorded within each experiment for different sample dates. Information is provided for the time in years since the start of the experiment (“Time (years)”). \n\nIn RStudio, create RMarkdown file. Then, write your code into the R chunks of the file to aggregate the data per experiment and calculate the log-response ratio within each experiment, as specified below.\n\nA log-response ratio can be used to quantify the effect that a treatment (e.g., elevated CO\\(_2\\)) can have on your target variable \\(x\\) (e.g., soil organic matter content). The log-response ratio can be calculated as: \\(\\text{RR} = \\ln \\left( \\frac{x_\\text{elevated}}{x_\\text{ambient}} \\right)\\)\nAggregate data across all experiments for different years since the start of the experiment, distinguishing an early phase (&lt;3 years since start), a mid-phase (3-6 years since start), and a late phase (&gt;6 years since start). Calculate the log-response ratio for each phase. Calculate the log-response ratio for each parallel observation of SOC under ambient and elevated CO\\(_2\\), and then aggregate log response ratios by taking their mean (and not the other way round).\nPresent your results as tables using the knitr::kable() function.\nTip: Depending on your Excel settings, your exported csv is separated using , or ;. The read_csv() function only works with ,. So, if your file is separated with ;, either change the export settings or use read_csv2().Where would it have been useful if more information had been available.\n\nAnswer the following questions:\n\nWhat are the data that you are looking at?\nWhat do you expect your analysis to show, what is your hypothesis? How should soil organic matter content change under elevated CO\\(_2\\)?\nInterpret your results after aggregating the data: What do your final numbers mean? Do they support your initial hypothesis? Why so, why not?\nTip: Skim the paper and its references to better understand the data and processes you are looking at!``\n\n\n\n\nDeliverables for the report\nA key learning of this course is that you know how to create reproducible workflows and we pay a lot of attention on this during the assessment. You will learn more about the specific aspects to do so in Chapter 7 and Chapter 8. So, if the following instructions for submitting this report exercise may sound cryptic, hang tight! It will make more sense as you work through this course.\nYou have to submit all report exercises via your GitHub repository that you created in Section 2.2.8.4. For now, save this report exercise as a RMarkdown file with the name re_tidy.Rmd and place that file in a sub-directory called vignettes (a directory is simply a folder), which is located in your project directory (in full notation that means save your RMarkdown as ./vignettes/re_tidy.Rmd). Additionally, produce a HTML version of your solution by knitting your RMarkdown file (see the Knit command in the panel below the files tabs in RStudio). Save this HTML file in the same vignettes directory (./vignettes/re_tidy.html). For your cleaned dataset, pick a sensible name and put it into a sub-direcory called ./data. As with all other report exercises, make sure that your work is reproducible, for example, by letting another student download your repository from GitHub and knit your RMarkdown files on their computer.\n\nImportant: Structure your RMarkdown so that you first load necessary libraries, load your data and then write down your solution. Remember to access files with relative paths and not with hard-coded paths. So, access your file via here::here('data/tidy_data.csv), where here starts at the position of your .Rproj file. Never write hard-coded paths that looks like this: \"~/Desktop/Studium/.../tidy_data.csv\" or \"C:/Users/.../tidy_data.csv\" (see Section 2.2.7).\n\n\nTip: Your results do not have to match the results from the paper. It is important that you make your code reproducible, aggregate the data as instructed, and interpret your results.\n\n\nTip: If you are new to using RMarkdown, check out this guide.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "data_vis.html",
    "href": "data_vis.html",
    "title": "5  Data visualisation",
    "section": "",
    "text": "5.1 Learning objectives\nChapter lead author: Benjamin Stocker\nIn this chapter you will learn how to visualize data for exploration or publication.\nYou will learn among others the:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_vis.html#learning-objectives",
    "href": "data_vis.html#learning-objectives",
    "title": "5  Data visualisation",
    "section": "",
    "text": "appropriate choice of visualisations for different data types and different aspects of the data\ngrammar of graphics, i.e., using the {ggplot2} library\nthe proper use of colours in visualization",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_vis.html#setup",
    "href": "data_vis.html#setup",
    "title": "5  Data visualisation",
    "section": "5.2 Setup",
    "text": "5.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_vis.html#tutorial",
    "href": "data_vis.html#tutorial",
    "title": "5  Data visualisation",
    "section": "5.3 Tutorial",
    "text": "5.3 Tutorial\nVisualizations often take the center stage of publications and are often the main vehicles for transporting information in scientific publications and (ever more often) in the media. Visualizations communicate data and its patterns in visual form. Visualizing data is also an integral part of the exploratory data analysis cycle. Visually understanding the data guides its transformation and the identification of suitable models and analysis methods.\nThe quality of a data visualization can be measured by its effectiveness of conveying information about the data and thus of answering a question with the data and telling a story. Different aspects determine this effectiveness, including the appropriateness of visualization elements, the intuitiveness of how information can be decoded from the visualization by the reader, the visual clarity and legibility (taking into account the vision and potential vision deficiencies of the reader), the visual appeal, etc. This tutorial introduces data visualization under the premise that not all aspects of data visualization are a matter of taste. There are appropriate and less appropriate ways of encoding data in visual form.\nThis tutorial is inspired by the comprehensive and online available textbook Fundamentals of Data Visualization by Claus O. Wilke. Another excellent resource is the Chapter Data Visualisation in R for Data Science by Hadley Wickham\n\n5.3.1 The grammar of graphics\nIn Chapter 4, we learned about axes of variation in the data. For example, time is an axis of variation in our example data half_hourly_fluxes, or site identity and the date are axes of variation in our example data daily_fluxes. We have also learned that we can aggregate over axes of variation, and that we can often separate an axis of variation into a hierarchy of subordinate axes of variation (e.g., years, months, days, and a half-hourly time axis).\nIn this chapter, we will be working mainly with the same half-hourly time series data of ecosystem-atmosphere fluxes and parallel measurements of meteorological variables - as in Chapter 2 and Chapter 4. For time series data, the entry point of the exploratory data analysis cycle may be a visualization of some variable of interest (here GPP_NT_VUT_REF) against time:\n\nhalf_hourly_fluxes &lt;- readr::read_csv(\"data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv\")\n\nRows: 52608 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N...\ndttm  (2): TIMESTAMP_START, TIMESTAMP_END\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplot(half_hourly_fluxes$TIMESTAMP_START, half_hourly_fluxes$GPP_NT_VUT_REF, type = \"l\")\n\n\n\n\n\n\n\n\n\nTo reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here.\n\nYou may notice the spurious-looking values on the left, in the first third of year 2004. This demonstrates the role of visualisation in understanding the data and its quality. We’ll revisit this point later in this Chapter.\nFrom (Wilke):\n“All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics.”\nApplied to our example, the aesthetics are the x-axis and the y-axis of a cartesian coordinate system. TIMESTAMP_START is mapped onto the x-axis, GPP_NT_VUT_REF is mapped onto the y-axis, and their respective values specify the position of points in the cartesian coordinate system that are then connected with lines - making up the geometrical object that represents the data. Often, the aesthetic that is used to plot the target variable against corresponds to a known axis of variation in the data.\nThe notion of mapping data onto aesthetics and using objects whose geometry is defined by the aesthetics gives rise to the grammar of graphics and to the {ggplot2} R package for data visualisation (which we will use throughout the remainder of this course). The equivalent {ggplot2} code that follows the philosophy of the grammar of graphics is:\n\nggplot(data = half_hourly_fluxes, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe argument provided by the aes() statement specifies the aesthetics (x, and y) and which variables in data are mapped onto them. Once this is specified, we can use any suitable geometrical object that is defined by these aesthetics. Here, we used a line plot specified by geom_line(). Note also that the different objects to creating a ggplot graph are connected with a +, which takes a similar role as the pipe (|&gt;).\nThe data visualisation above is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying three years of half-hourly data in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span (selecting rows by index using dplyr::slice() in the code below). Visual clarity is also facilitated by an appropriate labeling (title and axes labels using labs()) and by a reduction of displayed elements to a minimum (therefore, the changing of the formatting theme by theme_classic()):\n\n# prepare plot data\nplot_data &lt;- half_hourly_fluxes |&gt; \n  dplyr::slice(24000:25000)\n\n# plot figure\nplotme &lt;- ggplot(\n    data = plot_data,\n    aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +\n  geom_line() +\n  labs(title = \"Gross primary productivity\", \n       subtitle = \"Site: CH-Lae\",\n       x = \"Time\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\nplotme\n\n\n\n\n\n\n\nFigure 5.1: An example time series created with {ggplot2}.\n\n\n\n\n\nFind a complete reference to {ggplot2} here. The grammar of graphics has found its way also into Python and you can use ggplot using the plotnine Python package (see here).\n\n\n5.3.2 Every data has its representation\nIn the above example, we mapped two continuous variables (TIMESTAMP_START and GPP_NT_VUT_REF) onto the aesthetics x, and y to visualize time series data. A line plot is an appropriate choice for such data as points are ordered along the time axis and can be connected by a line. Different “geometries” are suitable for visualizing different aspects of the data, and different variable types are suited to mapping onto different aesthetics. Common, available aesthetics are shown in Figure 5.2 and can be allocated to variable types:\n\nContinuous variables: position, size, color (a color gradient), line width, etc.\nCategorical variables: shape, color (a discrete set of colors), line type, etc.\n\n\n\n\n\n\n\nFigure 5.2: Common aesthetics to display different variable types. Figure from Wilke.\n\n\n\nNot only the different aesthetics, but also the type of geometry (the layers of the visualization added to a plot by + geom_*()) goes with certain types of variables and aspects of the data (but not with others). The sub-sections below provide a brief categorization of data visualization types. A more comprehensive overview is given by GGPlot2 Essentials for Great Data Visualization in R by Alboukadel Kassambara.\n\n5.3.2.1 One value per category\nProbably the simplest case of data visualization is where a single value is shown across a categorical variable. This calls for a bar plot (geom_bar()). In the example below, we plot the mean GPP for each month. The “custom plot” shown below is a demonstration for what you can do by combining different elements with {ggplot2}. Try to understand the command for creating the object plot_2. Both examples are based on the data frame daily_fluxes which we created in Chapter 4.\n\n# read in demo daily data\n# as saved in the previous chapter\ndaily_fluxes &lt;- read_csv(\"data/daily_fluxes.csv\")\n\nRows: 1096 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (5): GPP_NT_VUT_REF, n_datapoints, n_measured, SW_IN_F, f_measured\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Aggregate to monthly\nmdf &lt;- daily_fluxes |&gt; \n  dplyr::mutate(month = month(date, label = TRUE)) |&gt; \n  dplyr::group_by(month) |&gt; \n  dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF))\n\n# Bar plot\nplot_1 &lt;- ggplot(\n  data = mdf,\n  aes(x = month, y = GPP_NT_VUT_REF)) +\n  geom_bar(stat = \"identity\") +\n  theme_classic() +\n  labs(title = \"Bar plot\",\n       x = \"Month\", \n       y =expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")))\n\n# Custom plot\nplot_2 &lt;- ggplot(\n  data = mdf,\n  aes(x = month, y = GPP_NT_VUT_REF)) +\n  geom_segment(aes(x = month, xend = month, y = 0, yend = GPP_NT_VUT_REF), \n               size = 3, color = \"grey40\") +\n  geom_point(aes(x = month, y = GPP_NT_VUT_REF), size = 8, color = \"tomato\") +\n  geom_text(aes(x = month, y = GPP_NT_VUT_REF, label = format(GPP_NT_VUT_REF, digits = 2)),\n            size = 3, color = \"white\") +\n  theme_classic() +\n  labs(title = \"Custom plot\",\n       x = \"Month\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  scale_y_continuous(limits = c(0, 8.75), expand = c(0, 0)) +\n  coord_flip()\n\n# combine plots\ncowplot::plot_grid(plot_1, plot_2)\n\n\n\n\n\n\n\n\nAbove, we created two objects, plot_1 and plot_2, that contain the instructions for creating the plots. To combine multiple sub-plots within panels of a single plot, we used cowplot::plot_grid() from the {cowplot} library.\nNote also the stat = \"identity\" specification within the geom_bar() function call. This is required when the bar height is specified by a single value within each category (month in the example above). To visualize not a value per se but the count of values within categories, use stat = \"count\" to get the equivalent result as when aggregating by taking the number of observations within categories explicitly using the function dplyr::summarise(). This equivalency is demonstrated below.\n\n# subset plot data and count occurences of bad data (NEE_VUT_REF_QC == 0)\ndf_count &lt;- half_hourly_fluxes |&gt; \n  dplyr::filter(NEE_VUT_REF_QC == 0) |&gt; \n  dplyr::group_by(NIGHT) |&gt; \n  dplyr::summarise(count = n())\n\n# separate aggregation  \nplot_1 &lt;-ggplot(\n  data = df_count,\n  aes(x = NIGHT, y = count)) +\n  geom_bar(stat = \"identity\") +\n  labs(subtitle = \"Count via 'summarise' and 'stat = identiy'\") +\n  theme_classic()\n\n# prepare data (not summarizing counts)\nhalf_hourly_fluxes_bad &lt;- half_hourly_fluxes |&gt; \n  dplyr::filter(NEE_VUT_REF_QC == 0)\n\n# implicit aggregation by 'stat'\nplot_2 &lt;- ggplot(\n  data = half_hourly_fluxes_bad,\n  aes(x = NIGHT)) +\n  geom_bar(stat = \"count\") +\n  labs(subtitle = \"Count directly via 'stat = count'\") +\n  theme_classic()\n\n# combine plots\ncowplot::plot_grid(plot_1, plot_2)\n\n\n\n\n\n\n\n\n\n\n5.3.2.2 Distribution of one variable\nExamining the distribution of a variable is often the first step of exploratory data analysis. A histogram displays the distribution of numerical data by mapping the frequency (or count) of values within discrete bins (equally spaced ranges along the full range values of a given variable) onto the “height” of a bar, and the range of values within bins onto the position of the bar. In other words, it shows the count of how many points of a certain variable (below GPP_NT_VUT_REF) fall into a discrete set of bins. When normalizing (scaling) the “bars” of the histogram to unity, we get a density histogram. To specify the y-axis position of the upper end of the histogram bar as the density, use y = ..density.. in the aes() call. To show counts, use y = ..count...\n\nggplot(\n    data = half_hourly_fluxes,\n    aes(x = GPP_NT_VUT_REF, y = ..density..)\n    ) +\n  geom_histogram(fill = \"grey70\", color = \"black\") +\n  geom_density(color = \"red\") +  # we can overlay multiple plot layers!\n  labs(title = \"Histogram and density\", \n       x = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote that the red line plotted by geom_density() on top of the density histogram visualises the density distribution in continuous (not discrete, or binned) form. Note also that both “geoms” share the same aesthetics with aes() specified in the ggplot() function call.\n\n\n5.3.2.3 Distributions within categories\nTo visualize distributions of a single continuous variable within categories, perhaps the most common visualization type is the box plot. As described in Chapter 4, it shows the median (bold line in the center), the upper and lower quartiles, corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , and given by the upper and lower edge of the box plot. The lines extending from the box edges visualize the range of \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Any point outside this range is plotted by a point.\nThe box plot is rather reductionist in showing the data (the vector of all values is reduced to the median, \\(Q_1\\) , \\(Q_3\\), and outlying points) and may yield a distorted picture of the data distribution and does not reflect information about the data volume. For this reason, several journals are now requiring individual data points or at least the number of data points to be shown in addition to each box. Below, points are added by geom_jitter() , where points are “jittered”, that is, randomly spread out along the x-axis. Violin plots are a hybrid of a density plot and a box plot. The shape of their edge is given by the density distribution of the points they represent.\n\n# prepare plot data\nset.seed(1985)  # for random number reproducibility in sample_n() and jitter\nhalf_hourly_fluxes_subset &lt;- half_hourly_fluxes |&gt; \n  sample_n(300) |&gt; \n  mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE))\n\n# Boxplot \nplot_1 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = Night, y = VPD_F)) +\n  geom_boxplot(fill = \"grey70\") +\n  labs(title = \"Boxplot\") +\n  labs(y = \"VPD (hPa)\") +\n  theme_classic()\n\n# Box plot + jittered points\nplot_2 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = Night, y = VPD_F)) +\n  geom_boxplot(fill = \"grey70\", outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  labs(title = \"Boxplot + jitter points\") +\n  labs(y = \"VPD (hPa)\") +\n  theme_classic()\n\n# Violin plot\nplot_3 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = Night, y = VPD_F)) +\n  geom_violin(fill = \"grey70\") +\n  labs(title = \"Violin plot\") +\n  labs(y = \"VPD (hPa)\") +\n  theme_classic()\n\n# combine plots\ncowplot::plot_grid(plot_1, plot_2, plot_3, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n5.3.2.4 Regression of two continuous variables\nScatter plots visualize how two variables co-vary. The position of each point in a scatter plot is given by the simultaneously recorded value of two variables, provided in two columns along the same row in a data frame, and mapped onto two dimensions in a cartesian coordinate system. We can also say that two variables are regressed against each other.\nIn the figure below, we start with a simple scatter plot (a), regressing GPP against shortwave radiation. A visualization is supposed to tell a story with data. The positive and largely linear relationship between shortwave radiation and GPP is expected from theory and our process understanding of the dominant controls on photosynthesis - it’s mainly solar (shortwave) radiation (However, strictly speaking, this is supposed to emerge only at longer time scales, not in half-hourly data). The linear regression line, added by geom_smooth(method = \"lm\") in (a), indicates that relationship. You will learn more about linear regression in Chapter 9.\nAre there additional variables that modify the relationship between solar radiation and GPP? To visually investigate this, we can map additional variables in our data set onto additional aesthetics. For example, at night, photosynthesis ceases as shown in (b). Here, the variable NIGHT was mapped onto the aesthetic color of same geometry (geom_point()). By default, ggplot() used a continuous color scale, as indicated by the color key on the right. It did so although NIGHT is a categorical (a binary) variable because in the data frame, NIGHT is stored as a numeric value (as can be checked by class(half_hourly_fluxes$NIGHT)). To avoid this, and automatically trigger the use of a color scheme that is suitable for categorical variables, we specify aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT)) in (c). In (d), temperature, a continuous variable, is mapped onto a continuous color scheme.\n\n# prepare plot data\nhalf_hourly_fluxes_subset &lt;- half_hourly_fluxes |&gt;\n  sample_n(1000)\n\n# a\nplot_1 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(size = 0.75) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n# b\nplot_2 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = NIGHT)) +\n  geom_point(size = 0.75) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n# c\nplot_3 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))) +\n  geom_point(size = 0.75) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n# d\nplot_4 &lt;- ggplot(\n  data = half_hourly_fluxes_subset,\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = TA_F)) +\n  geom_point(size = 0.75) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")),\n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic() +\n  scale_color_viridis_c()\n\n# combine plots\ncowplot::plot_grid(plot_1, plot_2, plot_3, plot_4, ncol = 2, labels = \"auto\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 5.3: Regression plots.\n\n\n\n\n\n\n\n5.3.2.5 Use of colors\nThe above example demonstrates that color schemes have to be chosen depending on the nature of the data. Mapping a continuous variable onto the aesthetics color requires a continuous color scheme to be applied, categorical data requires discrete color schemes. More distinctions should be considered:\n\nContinuous variables should be distinguished further if they span a range that includes zero or not. If so, diverging color schemes should be used, where zero appears neutral (e.g., white). If zero is not contained within the range of values in the data, diverging color schemes should be avoided.\nContinuous or ordinal variables may be cyclic in nature. For example, hours in a day are cyclic, although there are twelve discrete numbers. The time 00:00 is nearer to 23:59 than it is from, for example, 01:00. The cyclical, or periodical nature of the data should be reflected in the choice of a color scheme where the edges of the range are more similar to one another than they are to the center of the range (see example below)\nMultisequential color schemes reflect that there is a natural distinction between two parts of the range of continuous values (see example below).\n\nChoices of colors and their combination is far from trivial. Colors in color schemes (or “scales”) should be:\n\nDistinguishable for people with color vision deficiency\nDistinguishable when printed in black and white\nEvenly spaced in the color space\nIntuitively encoding the information in the data (for example, blue-red for cold-hot)\nVisually appealing\n\nIn (d), we mapped temperature, a continuous variable, onto the color aesthetic of the points and chose the continuous {viridis} color scale by specifying + scale_color_viridis_c(). The viridis scales have become popular for their respect of the points listed above.\nFor further reading, several excellent resources exist that theorize and guide the use of color in data visualization. Excellent sources are:\n\nFabio Crameri’s Scientific colour maps, Crameri (2018) and its R package {scico} (on CRAN).\nPaul Tol’s Notes, available for example in the {khroma} R package (on CRAN).\n\n\n\n5.3.2.6 Regression within categories\nIn the sub-plot (d) above, we may observe a pattern: GPP recorded at low temperatures (dark colored points) tend to be located in the lower range of the cloud of points. We may formulate a hypothesis from this observation, guiding further data analysis and modelling. This illustrates how data visualization is an integral part of any (geo-) data science workflow.\nSince the relationship between incoming solar radiation and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (the site CH-Lae from which the data is recorded is located in a mixed forest), the slope of the regression between solar radiation and GPP should change between months.\nHence, let’s consider months as the categories to be used for separating the data and analyzing the bi-variate relationships separately within. Below, two alternatives are presented. Either the data is separated into a grid of sub-plots, or the data is separated by colors within the same plot panel.\nSeparation by color\n\n# prepare data\ndaily_fluxes &lt;- daily_fluxes |&gt;\n  dplyr::mutate(month = month(date, label = TRUE))\n\nggplot(\n  data = daily_fluxes,\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = month)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ x + 0, method = \"lm\", se = FALSE) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")) ) +\n  theme_classic() +\n  scico::scale_color_scico_d(palette = \"romaO\")\n\n\n\n\n\n\n\n\nNote three aspects here. First, the color-mapping is specified within aes() in the ggplot() function call and then adopted for all subsequent additions of geoms. Hence, also the geom_smooth() thus takes the color information, and not by a “hard-coded” specification of color = inside the geom_smooth() call as done in Fig. Figure 5.3.\nSecond, we specified a formula for the linear regression “smooting curve” to force the lines through the origin (y ~ x + 0). This is motivated by our a priori understanding of the process generating the data: when solar radiation is zero, photosynthesis (and hence GPP) should be zero.\nThird, we chose a color palette that reflects the cyclic (or periodic) nature of the categories (months). January is closer to December than it is to April. Therefore, their respective colors should also be closer in the color space. An appropriate palette for this is \"romaO\" from the {scico} package.\nSeparation into sub-plots\nYet another “mapping” is available with facet_wrap(). It separates the visualisation into different sub-plots, each showing only the part of the data that falls into the respective category, separated by facet_wrap(). Note, this mapping is not dealt with the same way as other aesthetics - not with specifying it with aes()), but with adding the facet_wrap() with a + to the ggplot() object. The variable by which facet_wrap() separates the plot has to be specified as an argument with a preceeding ~. Here, this is ~month.\n\nggplot(\n  data = daily_fluxes, # reusing the previously subset data (see above)\n  aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(formula = y ~ x + 0, method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")) ) +\n  facet_wrap(~month)\n\n\n\n\n\n\n\n\nYou may object here that a linear regression is not a good model for our data. Instead, the relationship looks saturating, as indicated for example by the data in August. But we’ll get to modelling in later chapters. Nevertheless, the two visualizations above confirm our suspicion that the light-GPP relationship varies between months - a demonstration for why data visualization is an integral part of the scientific process.\n\n\n5.3.2.7 Time series\nA time series plot can be regarded as a special case of a regression of two variables. In this case, one variable is regressed against time. A defining aspect of time is that there is a natural order in time steps. Therefore, it makes sense to visualize temporal data using lines that connect the points using geom_line(). The example below shows the time series of daily GPP in three years.\n\nggplot(\n  data = daily_fluxes,\n  aes(x = date, y = GPP_NT_VUT_REF)) +\n  geom_line() +\n  labs(title = \"Line plot\", \n       x = \"Time\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n\n\n\n\n\n\n\nIn the line plot above, we see a spurious-looking part of the time series in the first third of year 2004. Is this bad data that should be removed? Also, in winter of 2005/2005, some daily GPP values appear as high as a typical summer level of GPP. Is this bad data? Remember, that in Chapter 4, we aggregated the half-hourly half_hourly_fluxes data frame to a daily data frame daily_fluxes from which data is visualized above. The aggregation kept a record of the fraction of actually measured (not gap-filled) half-hourly data points per day (f_measured). This yields a “data quality axis”. Is there a pattern between f_measured and the presumably bad data? Discerning such patterns is often only possible with a suitable visualization. What is suitable here?\nA solution is to “map” f_measured to the color axis. When adding such an additional mapping to visualisation dimensions (“aesthetics”), we have to specify it using aes(). This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write aes(x = date, y = GPP_NT_VUT_REF) in the ggplot() function call (indicating that all subsequent additions of geom_ layers share this x-y mapping); while aes(color = f_measured) is specified only in the geom_point() layer.\n\nggplot(\n    data = daily_fluxes,\n    aes(x = date, y = GPP_NT_VUT_REF)) +\n  geom_line() +\n  geom_point(aes(color = f_measured), size = 0.9) +\n  labs(x = \"Time\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here\n  theme_classic()\n\n\n\n\n\n\n\n\nWe observe the presumably bad data appear in yellow, and are therefore indeed characterised with a particularly low fraction of actually measured data from which their values are derived. This is an insight we would never have reached by just looking at the naked values in our data frames. Data visualizations are essential for guiding analyses and data processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria.\n\n\n5.3.2.8 Periodic data\nThe seasons are an important axis of variation in our data. Hence our data are periodic - with a periodicity of 365 days in the daily_fluxes dataset and with both 12 hours and 365 days in the half_hourly_fluxes dataset. A polar coordinate system, instead of a cartesian system, lends itself to displaying periodic data. A polar coordinate system reflects the fact that, for example, January 1st is closer to December 31st, although they are located on the extreme end of a linear spectrum of days in a year. In a polar coordinate system, the x-axis spans the angle (360\\(^\\circ\\), like a clock), while the y-axis spans the radius (distance from the center). This is specified by changing the coordinate system of the ggplot object by + coord_polar().\nBelow, we first aggregate the data to get a mean seasonal cycle from daily_fluxes (a, b), and to get a mean diurnal (daily) cycle from June data in half_hourly_fluxes (c, d). To get the mean seasonal cycle, we first determine the day-of-year (counting from 1 for January first to 365 for December 31st) using the {lubridate} function yday().\n\n# prepare plot data\ndaily_mean_fluxes &lt;- daily_fluxes |&gt;\n  mutate(doy = yday(date)) |&gt; \n  group_by(doy) |&gt; \n  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF))\n\n# seasonal cycle, cartesian\nplot_1 &lt;- ggplot(\n  data = daily_mean_fluxes,\n  aes(doy, GPP_NT_VUT_REF)) + \n  geom_line() + \n  labs(y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")),\n       x = \"Day of year\")\n\n# seasonal cycle, polar\nplot_2 &lt;- ggplot(\n  data = daily_mean_fluxes,\n  aes(doy, GPP_NT_VUT_REF)) + \n  geom_line() +\n  coord_polar() + \n  labs(y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")),\n       x = \"Day of year\")\n\n# prepare plot data (diurnal step)\ndaily_mean_hourly_fluxes &lt;- half_hourly_fluxes |&gt;\n  mutate(month = month(TIMESTAMP_START)) |&gt; \n  filter(month == 6) |&gt;  # taking only June data\n  mutate(hour = hour(TIMESTAMP_START)) |&gt; \n  dplyr::group_by(hour) |&gt; \n  dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF))\n\n# diurnal cycle, cartesian\nplot_3 &lt;- ggplot(\n    data = daily_mean_hourly_fluxes,\n    aes(hour, GPP_NT_VUT_REF)) + \n  geom_line() + \n  labs(y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")),\n       x = \"Hour of day\")\n\n# diurnal cycle, polar\nplot_4 &lt;- ggplot(\n  data = daily_mean_hourly_fluxes,\n  aes(hour, GPP_NT_VUT_REF)) + \n  geom_line() +\n  coord_polar() + \n  labs(y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")),\n       x = \"Hour of day\")\n\n# combine plots\ncowplot::plot_grid(plot_1, plot_2, plot_3, plot_4, ncol = 2, labels = \"auto\")\n\n\n\n\n\n\n\n\n\n\n5.3.2.9 Density along two continuous variables\nScatter plots can appear “overcrowded” when points are plotted on top of each other and potentially important information is lost in the visualization.\n\nggplot(\n    data = half_hourly_fluxes,\n    aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point() +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")),\n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n\n\n\n\n\n\n\nTo avoid obscuring important details about the data, we may want to visualise the density of points. We want to plot how many points fall within bins of a certain range of values in GPP and shortwave radiation, or, in other words, within grid cells in the GPP-radiation space. We can visualize the data, for example, with a raster plot that measures the density using stat_density_2d() or with a binning into hexagonal cells using the simple geom_hex() layer.\n\n# density raster\nggplot(\n    data = daily_fluxes,\n    aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  stat_density_2d(\n    geom = \"raster\", # the geometric object to display the data\n    aes(fill = after_stat(density)), # using `density`, a variable calculated by the stat\n    contour = FALSE \n    ) +\n  scale_fill_viridis_c() +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")) ) +\n  theme_classic() + \n  scale_x_continuous(expand = c(0, 0)) +  # avoid gap between plotting area and axis\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n# density hexagonal bins\nggplot(\n    data = daily_fluxes,\n    aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_hex() +\n  scale_fill_viridis_c() +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")) ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAn alternative solution to “overplotting” points is described in this blog post.\n\n\n5.3.2.10 Raster data\nIn the figure above, the density of points in a grid of equally spaced bins along two axes, one for each variable, was shown. Often, data is organised along a grid of equally spaced bins by nature - think a matrix or raster data. Examples of such data are climate model outputs (which often span more than two dimensions), remote sensing images (again, just one “layer” of an image or one “band”), or images in general. In these cases, two (often spatial) axes span the space of a cartesian coordinate system and the value within each pixel is mapped onto the color aesthetic.\nThe base-R function image() can be used to visualize such spatial data as images.\n\nimage(volcano)\n\n\n\n\n\n\n\n\n{ggplot2} forces us to the data frame paradigm and therefore doesn’t lend itself naturally to raster data. We can, however, convert raster data into a data frame in a separate step.\n\n# example from https://github.com/thomasp85/scico\ndf_volcano &lt;- tibble(\n  x = rep(seq(ncol(volcano)), each = nrow(volcano)),\n  y = rep(seq(nrow(volcano)), ncol(volcano)),\n  height = c(volcano) - 140  # subtract 140 for example below\n)\n\nggplot(\n  data = df_volcano,\n  aes(x = x, y = y, fill = height)) + \n  geom_raster() + \n  scico::scale_fill_scico(palette = 'bukavu', midpoint = 0) + \n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nIn this example, we used the multisequential \"bukavu\" color scale from the {scico} package to reflect values above 0 (e.g., elevation above the sea level) and below 0 (ocean depth).\n\n\n5.3.2.11 Geospatial data\nRaster data often reflects values in geographic space. Also points and polygons may be located in geographic space. This opens the door to geospatial data visualisation (a.k.a. creating maps) which does not form part of this course.\nInterested readers can find more content on geospatial data visualisation here:\n\nTimo Grossenbacher’s blog on geographic data journalism (link)\nKoen Hufken’s blog containing various worked example.\n\n\n\n\n5.3.3 Writing to file\nPlots can be saved in figure files in any common format. To write the figure that was rendered last into a file, call the ggsave(filename) function. The format of the image is automatically determined based on filename. E.g., a PDF is created if the filename is. For example,\n\nplotme\nggsave(\"./figures/plot_gpp.pdf\")\n\n… saves the plot generated by printing plotme (created in the code chunk preceeding Figure 5.1) into a PDF located in the appropriate (see also Section 2.2.7.1) what’s “appropriate”) sub-directory. Plots are ideally saved as vector graphics - the types of graphics that are defined by geometric shapes (polygons, lines, points), not rasters. Vector graphics never look “pixelated”. When saving a ggplot object as PDF, it’s saved as a vector graphic. Other vector graphics formats are .eps, or .svg. Beware however, when plotting very large numbers points, lines, or polygons, file sizes can get excessively large and render very slowly. In such cases, consider representing the density of overlaying points which creates a vector graphics, e.g., of rectangles (raster), hexagons, or other polygons (see Section 5.3.2.9). Alternatively, plots may be saved as a raster image (e.g., PNG with files ending on .png, giving the option to manually set the resolution with the argument dpi) or a compressed image format (e.g, JPEG). When rendering an RMarkdown file, figures are placed in the output and, unless explicitly triggered with a ggsave() call inside an executed code chunk, no separate figure files are created.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_vis.html#exercises",
    "href": "data_vis.html#exercises",
    "title": "5  Data visualisation",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\nSpurious data\nIn Section 4.3.10.2, we discovered that certain values of GPP_NT_VUT_REF in the half-hourly data half_hourly_fluxes (to be read from file data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv) are repeated with a spuriously high frequency. Determine all values of GPP_NT_VUT_REF that appear more than once in half_hourly_fluxes and label them as being “spurious”. Visualise the time series of the first year of half-hourly GPP, mapping the information whether the data is spurious or not to the color aesthetic.\nThen aggregate half-hourly to daily data, taking the mean of GPP_NT_VUT_REF and recording the proportion of underlying half-hourly data points that are “spurious”. Visualise the time series of daily GPP_NT_VUT_REF with the color scale indicating the proportion of spurious half-hourly data that was used for determining the respective date’s mean GPP.\n\nHint: The base-R function duplicated(df, ...) takes a data frame df as input and returns a logical vector of length nrow(df) specifying whether a row has an exact duplicate in the same data frame.\n\n\n\nIdentifying Outliers\nA key part of data cleaning is to detect and understand outliers. Visualisations can help. Your task here is to find outliers in GPP_NT_VUT_REF.\nFirst, using the half-hourly fluxes data, determine “outliers” as those values of GPP_NT_VUT_REF that fall outside \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red.\n\nHint: Use boxplot.stats() to return a list containing a vector of the data points which lie beyond the extremes of the whiskers of the boxplot.\n\n\nHint: Use scale_color_manual() to mannually define the color scale.\n\nNow, we want to “control” for the influence of shortwave radiation on GPP and define outliers with respect to the distribution of residuals of the linear regression between the two variables. Relax the definition of what is considered an outlier by setting adjusting their definition to falling outside \\(( Q_1 - 5 (Q_3 - Q_1)\\) to \\(Q_3 + 5 (Q_3 - Q_1)\\). Again, plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red.\n\nHint: Fit the linear regression model as lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes) and obtain the residuals from the object returned by the lm() function (see ‘Value’ in its help page).\n\n\nHint: The output of boxplot.stats(x) is a list, containing an element out. out is a named vector of the oulier values with names referring to the row numbers of x. Use as.integer(names(boxplot.stats(x)$out)) to get row numbers.\n\n\n\nVisualising diurnal and seasonal cycles of GPP\nAs explored in the previous Chapter’s exercises, GPP varies over diurnal and seasonal cycles. Create a publication-ready figure that visualises the mean diurnal cycle of GPP for each day-of-year (mean across multiple years). Make sure that the figure is properly labelled, and legible for readers with a color vision deficiency.\n\nHint: To get the diurnal and seasonal cycles, summarise the half-hourly data by the hour of the day and the day of the year simultaneously using multiple grouping variables within group_by() and calculate mean values for GPP for each group.\n\n\nHint: Chose an appropriate visualisation that maps the hour-of-day to the x-axis and the day-of-year to the y-axis.\n\n\n\nTrend in carbon dioxide concentrations\nThis exercise explores the longest available atmospheric CO\\(_2\\) record, obtained at the Mauna Loa observatory in Hawaii. Atmospheric CO\\(_2\\) in the northern hemisphere is characterised by seasonal swings, caused by the seasonal course of CO\\(_2\\) uptake and release by the terrestrial biosphere. We’ve explored the seasonality of the CO\\(_2\\) uptake measured at one site (in Switzerland) extensively in this an previous chapters. Your task here is to calculate and visualise the long-term trend of CO\\(_2\\). Follow these steps:\n\nDownload the monthly mean CO2\\(_2\\) data as a CSV file from here and read it into R.\nMake a simple graph for visualizing the monthly CO\\(_2\\) time series.\nWrite a function that computes a 12-month running mean of the CO\\(_2\\) time series. The running mean for month \\(m\\) should consider values of \\(m-5\\) to \\(m+6\\). Define arguments for the function that let the user specify the width of the running mean “box” (i.e., setting the \\(5\\) and \\(6\\) to any other integer of choice)\nMake a publication-ready figure that shows the monthly and the 12-month running mean time series of the CO\\(_2\\) record.\n\n\nHint: You don’t need to clean the .txt file by hand, find a suitable function in R.\n\n\nHint: Arguments to your function may be a vector of the original (monthly) data and a parameter defining the number of elements over which the mean is to be taken.\n\n\nHint: To automatically render the time axis with ggplot, you can create a time object by combining the year and month columns: lubridate::ymd(paste(as.character(year), \"-\", as.character(month), \"-15\"))",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_vis.html#report-exercises",
    "href": "data_vis.html#report-exercises",
    "title": "5  Data visualisation",
    "section": "5.5 Report Exercises",
    "text": "5.5 Report Exercises\n\nTelling a story from data\nIn the previous exercises and tutorials, you have learned how to wrangle data, fit simple linear regression models and identify outliers, create figures for temporal patterns, and develop and test hypotheses. Use these skills to analyze the airquality dataset (directly available in R, just type datasets::airquality into the console). The target variable of this dataset is the ozone concentration and your task is to tell a story about it. Look at the other variables in the dataset and become creative! Think of what patterns and relationships could be interesting to talk about. Your report must include the following elements:\n\nA description of the airquality dataset (where is it from, what are the variables’ units, etc.).\nA specific question that you want to answer through analyzing the data.\nAt least three statistical metrics from your dataset that aid you in answering your question (e.g., mean values, ranges, etc.).\nAt least three publishable figures or tables that show important relationships that aid you in answering your question (e.g., outliers, temporal patterns, scatterplots, etc.).\nMake sure to interpret and discuss your results and hypotheses. Why were you right / why were you wrong?\n\n\nImportant: The text alone should not exceed one A4 page (max. 400 words). Hint: Metrics, figures, tables, etc. without any written-out explanation what they show do not count.  Hint: To get more background information on the data, use the help functionalities in RStudio.\n\n\n\nDeliverables for the report\nFollowing the same requirements as mentioned in Section 4.6, present your solutions in a file called re_airquality.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data visualisation</span>"
    ]
  },
  {
    "objectID": "data_variety.html",
    "href": "data_variety.html",
    "title": "6  Data variety",
    "section": "",
    "text": "6.1 Learning objectives\nChapter lead author: Koen Hufkens\nAs a scientist you will encounter a variety of data (formats). In this section, you will learn some of the most common formats, their structure, and the advantages and disadvantages of using a particular data format. Only singular files are considered in this section, and databases are not covered although some files (formats) might have a mixed use.\nMore and more data moves toward a cloud server-based model where data is queried from an online database using an Application Programming Interface (API). Although the explicit use of databases is not covered, you will learn basic API usage to query data which is not represented as a file.\nIn this chapter you will learn:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data variety</span>"
    ]
  },
  {
    "objectID": "data_variety.html#learning-objectives",
    "href": "data_variety.html#learning-objectives",
    "title": "6  Data variety",
    "section": "",
    "text": "how to recognize data and file formats\nunderstand data and file format limitations\n\nmanipulation wise\ncontent wise\n\nhow to read and/or write data in a particular file format\nhow to query an API and store its data locally",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data variety</span>"
    ]
  },
  {
    "objectID": "data_variety.html#setup",
    "href": "data_variety.html#setup",
    "title": "6  Data variety",
    "section": "6.2 Setup",
    "text": "6.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data variety</span>"
    ]
  },
  {
    "objectID": "data_variety.html#tutorial",
    "href": "data_variety.html#tutorial",
    "title": "6  Data variety",
    "section": "6.3 Tutorial",
    "text": "6.3 Tutorial\n\n6.3.1 Files and file formats\n\n6.3.1.1 File extensions\nIn order to manipulate data and make some distinctions on what a data file might contain, files carry a particular file format extension. These file extensions denote the intended content and use of a particular file.\nFor example, a file ending in .txt suggests that it contains text. A file extension allows you, or a computer, to anticipate the content of a file without opening the file.\nFile extensions are therefore an important tool in assessing what data you are dealing with, and what tools you will need to manipulate (read and write) the data.\n\nNOTE: File extensions can be changed. In some cases, the file extension does not represent the content of the data contained within the file.\n\n\nTIP: If a file doesn’t read, it is always wise to try to open the file in a text editor and check the first few lines to verify if the data has a structure that corresponds to the file extension.\n# On a linux/macos system you can use the terminal command (using \n# the language bash) to show the first couple lines of a file\nhead your_file\n\n# alternatively you can show the last few lines\n# of a file using\ntail your_file\n\n\n\n6.3.1.2 Human-readable data\nOne of the most important distinctions in data formats falls along the line of it being human-readable or not. Human-readable data is, as the term specifies, made up of normal text characters. Human-readable text has the advantage that it is easily read and edited using conventional text editors. This convenience comes at the cost of the files not being compressed in any way, and file sizes can become unnecessarily large. However, for many applications where file sizes are limited (&lt;50MB), human-readable formats are the preferred option. Most human-readable data falls in two broad categories, tabular data and structured data.\n\nTabular data\nOften, human-readable formats provide data in tabular form using a consistent delimiter. This delimiter is a character separating columns of a table.\ncolumn_one, column_two, column_three\n1, 2, 3\n1, 2, 3\n1, 2, 3\nCommon delimiters in this context are the comma (,), as shown in the above example. A file with this particular format often carries the comma-separated values file extension (*.csv). Other delimiters are the tabulation (tab) character. Files with tab delimited values have the *.tsv format.\n\nTIP: File extensions aren’t always a true indication of the delimiter used. For example, .txt files often contain comma or tab separated data. If reading a file using a particular delimiter fails it is best to check the first few lines of a file.\n\n\n\nStructured data\nTabular data is row and column-oriented and therefore doesn’t allow complex structured content, e.g. tables within tables. This issue is sidestepped by the JSON format. The JSON format uses attribute-value pairs to store data, and is therefore more flexible in terms of accommodating varying data structures. Below, you see an example of details describing a person, with entries being fo varying length and data types.\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"isAlive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"streetAddress\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postalCode\": \"10021-3100\"\n  },\n  \"phoneNumbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n      \"Catherine\",\n      \"Thomas\",\n      \"Trevor\"\n  ],\n  \"spouse\": null\n}\n\nNOTE: Despite being human-readable, a JSON file is considerably harder to read than a comma separated file. Editing such a file is therefore more prone to errors if not automated.\n\nOther human-readable structured data formats include the eXtensible Markup Language (XML), which is commonly used in web infrastructure. XML is used for storing, transmitting, and reconstructing arbitrary data but uses (text) markup instead of attribute-value pairs.\n&lt;note&gt;\n&lt;to&gt;Tove&lt;/to&gt;\n&lt;from&gt;Jani&lt;/from&gt;\n&lt;heading&gt;Reminder&lt;/heading&gt;\n&lt;body&gt;Don't forget me this weekend!&lt;/body&gt;\n&lt;/note&gt;\n\n\n\n6.3.1.3 Writing and reading human-readable files in R\nThere are a number of ways to read human-readable formats into an R work environment. Here the basic approaches are listed, in particular reading CSV and JSON data.\nLarge volumes of data are available as CSV files or similar. Understanding how to read in such data into a programming environment is key. In this context the read.table() function is a general purpose tool to read in text data. Depending on the format, additional meta-data or comments, and certain parameters need to be specified.\nIts counterpart is a function to write human-readable data to file, called - you guessed it - write.table(). Again, parameters are required for maximum control over how things are written to file, by default though data are separated by a single empty space ” “, not a comma.\n\nNote: In Chapter 4, we used the {readr} (tidyverse) function read_csv(). It serves the same purpose as read.csv(), but is faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. For particularly large data, you may consider even better solutions for fast reading, see here.\n\nBelow, you find and example in which a file is written to a temporary location, and read in again using the above mentioned functions.\n\n# create a data frame with demo data\ndf &lt;- data.frame(\n  col_1 = c(\"a\", \"b\", \"c\"),\n  col_2 = c(\"d\", \"e\", \"f\"),\n  col_3 = c(1,2,3)\n)\n\n# write table as CSV to disk\nwrite.table(\n  x = df,\n  file = file.path(tempdir(), \"your_file.csv\"),\n  sep = \",\",\n  row.names = FALSE\n)\n\n# Read a CSV file\ndf &lt;- read.table(\n  file.path(tempdir(), \"your_file.csv\"),\n  header = TRUE,\n  sep = \",\"\n)\n\n# help files of both functions can be accessed by\n# typing ?write.table or ?read.table in the R console\n\nIn this example, a data frame is generated with three columns. This file is then written to a temporary file in the temporary file directory tempdir(). Here, tempdir() returns the location of the temporary R directory, which you can use to store intermediate files.\nWe use the file.path() function to combine the path (tempdir()) with the file name (your_file.csv). Using file.path() is good practice as directory structures are denoted differently between operating systems, e.g., using a backslash (\\) on Windows vs. a slash (/) on Unix-based systems (Linux/macOS). The file.path() function ensures that the correct directory separator is used.\nNote that in this command, we have to manually set the separator (sep = \",\") and whether a header is present (header = TRUE). Depending on the content of a file, you will have to alter these parameters. Additional parameters of the read.table() function allow you to specify comment characters, skip empty lines, etc.\nSimilar to this simple CSV file, we can generate and read JSON files. For this, we do need an additional library, as default R install does not provide this capability. However, the rest of the example follows the above workflow.\n\n# we'll re-use the data frame as generated for the CSV\n# example, so walk through the above example if you\n# skipped ahead\n\n# load the library\nlibrary(\"jsonlite\")\n\n\n# write the file to a temporary location\njsonlite::write_json(\n  x = df,\n  path = file.path(tempdir(), \"your_json_file.json\")\n)\n\n\n# read the freshly generated json file\ndf_json &lt;- jsonlite::read_json(\n  file.path(tempdir(), \"your_json_file.json\"),\n  simplifyVector = TRUE\n)\n\n# check if the two data sets\n# are identical (they should be)\nidentical(df, df_json)\n\n[1] TRUE\n\n\nNote that the reading and writing JSON data is easier, as the structure of the data (e.g., field separators) are more strictly defined. While reading the data, we use the simplifyVector argument to return a data frame rather than a nested list. This works as our data has a tabular structure, but this might not always be the case. Finally, we compare the original data with the data read in using identical().\n\nTIP: In calling the external library we use the :: notation. Although by loading the library with library() makes all jsonlite functions available, the explicit referencing of the origin of the function makes debugging often easier.\n\n\n\n6.3.1.4 Binary data\nAll digital data which is not represented as text characters can be considered binary data. Binary data can vary in its content from an executable, which runs a program, to the digital representation of an image (jpeg images). However, in all cases, the data is represented as bytes (made of eight bits) and not text characters.\nOne of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software, other than a text editor, to manipulate the data. For example, digital images in a binary format require image manipulation software.\nMore so than human-readable data, the file format (extension) determines how to treat the data. Knowing common data formats and their use cases is therefore key.\n\n\n6.3.1.5 Common file formats\nEnvironmental sciences have particular file formats which dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Here we will list some of the most common formats you will encounter.\n\n\n\nFile format (extension)\nFormat description\nUse case\nR Library\n\n\n\n\n*.csv\ncomma separated tabular data\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.txt\ntabular data with various delimiters\nGeneral purpose flat files with row and column oriented data\nbase R\n\n\n*.json\nstructured human-readable data\nGeneral purpose data format. Often used in web application. Has geospatial extensions (geojson).\njsonlite\n\n\n*.nc\nNetCDF data array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data.\nncdf4, terra, raster\n\n\n*.hdf\nHDF array data\nArray-oriented data (matrices with &gt; 2 dimensions). Commonly used to store Earth observation data.\nhdf\n\n\n*.tiff, *.geotiff\nGeotiff multi-dimensional raster data (see below)\nLayered (3D) raster (image) data. Commonly used to represent spatial (raster) data.\nterra, raster\n\n\n*.shp\nShapefile of vector data (see below)\nCommon vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values.\nsp, sf\n\n\n\n\n\n\n6.3.2 Meta-data\nMeta-data is data that is associated with the main data file and is key to understanding the file content and the context of the data. In some cases, you will find this data only as a general description referencing the file(s) itself. In other cases, meta-data is included in the file itself.\nFor example, many tabular CSV data files contain a header specifying the content of each column, and at times a couple of lines of data specifying the content of the file itself - or context within which the data should be considered.\n# This is meta-data associated with the tabular CSV file\n# for which the data is listed below.\n# \n# In addition to some meta-data, the first row of the data\n# contains the column header data\ncolumn_one, column_two, column_three\n1, 2, 3\n1, 2, 3\n1, 2, 3\nIn the case of binary files it will not be possible to read the meta-data directly as plain text. In this case, specific commands can be used to read the meta-data included in a file. The example below shows how you would list the meta-data of a GeoTiff file using the bash.\n# list geospatial data for a geotiff file\ngdalinfo your_geotiff.tiff\n\nTIP: Always keep track of your meta-data by including it, if possible, in the file itself. If this is not possible, meta data is often provided in a file called README. Meta-data is key in making science reproducible and guaranteeing consistency between projects. Key meta-data to retain are:\n\nthe source of your data (URL, manuscript, DOI)\nthe date when the data was downloaded\nmanipulations on the data before using the data in a final workflow\n\n\nMeta-data of data read into R can be accessed by printing the object itself, i.e., calling the object in the console. If it is a simple table, the first lines of the table will be shown. If it is a more complex object, the meta data will be output as a formatted statement. You can also use the str() or summary() functions to summarize data and meta-data.\n\n\n6.3.3 Spatial data representation\nEnvironmental data often has an explicit spatial and temporal component. For example, climate data is often represented as 2D maps which vary over time. This spatial data requires an additional level of understanding of commonly used data formats and structures.\nIn general, we can distinguish two important data models when dealing with spatial data, the raster and vector data model. Both data have their typical file formats (see above) and particular use cases. The definition of these formats, optimization of storage and math/logic on such data are the topic of Geographic Information System (GIS) science and beyond the scope of this course. We refer to other elective GIS courses for a greater understanding of these details. However, a basic understanding of both raster and vector data is provided here.\n\n6.3.3.1 Raster data model\nThe basic raster model represents geographic (2D) continuous data as a two-dimensional array, where each position has a geographic (x, y) coordinate, a cell size (or resolution) and a given extent. Using this definition, any image adheres to the raster model. However, in most geographic applications, coordinates are referenced and correspond to a geographic position, e.g., a particular latitude and longitude. Often, the model is expanded with a time dimension, stacking various two-dimensional arrays into a three-dimensional array.\nThe raster data model is common for all data sources which use either imaging sensors, such as satellites or unmanned aerial vehicles (UAVs), or outputs of models that operate on a cartesian grid, including most climate and numerical weather prediction models.\nAdditional meta data stores both the geographic reference system, the definition and format of the time information, and well as other data which might be helpful to end users (e.g., variable units). Within the environmental sciences, NetCDF and GeoTiff are common raster data file formats.\n\n\n6.3.3.2 Vector data model\nThe vector data model, in contrast to the raster data model, describes (unbound) features using a geometry (location, shape) using coordinates and linked feature attributes. Geometries can be points, lines, polygons, or even volumes.\nVector data does not have a defined resolution, making them scale-independent. This makes the vector data model ideal for discrete features such as roads or building outlines. Conversely, vector data is poorly suited for continuous data.\nConversions between the vector and raster model are possible, but limitations apply. For example, when converting vector data to raster data a resolution needs to be specified, as you lose scale independence of the vector format. Conversions from raster to vector are similarly limited by the original resolution of the raster data. In this course we will focus on raster data only, the most common format within the context of data science.\n\n\n\n6.3.4 Online data sources\nThe sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time, gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() R function to grab data.\nToday, many of the data described in previous sections are warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services has become key in gathering research data.\n\n6.3.4.1 Direct downloads\nBefore diving into a description of APIs, we remind you that some file reading functions in R are web-aware, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment.\nAlthough using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source.\n\n# define a URL with data of interest\n# in this case annual mean CO2 levels at Mauna Loa\nurl &lt;- \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv\"\n\n# read in the data directly from URL\ndf &lt;- read.table(\n  url,\n  header = TRUE,\n  sep = \",\"\n)\n\n\n\n6.3.4.2 APIs\nWeb-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL).\nTo reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead.\n\nDedicated API libraries\nAs an example of a dedicated library, we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive.\n\n# load the library\nlibrary(\"MODISTools\")\n\n# list all available products\nproducts &lt;- MODISTools::mt_products()\n\n# print the first few lines\n# of available products\nprint(head(products))\n\n       product\n1       Daymet\n2 ECO4ESIPTJPL\n3      ECO4WUE\n4       GEDI03\n5     GEDI04_B\n6      MCD12Q1\n                                                                         description\n1 Daily Surface Weather Data (Daymet) on a 1-km Grid for North America, Version 4 R1\n2               ECOSTRESS Evaporative Stress Index PT-JPL (ESI) Daily L4 Global 70 m\n3                          ECOSTRESS Water Use Efficiency (WUE) Daily L4 Global 70 m\n4                GEDI Gridded Land Surface Metrics (LSM) L3 1km EASE-Grid, Version 2\n5     GEDI Gridded Aboveground Biomass Density (AGBD) L4B 1km EASE-Grid, Version 2.1\n6              MODIS/Terra+Aqua Land Cover Type (LC) Yearly L3 Global 500 m SIN Grid\n  frequency resolution_meters\n1     1 day              1000\n2    Varies                70\n3    Varies                70\n4  One time              1000\n5  One time              1000\n6    1 year               500\n\n# download a demo dataset\n# specifying a location, a product,\n# a band (subset of the product)\n# and a date range and a geographic\n# area (1 km above/below and left/right).\n# Data is returned internally and the\n# progress bar of the download is not shown.\nsubset &lt;- MODISTools::mt_subset(\n  product = \"MOD11A2\",\n  lat = 40,\n  lon = -110,\n  band = \"LST_Day_1km\",\n  start = \"2004-01-01\",\n  end = \"2004-02-01\",\n  km_lr = 1,\n  km_ab = 1,\n  internal = TRUE,\n  progress = FALSE\n)\n\n# print the dowloaded data\nprint(head(subset))\n\n      xllcorner  yllcorner         cellsize nrows ncols        band  units\n1.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n3.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n4.1 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n1.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n2.2 -9370963.05 4445948.79 926.625433055834     3     3 LST_Day_1km Kelvin\n    scale latitude longitude     site product      start        end complete\n1.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n3.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n4.1  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n1.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n2.2  0.02       40      -110 sitename MOD11A2 2004-01-01 2004-02-01     TRUE\n    modis_date calendar_date   tile     proc_date pixel value\n1.1   A2004001    2004-01-01 h09v05 2020168005635     1 13148\n2.1   A2004009    2004-01-09 h09v05 2020168010833     1 13160\n3.1   A2004017    2004-01-17 h09v05 2020168012220     1 13398\n4.1   A2004025    2004-01-25 h09v05 2020168013617     1 13412\n1.2   A2004001    2004-01-01 h09v05 2020168005635     2 13153\n2.2   A2004009    2004-01-09 h09v05 2020168010833     2 13140\n\n\nA detailed description of all functions of the {MODISTools} R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, as requiring parameters, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below).\n\n\nGET\nDepending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url).\n\n# formulate a named list query to pass to httr\nquery &lt;- list(\n  \"argument\" = \"2\",\n  \"another_argument\" = \"3\"\n)\n\n# The URL of the API (varies per product / param)\nurl &lt;- \"https://your.service.endpoint.com\"\n\n# download data using the\n# API endpoint and query data\n# status variable will include if\n# the download was successful or not\n# the write_disk() function captures\n# data if available and writes it to\n# disk\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = \"/where/to/store/data/filename.ext\",\n    overwrite = TRUE\n  )\n)\n\nBelow, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND).\n\n# set API URL endpoint\n# for the total sand content\nurl &lt;- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4\"\n\n# formulate query to pass to httr\nquery &lt;- list(\n  \"var\" = \"T_SAND\",\n  \"south\" = 32,\n  \"west\" = -81,\n  \"east\" = -80,\n  \"north\" = 34,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SAND.nc\"),\n    overwrite = TRUE\n  )\n)\n\n# to visualize the data\n# we need to load the {terra}\n# library\nlibrary(\"terra\")\n\nterra 1.8.21\n\nsand &lt;- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nterra::plot(sand)\n\n\n\n\n\n\n\n\n\n\nAuthentication\nDepending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return.\n\n# an authenticated API query\nstatus &lt;- httr::POST(\n  url = url,\n  httr::authenticate(user, key),\n  httr::add_headers(\"Accept\" = \"application/json\",\n                    \"Content-Type\" = \"application/json\"),\n  body = query,\n  encode = \"json\"\n)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data variety</span>"
    ]
  },
  {
    "objectID": "data_variety.html#exercises",
    "href": "data_variety.html#exercises",
    "title": "6  Data variety",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\nFiles and file formats\n\nReading and writing human-readable files\nWhile not leaving your R session, download and open the files at the following locations:\nhttps://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_1.csv\nhttps://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_2.csv\nhttps://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_3.csv\nOnce loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory.\n\n\nReading and writing binary files\nDownload and open the following file:\nhttps://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc\n\nWhat file format are we dealing with?\nWhat library would you use to read this kind of data?\nWhat does this file contain?\nWrite this file to disk in a different geospatial format you desire (use the R documentation of the library used to read the file and the chapter information).\nDownload and open the following file: https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.tif. Does this data seem familiar, and how can you tell? What are your conclusions?\n\n\n\n\nAPI use\n\nGET\n\nDownload the HWSD total sand content data for the extent of Switzerland following the tutorial example. Visualize/plot the data as a simple map.\nDownload the HWSD topsoil silt content for the extent of Switzerland.\n\n\n\n6.4.0.1 Dedicated library\n\nUse the {hwsdr} library (a dedicated package for the API) to download the same data. How does this compare to the previous code written?\nList how many data products there are on the ORNL MODIS data repository.\nDownload the MODIS land cover map for the canton of Bern.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data variety</span>"
    ]
  },
  {
    "objectID": "open_science.html",
    "href": "open_science.html",
    "title": "7  Open science practices",
    "section": "",
    "text": "7.1 Learning objectives\nChapter lead author: Koen Hufkens\nIn this chapter, you will learn the reasons for practicing open science and some of the basic methodological techniques that we can use to facilitate an open science workflow.\nIn this chapter you will learn how to:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open science practices</span>"
    ]
  },
  {
    "objectID": "open_science.html#sec-learning-objectives-6",
    "href": "open_science.html#sec-learning-objectives-6",
    "title": "7  Open science practices",
    "section": "",
    "text": "structure a project\nmanage a project workflow\ncapture a session or machine state\nuse dynamic reporting\nensure data and code retention",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open science practices</span>"
    ]
  },
  {
    "objectID": "open_science.html#tutorial",
    "href": "open_science.html#tutorial",
    "title": "7  Open science practices",
    "section": "7.2 Tutorial",
    "text": "7.2 Tutorial\nThe scientific method relies on repeated testing of a hypothesis. When dealing with data and formal analysis, one can reduce this problem to the question: could an independent scientist attain the same results given the described methodology, data and code?\nAlthough this seems trivial, this issue has vexed the scientific community. These days, many scientific publications are based on complex analyses with often large data sets. More so, methods in publications are often insufficiently detailed to really capture the scope of an analysis. Even from a purely technical point of view, the reproducibility crisis or the inability to reproduce experimental results, is a complex problem. This is further compounded by social aspects and incentives. In recent decades, scientific research has seen a steady increase in speed due to the digitization of many fields and the commodification of science.\nAlthough digitization has opened up new research possibilities, its potential for facilitating, accelerating, and advancing science are often not fully made use of. Historically, research and its output in the form of data and code has been confined to academic journals, data and code has often not been made available, and data were not shared or behind pay-walls. This limits the impact of science also in the public domain. (In many ways, this is still the case today, in year 2023 as we write). Digitization has made research output better visible and accessible, but practical obstacles and weak standards often prevent it from uptake, re-use, and further development by the wider community.\nOpen and reproducible science is a movement to make scientific research (output) widely accessible to the larger public, increase research transparency and enabling robust and verifiable science. Open science aims to be as open as possible about the whole scientific process, and as closed as desirable (e.g. privacy or security reasons).\nIt is important to acknowledge that there is a spectrum of reproducible data and code workflows which depends on the state or source of the data and the output of the code (or analysis). Within the context of this course, we focus primarily on the practical aspects for reproducible science, i.e., ensuring that given the same data and code, the results will be similar.\n\n\n\n\n\n\nFigure 7.1: The reproducibility matrix by The Turing Way.\n\n\n\nThe basics of open science coding and data practices rely on a number of simple concepts. The sections below describe a selection of the most important ones. Sticking to these principles and tools will increase the reproducibility of your work greatly.\n\n7.2.1 Project structure\nReproducible science relies on a number of key components. Data and code management and the tracking of required meta-data is the first step in an open science workflow.\nIn Chapter 2, you learned how to setup an R project. An R project gathers all components of your analysis in a single directory. Although current computers make it easy to “find” your files and are largely file location-agnostic, this is not the case in many research environments. Projects grow quickly, and often, the number of files will flood a single directory. Therefore, files need a precise and structured location. This structure allows you to determine both the function and order of a workflow without reading any code.\nIt is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structuring a project in one folder also makes projects portable. All parts reside in one location making it easy to create a git project from this location (see Chapter 8), or just copy the project to a new drive.\nAn example data structure for raw data processing is given below and we provide an R project template to work from and adjust through our lab GitHub profile. A full description on using the template is provided in Chapter 8.\ndata-raw/\n├─ raw_data_product/\n├─ 00_download_raw_data.R\n├─ 01_process_raw_data.R\n\n\n7.2.2 Managing workflows\nAlthough some code is agnostic to the order of execution, many projects are effectively workflows, where the output of one routine is required for the successful execution of the next routine.\nIn order to make sure that your future self, or a collaborator, understands the order in which things should be executed, it is best to number scripts accordingly. This is the most basic approach to managing workflows.\nIn the example below, all statistics code is stored in the statistics folder in an overall analysis folder (which also includes code for figures). All statistical analyses are numbered to ensure that the output of a first analysis is available to the subsequent one.\nanalysis/\n├─ statistics/\n│  ├─ 00_randomforest_model.R\n│  ├─ 01_randomforest_tuning.R\n├─ figures/\n│  ├─ global_model_results_map.R\n│  ├─ complex_process_visualization.R\nThe code-chunk above is a visualisation of a folder (aka. directory) structure on your computer. The lines and indents denote folder levels. In this example, you have a folder analysis which holds two more folders statistics and figures, and in both sub-folders, you have different *.R files (* is a so-called “wild-card” which is a placeholder for any text). Note that different people may use different symbols to visualise folder structures but generally, folder levels are shown with indents, and files are identifiable by their suffixes.\n\n7.2.2.1 Automating and visualizing workflows with targets\nTo sidestep some of the manual management in R you can use a dedicated pipeline tool like the {targets} package in R. The package learns how your pipeline fits together, skips tasks that are already up-to-date, and runs only the necessary computation. {targets} can also visualize the progress of your workflow.\n\n\n\n\n\n\nFigure 7.2: A targets visualized workflow by rOpenSci.\n\n\n\nDue to the added complexity of the {targets} package, we won’t include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples here.\n\n\n\n7.2.3 Capturing your session state\nOften, code depends on various components, packages or libraries. These libraries and all software come in specific versions, which might or might not alter the behaviour of the code and the output it produces.\nIf you want to ensure full reproducibility, especially across several years, you will need to capture the state of the system and libraries with which you ran the original analysis.\nIn R the {renv} package serves this purpose and will provide an index of all the packages used in your project as well as their version. For a particular project, it will create a local library of packages with a static version. These static packages will not be updated over time, and therefore ensure consistent results. This makes your analysis isolated, portable, and reproducible. The analogue in Python would be the virtual environments, or venv program.\nWhen setting up your project you can run:\n# Initiate a {renv} environment\nrenv::init()\nTo initiate your static R environment. Whenever you want to save the state of your project (and its packages) you can call:\n# Save the current state of the environment / project\nrenv::snapshot()\nTo save any changes made to your environment. All data will be saved in a project description file called a lock file (i.e. renv.lock). It is advised to update the state of your project regularly, and in particular before closing a project.\nWhen you move your project to a new system, or share a project on github with collaborators, you can revert to the original state of the analysis by calling:\n# On a new system, or when inheriting a project\n# from a collaborator you can use a lock file\n# to restore the session/project state using\nrenv::restore()\n\nNOTE: As mentioned in the {renv} documentation: “For development and collaboration, the .Rprofile, renv.lock and renv/activate.R files should be committed to your version control system. But the renv/library directory should normally be ignored. Note that renv::init() will attempt to write the requisite ignore statements to the project .gitignore.” We refer to Section 7.1 for details on github and its use.\n\n\n\n7.2.4 Capturing a system state\nAlthough R projects and the use of {targets} make your workflow consistent, the package versions used between various systems (e.g., your home computer or the cluster at the university might vary). To address issues with changes in the versions of package, you can use the {renv} package which manages package version (environments) for you. When tasks are even more complex and include components outside of R, you can use Docker to provide containerization of an operating system and the included ancillary application.\nThe {rocker} package provides access to some of these features within the context of reproducible R environments. Using these tools, you can therefore emulate the state of a machine, independently of the machine on which the docker file is run. These days, machine learning applications are often deployed as docker sessions to limit the complexity of installing required software components. The application of docker-based installs is outside the scope of the current course, but feel free to explore these resources as they are widespread in data science.\n\n\n7.2.5 Readable reporting using Rmarkdown\nWithin Rstudio, you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for reporting, i.e., writing your final document and presenting your analysis results. A Rmarkdown document consists of a header that specifies document properties (whether it should be rendered as an html page, a docx file or a pdf), and the actual content. You have encountered RMarkdown already in Section 2.2.6.\n\n\n7.2.6 Project structure\nIn R projects, all files can be referenced relative to the top-most path of the project. When opening your_project.Rproj in RStudio, you can load data that is located in a sub-directory of the project directory ./data/ by read.table(\"./data/some_data.csv\"). The use of relative paths and consistent directory structures across projects, enables that projects can easily be ported across computers and code adopted across projects.\nproject/\n├─ your_project.Rproj\n├─ vignettes/\n│  ├─ your_dynamic_document.Rmd\n├─ data/\n│  ├─ some_data.csv\nRmarkdown files commonly reside in a sub-directory ./vignettes/ and are rendered relative to the file path where it is located. This means that to access data which resides in data/ using code in ./vignettes/your_dynamic_document.Rmd, we would have to write:\n\ndata &lt;- read.table('../data/some_data.csv')\n\nNote the ../ to go one level up. To allow for more flexibility in file locations within your project folder, you may use the {here} package package. I gathers the absolute path of the R project and allows for a specification of paths inside scripts and functions that always start from the top project directory, irrespective of where the script that implements the reading-code is located. Therefore, we can just write:\n\ndata &lt;- read.table(here::here('data/some_data.csv'))\n\nBut why not use absolute paths to begin with? Portability! When I would run your \\*.Rmd file with an absolute path on my computer, it would not render as the file some_data.csv would then be located at: /my_computer/project/data/some_data.csv\n\n7.2.6.1 Limitations of notebooks\nThe file referencing issue and the common use of Rmarkdown, and notebooks in general, as a one size fits all solution, containing all aspects from data cleaning to reporting, implies some limitations. RMarkdown documents mix two cognitive tasks, writing text content (i.e. reporting) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa.\nIf your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your RMarkdown file contains more text than code, it probably is easier to collaborate on a true word processing file (or cloud-based solution). Notebooks, such as RMarkdown, are most suitable for communicating implementations, demonstrating functions, and reporting reproducible results. They can also be used like lab notes. They are less suited for code development.\n\n\n\n7.2.7 Data retention\nCoding practices and documenting all moving parts in a coding workflow is only one practical aspect of open science. An additional component is long-term data and code retention and versioning.\nIn Chapter 8, you will learn more about git for code management and collaboration. Several online make use of git for providing web-based collaboration functionalities and remote storage of your repositories. Examples are GitHub, GitLab, Codeberg, or Bitbucket. However, their remote storage service should only be considered an aid for collaboration, and not a place to store code into perpetuity. Furthermore, these services mostly have a limit to how much data can be stored in a repository (mostly ~2 GB). For small projects, data can be included in the repository itself. For larger projects and for making larger datasets accessible, this won’t be possible.\nTo ensure long-term storage of code and data, outside of commercial for profit services (e.g., Dropbox, Google Drive etc), it is best to rely on public permanent repositories, such as Zenodo. Zenodo is an effort by the European commission, but accessible to all, to facilitate archiving of science projects of all nature (code and data) up to 50 GB. In addition, Zenodo provides a citable digital object identifier or DOI. This allows data and code, even if not formally published in a journal, to be cited. Other noteworthy open science storage options include Dryad and the Center for Open Science.\nThe broad-purpose permanent data repositories mentioned above are not edited and are therefore not ideal for data discovery. In contrast, edited data repositories often have a specific thematic scope and different repositories are established in different research communities. Below you find a list of widely used data repositories, generalist and others, that provide manual or automated download access to their data. Note that this list contains some example and is far from extensive.\n\n\n\nData type\nWebsite\nDescription\nDownload\n\n\n\n\nCopernicus Climate Data Store\nhttps://cds.climate.copernicus.eu\nFreely available climate data (reanalysis as well as future projections)\nAPI\n\n\nOak Ridge National Laboratories Digital Active Archive Center (ORNL DAAC)\nhttps://daac.ornl.gov/\nEnvironmental data of varying sources, either remote sensing, field work and or re-analysis.\nmultiple APIs or manual downloads\n\n\nLand Processes Digital Active Archive Center (LP DAAC)\nhttps://lpdaac.usgs.gov/\nRemote sensing (analysis ready) data products.\nLogin walled automated downloads\n\n\nEnvironmental Data Initiative\nhttps://edirepository.org/\nGeneralist data repository for study data, with a strong focus on biology.\nManual download\n\n\nDryad\nhttps://datadryad.org\nGeneralist data repository for study data, with a strong focus on biology.\nManual downloads\n\n\nZenodo\nhttps://zenodo.org/\nGeneralist data repository for study data.\nManual downloads\n\n\nEurostat\nhttps://ec.europa.eu/eurostat\nGeneralist data repository for EU wide (demographic) data.\nManual downloads\n\n\nSwiss Open Government data\nhttps://opendata.swiss/en/\nGeneralist data repository from the Swiss Federal statistics office.\nAPI or manual downloads",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open science practices</span>"
    ]
  },
  {
    "objectID": "open_science.html#exercises",
    "href": "open_science.html#exercises",
    "title": "7  Open science practices",
    "section": "7.3 Exercises",
    "text": "7.3 Exercises\n\nExternal data\nYou inherit a project folder which contains the following files.\n~/project/\n├─ survey.xlsx\n├─ xls conversion.csv\n├─ xls conversion (copy 1).csv\n├─ Model-test_1.R\n├─ Model-test-final.R\n├─ Plots.R\n├─ Figure 1.png\n├─ test.png\n├─ Rplot01.png\n├─ Report.Rmd\n├─ Report.html\n├─ my_functions.R\nWhat are your steps to make this project more reproducible? Write down how and why you would organize your project.\n\n\nA new project\nWhat are the basic steps to create a reproducible workflow from a file management perspective? Create your own R project using these principles and provide details the on steps involved and why they matter.\nThe project should be a reproducible workflow:\n\nDownload and plot a MODIS land cover map for Belgium using skills you learned in Chapter 6.\nWrite a function to count the occurrences of land cover classes in the map as a formal function using skills you learned in Chapter 4.\nCreate a plot of the land cover map, see Chapter 5.\nWrite a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow.\n\n\n\nTracking the state of your project\n\nTrack the packages you use in the project you created using {renv}.\nInstall any additional library and update the state of your project.\nCreate a simple {targets} project using the above workflow\n\nMake changes to the API download routine.\nRerun the targets project.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open science practices</span>"
    ]
  },
  {
    "objectID": "code_management.html",
    "href": "code_management.html",
    "title": "8  Code management",
    "section": "",
    "text": "8.1 Learning objectives\nChapter lead author: Koen Hufkens\nIn this chapter you will learn how to manage your code with common version control tools, i.e., git.\nYou will learn how to:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code management</span>"
    ]
  },
  {
    "objectID": "code_management.html#learning-objectives",
    "href": "code_management.html#learning-objectives",
    "title": "8  Code management",
    "section": "",
    "text": "Create a git project (new or from a template)\nTrack changes in your code project\nCollaborate with others\nEnsure reproducibility of your project by openly sharing your work and progress.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code management</span>"
    ]
  },
  {
    "objectID": "code_management.html#tutorial",
    "href": "code_management.html#tutorial",
    "title": "8  Code management",
    "section": "8.2 Tutorial",
    "text": "8.2 Tutorial\nCode management is key for managing any data science project, especially when collaborating. Proper code management limits mistakes, such as code loss, and increases efficiency by structuring projects.\nIn this chapter, we will discuss the management of code in both the location sense, where things are kept, and tracking temporal changes over time using a version control system.\nCurrent version control of code is dominated by the software tool git. However, version control has a long history and can be found not only in code development practices. For example, whenever you use track changes in a text document, you apply a form of version control. That is, you track changes in your text over time and selectively accept changes. In this respect, git, as a tool for version control of code, does not differ much from track changes of a text document. In contrast to track changes in a text document, with git the user has manual control over staging and committing edits on a file.\n\n\n\n\n\n\nFigure 8.1: The git workflow- by Paola Corrales and Elio Campitelli\n\n\n\nGit allows for the collaboration of multiple contributors on the same code and manages the integration of contributions into the repository. Built upon git are cloud-based platforms such as GitHub, GitLab, Codeberg, or Bitbucket which make these collaborative decisions and operations even easier.\n\n\n\n\n\n\nFigure 8.2: The github remote workflow- by Paola Corrales and Elio Campitelli\n\n\n\nIn this chapter, you will learn how to use git and GitHub to manage your project and collaborate on code.\n\nNOTE: Coding style, and documentation practices of the code itself have been covered previously in Chapter 3. Although the tutorial below focuses on GitHub, the jargon and operations are transferable to other platforms such as GitLab and Codeberg.\n\n\n8.2.1 Git and local version control\nGit allows for the tracking of changes in code (or any file) within a git project. A git project is defined by the topmost directory in which a git project is created. For example, the following project is not tracked for changes using git.\nproject/\n├─ YOUR_PROJECT.Rproj\nYou can start tracking a project by initiating a local git repository using the following code in R. We’ll use the {usethis} package to make some of the setup a project easier.\n\nusethis::use_git()\n\nThis will create a git repository in your project. It will also create a .gitignore file which specifies which files NOT to track (even if asked to). In addition it will make an first commit.\n\n8.2.1.1 git add\nBefore we can track anything, we need to tell git which files to track. In git-speak, we stage the files. We therefore have to add them to an index of tracked files. You can either do this on the command line using:\ngit add your_file.csv\nOr using the RStudio Git panel. In this panel, you will see all un-tracked files or directories highlighted with a yellow question mark.\n\n\n\n\n\n\nFigure 8.3: Unstaged files in a git enabled R project.\n\n\n\nYou select the file tick boxes to the left to stage all files for inclusion into the git repository. Once staged, the next step will be to finally commit these staged files to be included in git tracking.\n\n\n\n\n\n\nFigure 8.4: Staged files in a git enabled R project.\n\n\n\n\n\n8.2.1.2 git commit\nTo store any changes to the files which were staged we need to commit these changes. We therefore hit the commit button. A new window will pop up.\n\n\n\n\n\n\nFigure 8.5: Entering a commit message.\n\n\n\nEach commit needs a brief message describing what you have included in the staged files, or the commit message, as shown in the panel on the right. You need to provide this small message before pressing the commit button once more. This will let git track the changes to these files. A message will be shown if the commit is successful.\n\n\n\n\n\n\nFigure 8.6: A completed commit.\n\n\n\nWith this, you will track all files locally. Any new changes to a file will need to be committed to the git repository once more. So, unlike cloud services such as Dropbox, your files are not automatically tracked. Instead, this is a manual step. As with normal documents, you are advised to save (commit) your changes to your project frequently. And remember, if you create a new file, you will need to add it before you can commit it.\nYou can commit changes of staged files using the command line as well using the following command.\ngit commit -m \"A message\"\"\n\n\n\n8.2.2 Remote version control\nKeeping files and working with git locally limit the extent in which you can collaborate with others. This is where remote cloud-based git solutions, such as GitHub, GitLab and Codeberg, come in. They provide a cloud-based git repository which you can associate with your local project (see figure above).\nTo create a remote project and successfully associate it with an R project, we first have to specify some details, such as the user name and email you used is singing up for GitHub. To not leave your R session, you can use the {usethis} package for this.\n# Configure your project\nlibrary(usethis)\nusethis::use_git_config(user.name = \"Jane Doe\",\n               user.email = \"jane@example.org\")\nFor security reasons, the use of your GitHub password is not allowed in remote actions. You therefore need to generate a personal access token (PAT) which can be restricted in time and functionality. To proceed, first generate a GitHub PAT using these instructions.\nTo create a new project on GitHub, hit the “+” sign top left on the GitHub main page (once logged in), and select the “new repository” from the dropdown menu.\n\n\n\n\n\n\nFigure 8.7: Create a new github repository.\n\n\n\nA new interface will open up in which you should not use any template, but specify your own project name and brief description. Make sure your project is public, and all other settings are kept as is before you hit the “Create repository” button.\n\n\n\n\n\n\nFigure 8.8: Set the github project name.\n\n\n\nNote the URL that is generated for your project. You will need it when creating a new RStudio project linked to GitHub.\n\n\n\n\n\n\nFigure 8.9: A repository link you need during the R project wizard.\n\n\n\nNext, we’ll setup an R project which is associated with the repository. Use: File &gt; New Project &gt; Version Control &gt; Git.\n\n\n\n\n\n\nFigure 8.10: Linking the github project to a new git enabled R project.\n\n\n\nIn the “repository URL”, paste the URL of your new GitHub repository. In the example above, this would be https://github.com/khufkens/YOUR_PROJECT.git. Select a location where to store the project, select the “Open in new session” option and click “Create Project”. A window will pop up, asking for your GitHub username and a password. This password is not your GitHub login password but the PAT described above. After entering your credential, RStudio creates a \\*.Proj file as well as a .gitignore file. You can add both files as you would otherwise (see Section 8.2.1.1, and these files are tracked locally.\n\n8.2.2.1 git push\nOnce a remote git service has been configured, you can push your local git repository to this remote repository, i.e. syncing both. You can use both the push buttons in the RStudio panel for this or the command line using git push. At the end of a day or a session, it is always advised to push your changes to your remote repository to store any changes.\n\nNOTE: Syncing between GitHub and your local repository is a manual task. If not performed the repository, it will not be synced. To retain all your changes, sync both repositories often!\n\n\n\n\n\n\n\nFigure 8.11: Remote git workflow - by Paola Corrales and Elio Campitelli\n\n\n\n\n\n8.2.2.2 git pull and merge conflicts\ngit pull compares your local git repository with the remote one and integrates the more recent changes if there are any. Note that if you make changes on the same line in file on both sides, i.e., in the remote (e.g., by another person) and in the local repository at the same time, you will generate a merge conflict. A merge conflict states that the remote and local changes can’t be reconciled without supervised intervention on your part. Changes will be made to your local repository, but the files will include the below syntax for highlighting conflicting differences.\n    &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n    Edited line from remote\n    ==========\n    Edited same line locally\n    &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nYou will need to remove the &lt;&lt;&lt;, === and &gt;&gt;&gt; brackets and retain the changes you want to keep for resolving the conflict. Then, commit the changes again.\n\n\n8.2.2.3 git clone\nYou can create a local copy of your remote repository that’s hosted on GitHub using the git clone command. For example, on the command line, you can use:\n# create a local copy of the remote github repository\ngit clone git@github.com:khufkens/YOUR_PROJECT.git\nYou can then start working on this repository by using the modify -&gt; staged -&gt; commit -&gt; push workflow.\n\n\n8.2.2.4 git fork and pull request\nYou can also create a copy of any public GitHub project from into your own GitHub account by creating a fork. You can create a fork of a project by clicking the fork button top right on any public GitHub project page. The number of forks of a project is displayed next to the button. In case of the {rpmodel} package there are currently 24 forks of the project.\n\n\n\n\n\n\nFigure 8.12: Creating a fork of an existing project (rpmodel)\n\n\n\nYou can give the forked project a new name and description if so desired.\n\n\n\n\n\n\nFigure 8.13: Github fork settings\n\n\n\nA fork allows you to experiment with the code stored in the original project without affecting the original repository. However, the relation to the original project is maintained. If you want to contribute changes to the original project you can do so with a pull request.\n\nNOTE: To make changes to a forked project, you will first have to clone it to your local system. See workflow above.\n\nIn a forked project, go to the Pull requests tab and click the green New pull request button. You will then have to provide a description of the changes you made. This information will be forwarded to the original owner of the project, who can accept these changes and accept the pull request and “pull” in the changes.\n\n\n\n\n\n\nFigure 8.14: Creating a new github pull request\n\n\n\n\n\n\n8.2.3 Location based code management - github templates\nBoth code (and data) management require you to be conscientious about where you store your code (and data). Structuring your projects using the same template will allow you to understand where all pieces of an an analysis are stored. This has been mentioned in Chapter 7.\nIn our R project template, we provide a project structure for both data and code which removes the mental overhead of structuring data projects. This project structure sorts code, data and reporting in a consistent way.\nYou can use the template in combination with a GitHub-based version control approach to manage your projects. Simply create a new project from this template and clone the project to your local computer. Any changes to the project can be tracked by the workflows described above.\nTo use the template, create a new repository on GitHub, as you otherwise would using the big green button. If you are in the project on GitHub, you can hit the green button top right (Use this template).\n\n\n\n\n\n\nFigure 8.15: Use a github project as a template\n\n\n\nOtherwise, you can select the repository from the template dropdown menu, select geco-bern/R-project-template.\n\n\n\n\n\n\nFigure 8.16: Using a new template based repository\n\n\n\nProceed as usual by naming your repository. However, be careful to select the correct owner of the project if you have multiple identities. Rename the default .Proj file.\n\n\n\n\n\n\nFigure 8.17: Assigning a new template based repository\n\n\n\nYou can now clone the project to your local computer and continue to populate it with code and data.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code management</span>"
    ]
  },
  {
    "objectID": "code_management.html#exercises",
    "href": "code_management.html#exercises",
    "title": "8  Code management",
    "section": "8.3 Exercises",
    "text": "8.3 Exercises\n\nLocation based code management\nCreate a new R project using the git R project template shown above.\n\nMake some changes to the README.md\nPut a small data set in the appropriate directory.\nMake sure that both local and remote repositories (projects) are synced.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code management</span>"
    ]
  },
  {
    "objectID": "code_management.html#report-exercises",
    "href": "code_management.html#report-exercises",
    "title": "8  Code management",
    "section": "8.4 Report Exercises",
    "text": "8.4 Report Exercises\n\nCollaborative Work on Github\nThis is a team exercise, so team up with someone else in the classroom. You will learn about how to collaborate online using git and Github.\n\nImportant: When creating your repositories, make sure that you set the repository to be public and not private.\n\n\nPerson 1 - Create a new repository (can be the same as you created following the tutorial but should no be the same as the one where you hand in your report)\nPerson 2 - Fork the GitHub project that Person 1 created in Step 1.\nPerson 2 - Create a new file in this project\nPerson 2 - Commit and push these changes to this project. Create a pull request to the original project of Person 1.\nPerson 1 - Review the pull request from Person 2. Provide some comments, accept the pull request, letting it the new code by Person 2 be integrated into the project.\nPerson 1 - Add a new file to your own project, and update the GitHub project.\nPerson 2 - Sync your forked project to integrate the changes made by Person 1 into your own repository.\nVoluntary: Can you force a merge conflict, for example by editing the same file at once, and resolve?\n\nTo complete the exercise, reverse rolls between Person 1 and Person 2.\n\n\nDeliverables for the report\nThis pair-coding exercise is part of your final performance assessment. We will check each repositories’ commit history to see whether this pair-coding exercise was done correctly. So, follow the steps above precisely!\nWhen you submit your report by mail at the end of the course, you have to provide the links to your GitHub account, to your report repositories that holds all other report exercises, and to the two repositories that you created during this pair-coding work exercise (your repository that your friend forked and the repository that you forked from your friend). Alternatively you can also create a ./vignettes/re_paircoding.Rmd in your report repository, where you provide these links.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code management</span>"
    ]
  },
  {
    "objectID": "regression_classification.html",
    "href": "regression_classification.html",
    "title": "9  Regression and classification",
    "section": "",
    "text": "9.1 Learning objectives\nChapter lead author: Pepa Aran\nAfter completing this tutorial, you will be able to:\nContents of this Chapter are inspired and partly adopted by the excellent book by Boehmke and Greenwell.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#learning-objectives",
    "href": "regression_classification.html#learning-objectives",
    "title": "9  Regression and classification",
    "section": "",
    "text": "Understand the basics of regression and classification models.\nFit linear and logistic regression models in R.\nChoose and calculate relevant model performance metrics.\nEvaluate and compare regression models.\nDetect data outliers.\nSelect best predictive variables.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#setup",
    "href": "regression_classification.html#setup",
    "title": "9  Regression and classification",
    "section": "9.2 Setup",
    "text": "9.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(ggplot2)\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#tutorial",
    "href": "regression_classification.html#tutorial",
    "title": "9  Regression and classification",
    "section": "9.3 Tutorial",
    "text": "9.3 Tutorial\n\n9.3.1 Types of models\nModels try to explain relationships between variables through a mathematical formulation, particularly to predict a given target variable using other explanatory variables, also called predictors. Generally, we say that the target variable \\(Y\\) is a function (denoted \\(f\\)) of a set of explanatory variables \\(X_1, X_2, \\dots, X_p\\) and some model parameters \\(\\beta\\). Models can be represented as: \\[Y \\sim f(X_1, X_2, \\dots, X_p, \\beta)\\]\nThis is a very general notation and depending on the structure of these components, we get to different modelling approaches.\nThe first distinction comes from the type of target variable. Whenever \\(Y\\) is a continuous variable, we are facing a regression problem. If \\(Y\\) is categorical, we speak of classification.\n\n\n\n\n\n\n\n\n\nRegression\nClassification\n\n\n\n\nTarget variable\nContinuous\nCategorical\n\n\nCommon models\nLinear regression, polynomial regression, KNN, tree-based regression\nLogistic regression, KNN, SVM, tree classifiers\n\n\nMetrics\nRMSE, \\(R^2\\), adjusted \\(R^2\\), AIC, BIC\nAccuracy, precision, AUC, F1\n\n\n\n\n\n9.3.2 Regression\nIn this section, we will introduce the most basic regression model - linear regression. We will explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented.\n\n9.3.2.1 Linear regression\nTheory\nLet’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective. Here, we will take a data-fitting approach.\nFor example, we can try to explain the relationship between GPP and short wave radiation, like in Chapter 5. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values.\n\n# read and format data from Ch 3\nhalf_hourly_fluxes &lt;- readr::read_csv(\"./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv\")\n\nset.seed(2023)\nplot_1 &lt;- half_hourly_fluxes |&gt;\n  sample_n(2000) |&gt;  # to reduce the dataset\n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(size = 0.75, alpha = 0.4) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (gC m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\nsegment_points &lt;- data.frame(x0 = 332, y0 = 3.65, y_regr = 8.77)\n\nplot_1 +\n  geom_segment(aes(x = x0, y = y0, xend = x0, yend = y_regr), \n               data = segment_points,\n               color = \"blue\", lwd = 1.2, alpha = 0.8)\n\n\n\n\n\n\n\n\n\nTo reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here.\n\nWe want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable \\(X\\) and our target \\(Y\\): \\[\nY_i \\sim \\beta_0 + \\beta_1 X_i, \\;\\;\\; i = 1, 2, ...n \\;,\n\\] where \\(Y_i\\) is the i-th observation of the target variable, and \\(X_i\\) is the i-th value of the (single) predictor variable. \\(n\\) is the number of observations we have and \\(\\beta_0\\) and \\(\\beta_1\\) are constant coefficients (model parameters). We call \\(\\beta_0\\) the intercept and \\(\\beta_1\\) the slope of the regression line. Generally, \\(\\hat{Y}\\) denotes the model prediction.\nFitting a linear regression is finding the values for \\(\\beta_0\\) and \\(\\beta_1\\) such that, on average over all points, the distance between the line at \\(X_i\\), that is \\(\\beta_0 + \\beta_1 X_i\\) (blue segment in the plot above), and the observed value \\(Y_i\\), is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is: \\[\n\\min_{\\beta_0, \\beta_1} \\sum_i (Y_i - \\beta_0 - \\beta_1 X_i)^2 .\n\\] This linear model can be used to make predictions on new data, which are obtained by \\(\\hat{Y}_\\text{new} = \\beta_0 + \\beta_1 X_\\text{new}\\). When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction.\nIt’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of \\(p\\) predictor variables: \\[Y \\sim \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\; ... \\; + \\beta_p X_p \\;.\\] Note that here, \\(X_1, \\dots, X_p\\) and \\(Y\\) are vectors of length corresponding to the number of observations in our data set (\\(n\\) - as above). Analogously, calibrating the \\(p+1\\) coefficients \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) is to minimize the sum of square errors \\(\\min_{\\beta} \\sum_i (Y_i - \\hat{Y}_i)^2\\).\nWhile the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions.\nImplementation in R\nTo fit a univariate linear regression model in R, we can use the lm() function. Already in Chapter 4, we created linear models by doing:\n\n# numerical variables only, remove NA\ndf &lt;- half_hourly_fluxes |&gt;\n  dplyr::select(-starts_with(\"TIMESTAMP\")) |&gt;\n  tidyr::drop_na()\n\n# fit univariate linear regression\nlinmod1 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = df)\n\nHere, GPP_NT_VUT_REF is \\(Y\\), and SW_IN_F is \\(X\\). The formula notation GPP_NT_VUT_REF ~ SW_IN_F is common in R and can be used for a most functions in different packages. The (single) variable to the left of the ~ is the target variable (\\(Y\\)). The variable to its right is the predictor. Of course, we can include multiple predictors for a multivariate regression, for example as:\n\n# fit multivariate linear regression\nlinmod2 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = df)\n\nThe shorthand for including all predictors that are available in the data (here, all columns other than GPP_NT_VUT_REF in df), we can write:\n\nlinmod3 &lt;- lm(GPP_NT_VUT_REF ~ ., data = df)\n\nlinmod* is now a model object of class \"lm\". It is a list containing the following components:\n\nls(linmod1)\n\n [1] \"assign\"        \"call\"          \"coefficients\"  \"df.residual\"  \n [5] \"effects\"       \"fitted.values\" \"model\"         \"qr\"           \n [9] \"rank\"          \"residuals\"     \"terms\"         \"xlevels\"      \n\n\nEnter ?lm in the console for a complete documentation of these components and other details of the linear model implementation.\nR offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the residuals are the difference between the observed target values and the predicted values.\n\nsummary(linmod1)\n\n\nCall:\nlm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.699  -2.092  -0.406   1.893  35.153 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.8732273  0.0285896   30.54   &lt;2e-16 ***\nSW_IN_F     0.0255041  0.0001129  225.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.007 on 41299 degrees of freedom\nMultiple R-squared:  0.5525,    Adjusted R-squared:  0.5525 \nF-statistic: 5.099e+04 on 1 and 41299 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also extract coefficients \\(\\beta\\) with\n\ncoef(linmod1)\n\n(Intercept)     SW_IN_F \n 0.87322728  0.02550413 \n\n\nand the residual sum of squares (which we wanted to minimize) with\n\nsum(residuals(linmod1)^2)\n\n[1] 1035309\n\n\nAlthough summary() provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using tidy() from the broom package:\n\nbroom::tidy(linmod1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.873   0.0286        30.5 1.25e-202\n2 SW_IN_F       0.0255  0.000113     226.  0        \n\n\nModel advantages and concerns\nAn advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that GPP_NT_VUT_REF increases by 0.0255 for a unit increase in SW_IN_F. Of course, the units of the coefficients depend on the units of GPP_NT_VUT_REF and SW_IN_F. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether GPP_NT_VUT_REF is given in (g C m\\(^{-2}\\)s\\(^{-1}\\)) or in (t C km\\(^{-2}\\) year\\(^{-1}\\)).\nAnother advantage of linear regression is that it’s much less prone to overfit than other algorithms. You’ll learn more about the concept of overfitting in Chapter 10. Not being prone to overfitting can also be a disadvantage: linear regression models are often not flexible enough to be effectively fit to the data. They are also not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, they often fit the data less well and generate poorer predictions than more complex models.\nA further limitation is that least squares regression requires \\(n&gt;p\\). In words, the number of observations must be greater than the number of predictors. If this is not given, one can resort to stepwise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. We will encounter stepwise regression in the exercises.\nWhen multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in Geography and Environmental Sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that in Chapter 11).\nAn alternative strategy is to use dimension reduction methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R here.\n\n\n9.3.2.2 Regression on categorical variables\nIn the regression within categories section of Chapter 5, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better.\nSo far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class numeric, otherwise the lm() function treats them as continuous variables. For example, although the variable NIGHT is categorical with values 0 and 1, the model linmod3 treats it as a number. We must make sure that categorical variables have class character or, even better, factor.\n\n# create month category\ndf_cat &lt;- half_hourly_fluxes |&gt;\n  mutate(MONTH = lubridate::month(TIMESTAMP_START)) |&gt;\n  tidyr::drop_na() |&gt;\n  dplyr::select(MONTH, GPP_NT_VUT_REF, SW_IN_F)\n\n\n# fix class of categorical variables\ndf_cat &lt;- df_cat |&gt;\n  mutate(MONTH = as.factor(MONTH))\n\nNow we can fit the linear model again:\n\nlinmod_cat &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat)\nsummary(linmod_cat)\n\n\nCall:\nlm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -2.346  -0.223   2.200  34.416 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.6146109  0.0893693  18.067  &lt; 2e-16 ***\nMONTH2      -1.8105447  0.1294675 -13.985  &lt; 2e-16 ***\nMONTH3      -2.8800172  0.1264177 -22.782  &lt; 2e-16 ***\nMONTH4      -2.5667281  0.1278097 -20.082  &lt; 2e-16 ***\nMONTH5      -0.0288745  0.1273491  -0.227 0.820631    \nMONTH6       0.4614556  0.1298069   3.555 0.000378 ***\nMONTH7       0.1697514  0.1283830   1.322 0.186100    \nMONTH8       1.2942463  0.1231252  10.512  &lt; 2e-16 ***\nMONTH9       0.5140562  0.1165474   4.411 1.03e-05 ***\nMONTH10     -0.4807082  0.1152536  -4.171 3.04e-05 ***\nMONTH11     -1.3370277  0.1159059 -11.535  &lt; 2e-16 ***\nMONTH12     -1.2634451  0.1151530 -10.972  &lt; 2e-16 ***\nSW_IN_F      0.0246420  0.0001169 210.810  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.865 on 41288 degrees of freedom\nMultiple R-squared:  0.5776,    Adjusted R-squared:  0.5775 \nF-statistic:  4704 on 12 and 41288 DF,  p-value: &lt; 2.2e-16\n\n\nIn the fit summary, you can observe that, there are MONTH2 to MONTH12 parameters. MONTH is a factor which can take 12 different values: 1 to 12. lm() uses one of the factor level as the reference, in this case 1, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month.\n\ndf_cat |&gt;\n  mutate(MONTH_NAME = lubridate::month(as.integer(MONTH), label = TRUE)) |&gt;\n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(formula = y ~ x + 0, method = \"lm\", color = \"red\", se = FALSE) +\n  labs(x = \"SW\", y = \"GPP\") +\n  facet_wrap(~MONTH_NAME) +\n  theme_classic()\n\n\n\n\n\n\n\n\nIn the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an interaction term MONTH:SW_IN_F in the regression formula, like this:\n\nlinmod_inter &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, data = df_cat)\n# equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat)\nsummary(linmod_inter)\n\n\nCall:\nlm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, \n    data = df_cat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.891  -2.113  -0.420   1.892  34.029 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.0449603  0.0944991  21.640  &lt; 2e-16 ***\nMONTH2          -1.5386938  0.1369424 -11.236  &lt; 2e-16 ***\nMONTH3          -1.5249304  0.1365863 -11.165  &lt; 2e-16 ***\nMONTH4          -1.0050639  0.1396023  -7.199 6.15e-13 ***\nMONTH5          -0.4502367  0.1412720  -3.187  0.00144 ** \nMONTH6          -1.2559057  0.1474257  -8.519  &lt; 2e-16 ***\nMONTH7          -0.8440097  0.1446838  -5.833 5.47e-09 ***\nMONTH8          -0.2188300  0.1346734  -1.625  0.10419    \nMONTH9          -1.3407190  0.1269387 -10.562  &lt; 2e-16 ***\nMONTH10         -0.9991456  0.1235627  -8.086 6.32e-16 ***\nMONTH11         -1.2124373  0.1230946  -9.850  &lt; 2e-16 ***\nMONTH12         -1.0724209  0.1210819  -8.857  &lt; 2e-16 ***\nSW_IN_F          0.0158600  0.0008758  18.110  &lt; 2e-16 ***\nMONTH2:SW_IN_F  -0.0030373  0.0011518  -2.637  0.00837 ** \nMONTH3:SW_IN_F  -0.0058229  0.0009713  -5.995 2.05e-09 ***\nMONTH4:SW_IN_F  -0.0038333  0.0009469  -4.048 5.17e-05 ***\nMONTH5:SW_IN_F   0.0087370  0.0009305   9.389  &lt; 2e-16 ***\nMONTH6:SW_IN_F   0.0135219  0.0009172  14.743  &lt; 2e-16 ***\nMONTH7:SW_IN_F   0.0110791  0.0009182  12.066  &lt; 2e-16 ***\nMONTH8:SW_IN_F   0.0151014  0.0009317  16.209  &lt; 2e-16 ***\nMONTH9:SW_IN_F   0.0180496  0.0009297  19.415  &lt; 2e-16 ***\nMONTH10:SW_IN_F  0.0097277  0.0009761   9.966  &lt; 2e-16 ***\nMONTH11:SW_IN_F -0.0011415  0.0010932  -1.044  0.29640    \nMONTH12:SW_IN_F -0.0099745  0.0012972  -7.689 1.52e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.593 on 41277 degrees of freedom\nMultiple R-squared:  0.6237,    Adjusted R-squared:  0.6234 \nF-statistic:  2974 on 23 and 41277 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n9.3.2.3 Polynomial regression\nFurthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, LOESS (local polynomial regression fitting), etc.\nLet’s fit a simple quadratic regression model, just for the month of August. For this we use the poly() function which constructs orthogonal polynomials of a given degree. Here, a second-order polynomial (a parabola) is fitted:\n\nquadmod &lt;- lm(GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), \n              data = df_cat |&gt;\n                filter(MONTH == 8))\nsummary(quadmod)\n\n\nCall:\nlm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, \n    MONTH == 8))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.367  -2.055  -0.253   1.801  32.375 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.13084    0.07944   89.77   &lt;2e-16 ***\npoly(SW_IN_F, 2)1  447.25113    4.61907   96.83   &lt;2e-16 ***\npoly(SW_IN_F, 2)2 -151.08797    4.61907  -32.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.619 on 3378 degrees of freedom\nMultiple R-squared:  0.7556,    Adjusted R-squared:  0.7555 \nF-statistic:  5223 on 2 and 3378 DF,  p-value: &lt; 2.2e-16\n\n\nIn the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials:\n\ndf_cat |&gt;\n  filter(MONTH == 8) |&gt;\n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(formula = y ~ x, method = \"lm\", aes(color = \"lm\"), se = FALSE) +\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", \n              aes(color = \"poly2\"), se = FALSE) +\n  geom_smooth(formula = y ~ poly(x, 3), method = \"lm\",\n              aes(color = \"poly3\"), se = FALSE) +\n  labs(x = \"SW\", y = \"GPP\", color = \"Regression\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n9.3.2.4 Metrics for regression evaluation\nMetrics measure the quality of fit between predicted and observed values. Metrics are essential to model fitting, model selection, and for describing and quantifying patterns in the data. Metrics are also key for guiding the training of machine learning models, as you will learn in Chapter 11. Different metrics measure different aspects of the model-data agreement. In other words, a single metric never captures all aspects and patterns of the model-data agreement. Therefore, a visual inspection of the model fit is always a good start of the model evaluation.\nTo get an intuitive understanding of the different abilities of different metrics, compare the scatterplots in Figure 9.1 and how different aspects of the model-data agreement are measured by different metrics. The observed target values \\(Y\\) are plotted against the predicted values \\(\\hat{Y}\\) from a regression model, and the dashed line represents the ideal fit: predictions matching the data perfectly. Definitions of the metrics displayed and other metrics are given below.\n\n\n\n\n\n\n\n\nFigure 9.1: Correlation plots between observed and fitted target values.\n\n\n\n\n\nCommon metrics used for evaluating regression fits are:\nMSE\nThe mean squared error is defined, as its name suggests, as: \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2\n\\]It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in Chapter 10, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is sensitive to large errors in single points, including outliers.\nRMSE\nThe root mean squared error is, as its name suggests, the root of the MSE: \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2}\n\\]Like the MSE, the RMSE also measures the magnitude of the errors and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data \\(Y\\) and is less sensitive to outliers than the MSE.\nMAE\nThe mean absolute error is similarly defined: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i = 1}^{n} |Y_i - \\hat{Y_i}|\n\\] Measuring the discrepancies between predictions and observations using absolute errors, instead of squared errors, gives less importance to errors of large magnitude and more importance to small errors than the MSE would. Hence, this measures is more stable in the presence of outliers.\n\\(R^2\\) - coefficient of determination\ndescribes the proportion of variation in \\(Y\\) that is captured by modelled values \\(\\hat{Y}\\). It tells us how much better our fitted values \\(\\hat{Y}\\) are than just taking the average of the target \\(\\bar{Y}\\) as predictions. In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, \\(R^2\\) measures consistency, or correlation, or goodness of fit. It is defined as: \\[\nR^2 = 1 - \\frac{\\sum_i (\\hat{Y}_i - Y_i)^2}{\\sum_i (Y_i - \\bar{Y})^2}\\\\\n\\] When the regression model is fitted by minimizing the MSE, the \\(R^2\\) takes values between 0 and 1. A perfect fit is quantified by \\(R^2 = 1\\). There is no generally valid threshold of \\(R^2\\) for a model to be considered “good”. It depends on the application and the nature of the data and the data-generating process. Note that the above equation can also be written as \\(R^2 = 1 - \\text{MSE}/var(Y)\\).\n\\(r\\) - Pearson’s correlation\nThe linear association between two variables (here \\(Y\\) and \\(\\hat{Y}\\)) is measured by the Pearson’s correlation coefficient \\(r\\). \\[\nr = \\frac{\\sum_i (Y_i - \\bar{Y}) (\\hat{Y_i} - \\bar{\\hat{Y}}) }{\\sqrt{ \\sum_i(Y_i-\\bar{Y})^2 \\; (\\hat{Y_i}-\\bar{\\hat{Y}})^2 } }\n\\]\nThe correlation calculated between the target \\(Y\\) and a predictor \\(X\\) can tell us about the predictive power of a regression model (the higher the correlation, the more powerful). We can also compute the correlation between the target \\(Y\\) and the predicted values \\(\\hat{Y}\\) by a model (multivariate, or even not linear) to assess the adequacy of the model chosen. See Figure Figure 9.1 as an example. It is noteworthy to mention that correlation is location and scale invariant, hence it will not detect model deviations like the ones in the middle row plots.\nThe squared value of the Pearson’s r is often reported as “\\(R^2\\)” but is not equivalent to the definition of the coefficient of determination given above. However, the square of the Pearson’s r is closely related to the coefficient of determination \\(R^2\\). For a linear regression, fitted by minimizing the MSE, they are identical (see proof here). In subsequent chapters, we will use “\\(R^2\\)” to refer to the square of the Pearson’s r between the observed \\(Y\\) and predicted \\(\\hat{Y}\\) values.\n\nNote the implementations in R.The \\(R^2\\) reported by the generic summary() function corresponds to the base-R function cor()^2 , to yardstick::rsq(), and to the definition of the square of the Pearson’s \\(r\\) given above. The yardstick::rsq_trad() returns the coefficient of determination as traditionally defined and is not equal to the values above, unless computed on the predicted values \\(\\hat{Y}\\).\n\n\n# generate correlated random data \nset.seed(1982)\ndf &lt;- tibble(x = rnorm(100)) |&gt; \n  mutate(y = x + rnorm(100)) |&gt;\n  mutate(y_fitted = lm(y ~ x)$fitted.values)\n\n# implementations using Pearson's correlation\nsummary(lm(y ~ x, data = df))$r.squared\n\n[1] 0.6186521\n\ncor(df$y, df$x)^2 # remember: location and scale invariant\n\n[1] 0.6186521\n\nyardstick::rsq(df, y, x) |&gt; pull(.estimate)\n\n[1] 0.6186521\n\n(sum((df$x - mean(df$x))*(df$y - mean(df$y))))^2/\n  (sum((df$y - mean(df$y))^2)*sum((df$x - mean(df$x))^2))\n\n[1] 0.6186521\n\n# implementations using coefficient of determination definition\n1 - sum((df$x - df$y)^2) / sum((df$y - mean(df$y))^2) # should be \\hat{y}, not x\n\n[1] 0.5993324\n\nyardstick::rsq_trad(df, y, x) |&gt; pull(.estimate) # incorrect\n\n[1] 0.5993324\n\nyardstick::rsq_trad(df, y, y_fitted) |&gt; pull(.estimate) # correct\n\n[1] 0.6186521\n\n\n\nAn “\\(R^2\\)” is commonly reported when evaluating the agreement between observed and predicted values of a given model. When the correlation between two different variables in a sample is quantified, \\(r\\) is commonly used to reflect also whether the correlation is positive or negative (\\(r\\) can attain positive or negative values in the interval \\([-1, 1]\\)). The coefficient of determination can return negative values when comparing observed and predicted values for uninformative estimates (worse than just using the average of \\(Y\\)) and is thus not actually bound between 0 and 1. Therefore, be careful with the interpreration of “\\(R^2\\)” and think on which variables it was computed and with which method.\n\n\nSometimes, the Person’s version is computed between \\(Y\\) and \\(x\\), and it leads to the same number due to its “location and scale invariant” property. Nevertheless, this is conceptually wrong, as we should look at the predictions, not the predictors: We are not predicting \\(Y\\) by just giving the values of \\(x\\) instead. Hence, especially when using {yardstick} functions, make sure you compute the values on \\(\\hat{Y}\\). When we have several predictors, it’s already clear that we should compare \\(Y\\) to \\(\\hat{Y}\\) instead of \\(Y\\) to each predictor separately.\n\nBias\nThe bias is simply the mean error: \\[\n\\text{bias} = \\frac{1}{n} \\sum_i^n{(\\hat{Y}_i - Y_i)}\n\\]\nSlope\nThe slope refers to the slope of the linear regression line between predicted and observed values. It is returned as the second element of the vector returned by coef(lm(..)):\n\ncoef(lm(y ~ y_fitted, data = df))[2]\n\ny_fitted \n       1 \n\n\n\n\n9.3.2.5 Metrics for regression model comparison\nIn general, the aim of regression modelling is to find a model that best explains the data - but not the random errors in the data. More complex models tend to overfit more than simpler models. The implication of overfitting is that the model fits the data used for model fitting well, but doesn’t fit well when evaluating the predictions of the same model to new data (data not used for model fitting). In such a case, the model’s generalisability is poor. We’ll learn more about overfitting and generalisability in the context of supervised machine learning in later chapters. Often, simpler models generalise better than more complex model. The challenge is to strike a balance between complexity and generalisability. But how to find the “sweet spot” of this trade-off?\nIn this context it should be noted that the \\(R^2\\) always increases when predictors are added to a model. Therefore, the \\(R^2\\) is not a suitable metric for comparing models that differ with respect to their number of predictors - a factor controlling model complexity. Cross-validation can be regarded as the “gold-standard” for measuring model generalisability if the data is plentiful. It will be introduced in the context of supervised machine learning in Chapter 11. However, when the data size is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors \\(p\\). Different metrics are available:\nAdjusted \\(R^2\\)\nThe adjusted \\(R^2\\) discounts values of \\(R^2\\) by the number of predictors. It is defined as \\[\n{R}^2_{adj} = 1 - (1-R^2) \\; \\frac{n-1}{n-p-1} \\;,\n\\] where \\(n\\) (as before) is the number of observations, \\(p\\) the number of parameters and \\(R^2\\) the usual coefficient of determination. Same as for \\(R^2\\), the goal is to maximize \\(R^2_{adj}\\).\nAIC\nThe Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as: \\[\n\\text{AIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + 2(p+2)\n\\] where \\(n\\) is the number of observations used for estimation, \\(p\\) is the number of explanatory variables in the model and SSE is the sum of squared errors (SSE\\(= \\sum_i (Y_i-\\hat{Y_i})^2\\)). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for generalisations to new data. Since it penalizes having many parameters, it will favor less complex models.\nAIC\\(_c\\)\nFor small values of \\(n\\) the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: \\[\n\\text{AIC}_c = \\text{AIC} + \\frac{2(p + 2)(p + 3)}{n-p-3}\n\\] Also AIC\\(_c\\) is minimized for an optimal predictive model.\nBIC\nThe Schwarz’s Bayesian Information Criterion is defined as \\[\n\\text{BIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + (p+2)  \\log(n)\n\\] Also for BIC, the goal is to minimize it. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC.\nImplementation in R\nLet’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like \\(R^2\\) and \\(R^2_{adj}\\) are given by the summary() function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below:\n\ncompute_regr_metrics &lt;- function(mod){\n  \n  p &lt;- length(mod$coefficients)\n  n &lt;- length(mod$residuals)\n  \n  tibble(\n    mse = mean(mod$residuals^2),\n    R2 = summary(mod)$r.squared,\n    R2_adj = summary(mod)$adj.r.squared,\n    AIC = extractAIC(mod)[2],\n    AIC_adj = extractAIC(mod)[2] + 2*(p+2)*(p+3)/(n-p-3),\n    BIC = BIC(mod) # this implementation is based on log-likelihood\n  )\n}\n\nlist_metrics &lt;- purrr::map(\n    list(linmod1, linmod2, linmod_cat, quadmod), \n    ~compute_regr_metrics(.))\nnames(list_metrics) &lt;- c(\"Linear model\", \n                         \"Linear model 2\", \n                         \"Linear + categories\",\n                         \"Quadratic model\")\nbind_rows(list_metrics, .id = \"type\")\n\n# A tibble: 4 × 7\n  type                  mse    R2 R2_adj     AIC AIC_adj     BIC\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Linear model         25.1 0.553  0.553 133058. 133058. 250293.\n2 Linear model 2       24.8 0.558  0.558 132590. 132590. 249842.\n3 Linear + categories  23.7 0.578  0.577 130700. 130700. 248030.\n4 Quadratic model      21.3 0.756  0.755  10350.  10350.  19972.\n\n\n\n\n\n9.3.3 Model selection\nModel selection refers to several techniques that help you compare models and select the one that best explains the data. Here, concepts will be explained using regression as an example, but are directly translated to classification problems.\nFirst, be systematic with model comparisons. Three key ideas in model selection are:\n\nComparisons should be hierarchical: compare a model to another that “contains it”, i.e. compare y ~ x1 to y ~ x1 + x2, and not y ~ x1 to y ~ x2 + x3.\nComplexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors.\nChoose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g., recall for a classification where you don’t want to miss any positives).\n\nIf you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best KNN non-parametric regression model and a random forest. Then compare those three final models and choose the one that answers your research question the best.\nOne must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. Chapter 10 introduces the concept of overfitting, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g., by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships).\n\n9.3.3.1 Variable selection\nLet’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of all available predictors, calculate a metric (e.g., AIC) and choose the best one (lowest AIC). The problem is, if you have, say, eight predictors, you would fit 40320 different regression models. This can be very computationally expensive.\nInstead, take a hierarchical, or “greedy”, approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called stepwise forward regression. The algorithm goes as follows:\n\nSet the number of predictors to be considered to \\(p=1\\).\nFit all regression models with \\(p\\) predictors and compute their \\(R^2\\).\nSelect the model with \\(p\\) predictors that achieves the highest \\(R^2\\) (best fitting model) and compute its AIC.\nIncrement to \\(p+1\\). Fit all regression models with \\(p+1\\) predictors that include the predictor selected at the previous step and compute their \\(R^2\\). Select the best fitting model and compute its AIC.\nIf the AIC of the model with \\(p+1\\) predictors is poorer than the AIC of the model with \\(p\\) predictors, retain the model with \\(p\\) predictors and quit. You have found the (presumably) optimal model. Otherwise, continue with with step 4.\n\nInstead of the AIC, you may also use the BIC for model comparison. Note that algorithm doesn’t consider all possible combinations of predictors and it is possible that the globally optimal model is thus not found. The function stats::step() implements the stepwise algorithm in R.\nThis stepwise approach can also be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression whereby the aim si to increase the degree of the polynomials - step by step. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”.\nMulticollinearity exists when there is a correlation between multiple predictors in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the correlated variables unstable. Since correlated variables contain similar information, it doesn’t matter whether we include one or the other in the model or even if we include both of them. The performance metrics will be similar. Hence, it becomes difficult to conclude which variables actually influence the target.\nThe variance inflation factor (VIF) is a score from economics that measures the amount of multicollinearity in regression based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as \\[\\text{VIF}_j = \\frac{1}{1 - R^2_j},\\] where \\(R^2_j\\) is the coefficient of determination for regressing the \\(j^{th}\\) predictor on the \\(p-1\\) remaining predictors. More specifically, \\[R^2_j = 1 - \\frac{\\sum_i (X_{j, i} - \\hat{X}_{j, i})^2}{\\sum_i (X_{j, i} - \\bar{X}_j)^2},\\] where \\(\\hat{X}_{j, i}\\) is the fitted value corresponding to the regression \\(X_j \\sim X_1 + ... X_{j-1} + X_{j+1} + ... + X_p\\) for the \\(i^{th}\\) observation. A VIF\\(_j\\) is computed for each of the \\(p\\) predictors in the multivariate regression model we are evaluating, and their values interpreted to detect multicollinearity. Meaning: if \\(\\text{VIF}_j = 1\\) variables are not correlated; if \\(1 &lt; \\text{VIF}_j &lt; 5\\) there is moderate collinearity between \\(X_j\\) and the rest of predictors; and if \\(\\text{VIF}_j \\geq 5\\) they are highly correlated. Because variable \\(X_{j}\\) can be almost fully explained by all the other predictors (high \\(R^2_j\\)), this variable is redundant in our final model.\nTo remedy collinearity, you may choose to use only one or two of those correlated variables. Another option would be to use Principal Component Analysis (PCA), which you may read more about here. What PCA does is to map the space of predictors into another space of smaller dimension, leading to a smaller set of predictor variables \\(\\{Z_1, ... , Z_q\\}\\), each of them being a linear combination of all the initial available predictors, that is \\(Z_1 = \\alpha^1_0 + \\alpha^1_1 X_1 + ... + \\alpha^1_p X_p\\), etc. If you have collinearity, those highly correlated variables would be summarized into one single new variable, called principal component.\nWhen we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check this post if you’re curious).\n\n\n\n9.3.4 Outlier detection\nDetecting outliers is important, because they can affect the fit of a model and render the model fitting not robust. When the data is large, individual points have less influence on the model fitting. Therefore, only outliers that are very far from normal values will affect the model fit (see below). Outliers are particularly problematic in the context of small data, because every value has a big influence on the fitted model.\nTake a look at the two linear regressions below and how one single point can strongly influence the fit. the added point (red circle around black dot) lies clearly outside the “cloud” of remaining points and doesn’t seem to follow the same pattern in the data.\n\nset.seed(2023)\nhalf_hourly_fluxes_small &lt;- half_hourly_fluxes |&gt;\n  sample_n(100) |&gt; # reduce dataset\n  select(SW_IN_F, GPP_NT_VUT_REF)\n\nplot_3 &lt;- half_hourly_fluxes_small |&gt;\n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(size = 0.75) +\n  geom_smooth(method = \"lm\", color = \"red\", fullrange = TRUE) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic() +\n  ylim(-20, 40) + \n  xlim(0, 1100)\n\nplot_4 &lt;- half_hourly_fluxes_small |&gt;\n  add_row(SW_IN_F = 1100, GPP_NT_VUT_REF = -20) |&gt; # add outlier\n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +\n  geom_point(size = 0.75) +\n  geom_smooth(method = \"lm\", color = \"red\", fullrange = TRUE) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic() +\n  geom_point(aes(x = 1100, y = -20), colour = 'red', shape = 1, size = 3) +\n  ylim(-20, 40) + \n  xlim(0, 1100)\n  \ncowplot::plot_grid(plot_3, plot_4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 9.2: The influence of an outlier on a regression fit.\n\n\n\n\n\nThe first step to identifying outliers is usually to look at the data, one variable at a time. Plot a histogram to check the distribution of a variable. The shape of the distribution is very informative for what can be considered an outlier. In Chapter 4 and Chapter 5 it was introduced how to identify values that fall “outside” a distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not.\n\n# create an outlier for demonstration purposes\nhalf_hourly_fluxes_outlier &lt;- half_hourly_fluxes_small |&gt;\n  add_row(SW_IN_F = 1100, GPP_NT_VUT_REF = -20)\n\n# Various ways to identify the outlier using graphs\nplot_5 &lt;- ggplot(\n  data = half_hourly_fluxes_outlier,\n  aes(x = GPP_NT_VUT_REF, y = after_stat(density))) +\n  geom_histogram(fill = \"grey70\", color = \"black\") +\n  geom_density(color = 'red')+\n  labs(title = 'Histogram, density and boxplot', \n       x = expression(paste(\"GPP (gC m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\nplot_6 &lt;- ggplot(\n  data = half_hourly_fluxes_outlier,\n  aes(x = \"\", y = GPP_NT_VUT_REF)) +\n  geom_boxplot(fill = \"grey70\", color = \"black\") +\n  coord_flip() +\n  theme_classic() +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(y = expression(paste(\"GPP (gC m\"^-2, \"s\"^-1, \")\")))\n\nplot_7 &lt;- ggplot(\n  data = half_hourly_fluxes_outlier,\n  aes(x = SW_IN_F, y = after_stat(density))) +\n  geom_histogram(fill = \"grey70\", color = \"black\") +\n  geom_density(color = 'red')+\n  labs(title = 'Histogram, density and boxplot', \n       x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\"))) +\n  theme_classic()\n\nplot_8 &lt;- ggplot(\n  data = half_hourly_fluxes_outlier,\n  aes(x = \"\", y = SW_IN_F)) +\n  geom_boxplot(fill = \"grey70\", color = \"black\") +\n  coord_flip() +\n  theme_classic() +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(y = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")))\n\ncowplot::plot_grid(plot_5, plot_7, plot_6, plot_8,\n                   ncol = 2, rel_heights = c(2,1),\n                   align = 'v', axis = 'lr')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA Q-Q Plot depicts the sample quantiles of a variable against the theoretical quantiles of a distribution that is assumed to uderly the data. In the histograms above, GPP looks somewhat Gaussian (normally distributed) but with fatter tails and slightly skewed to the right, while shortwave radiation is clearly skewed to the right, resembling an exponential distribution. The Q-Q plots below reveal that both variables are clearly not normally distributed.\n\nplot_9 &lt;- ggplot(\n    data = half_hourly_fluxes_outlier,\n    aes(sample = GPP_NT_VUT_REF)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(y = expression(paste(\"GPP (gC m\"^-2, \"s\"^-1, \")\")),\n       x = \"Theoretical normal quantiles\") +\n  theme_classic()\n\nplot_10 &lt;- ggplot(\n  data = half_hourly_fluxes_outlier,\n  aes(sample = SW_IN_F)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(y = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")),\n       x = \"Theoretical normal quantiles\") +\n  theme_classic()\n\ncowplot::plot_grid(plot_9, plot_10, ncol = 2)\n\n\n\n\n\n\n\n\nQQ-plots serve to investigate whether the data follows an assumed theoretical distribution. For linear and logistic regression, we would like predictor variables to look as normal, i.e. Gaussian, as possible. You’ve probably learned some of the reasons for this in the Quantitative Methods course (or a similar classical Statistics course), and we will not discuss this further here.\nNote however, that neither the histograms, nor the boxplots, or the QQ-plots revealed any evidently suspicious pattern of the added point (shortwave radiation = 1100 W m\\(^{-2}\\) s\\(^{-1}\\), and GPP = -20 \\(\\mu\\)mol CO\\(_2\\) m\\(^{-2}\\) s\\(^{-1}\\)) compared to the individual distributions of the two variables. Yet, in Figure 9.2, the influence of the outlying point on the fit was clear. Because the suspicious observation is off the multivariate pattern in the remaining data, it becomes very influential. That is, it has a big leverage. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs. Leverage” plot:\n\n# Fit regression with outlier\nlinmod_outlier &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, \n                     data = add_row(half_hourly_fluxes_small, \n                                    SW_IN_F = 1100, \n                                    GPP_NT_VUT_REF = -20))\n\nplot(linmod_outlier, 5)\n\n\n\n\n\n\n\n\nThis plot shows the leverage (see the mathematical definition here) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e., points that are far from the center of the predictor distribution, and large residuals, i.e., points that are far from the fitted regression line, are very influential. The Cook’s distance (definition here) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are drawn with a dashed line.\nFinally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#extra-material",
    "href": "regression_classification.html#extra-material",
    "title": "9  Regression and classification",
    "section": "9.4 Extra material",
    "text": "9.4 Extra material\n\n9.4.1 Classification\nClassification models predict a categorical target variable. Note that predictors of a classification model may still be, and often are, continuous. We will introduce a classification problem with a binary target, since it’s straightforward to generalize to categorical variables with more than two classes. As an example, we use the CO2 dataset from the {datasets} package, which contains data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.\n\n\n\n\n\n\nFigure 9.3: Echinochloa crus-galli, image from swbiodiversity.org\n\n\n\nWe will try to classify the origin of each plant (categorical variable Type with values Quebec or Mississippi) depending on the carbon dioxide uptake rate of the plant (continuous variable uptake measured in \\(\\mu\\)mol m\\(^{-2}\\)s\\(^{-1}\\)). More information on the dataset can be obtained by typing ?datasets::CO2 in the console.\n\ndatasets::CO2 |&gt;\n  ggplot(aes(x = uptake, y = Type, color = Type)) +\n  geom_point(size = 3, alpha = 0.5) +\n  theme_classic() +\n  labs(x = expression(paste(\"Uptake (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAt first sight, it’s easy to see that the carbon uptake is lower for the Mississippi type. Note that other predictors can be included in the model, but we’ll focus on a single predictor. Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification.\n\n9.4.1.1 Logistic regression\nTheory\nA classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of GPP_NT_VUT_REF ~ SW_IN_F was GPP_NT_VUT_REF\\(\\;=\\; \\beta_0 + \\beta_1\\)SW_IN_F. With the classification model Type ~ uptake, we cannot just write Type\\(\\;=\\; \\beta_0 + \\beta_1\\)uptake because Type is not a number. Hence, the categorical variable must be encoded, in this case 0 represents Quebec and 1 represents Mississippi.\nThe next issue is that a linear model makes continuous predictions in the entire real numbers space \\((-\\inf, \\inf)\\), but we want the predictions to be either 0 or 1. We can transform these values to be in the interval \\([0,1]\\) with a link function. For a binary response, it’s common to use a logit link function: \\[\\text{logit}(z) = \\frac{\\exp(z)}{1+\\exp(z)}.\\]\n\nggplot() +\n  geom_function(fun = function(x) exp(x)/(1 + exp(x))) +\n  xlim(-5, 5) +\n  labs(y = \"logit(x)\", x = \"x\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nCombining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive at the logistic regression model:\n\\[f(X, \\beta) = \\text{logit}(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}.\n\\] This predicted value can be understood as the probability of belonging to class 1 (in our example, Mississippi). A classification rule is defined such that an observation \\(X_{new}\\) with a predicted probability of belonging to class 1 higher than a given threshold \\(\\tau\\) (i.e. \\(f(X_{new}, \\beta) &gt; \\tau\\)) will be classified as 1. If the predicted probability is smaller than the threshold \\(\\tau\\), it will be classified as 0.\nA logistic regression model results in a linear classification rule. This means that the \\(p\\)-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with carbon uptake as predictor, the classification boundary would be a point dividing the real line. If we include a second predictor, we would obtain a line dividing the 2-dimensional plane in two.\nFurthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator of \\(\\beta\\) with an iterative algorithm. We will learn more about iterative model fitting, i.e. parameter optimization, in the context of supervised machine learning (Chapter 11).\nImplementation in R\nFirst, let’s see how the target variable is encoded. R directly loads the dataframe with Type as a factor and Quebec as the reference level. R factors work such that each factor level (here Quebec and Mississippi) corresponds to an integer value (its position given by levels(), here 1 and 2 respectively). We can fit a logistic model in R with this encoding.\n\nstr(datasets::CO2)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:\n $ Plant    : Ord.factor w/ 12 levels \"Qn1\"&lt;\"Qn2\"&lt;\"Qn3\"&lt;..: 1 1 1 1 1 1 1 2 2 2 ...\n $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Ambient carbon dioxide concentration\"\n  ..$ y: chr \"CO2 uptake rate\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(uL/L)\"\n  ..$ y: chr \"(umol/m^2 s)\"\n\nlevels(datasets::CO2$Type)\n\n[1] \"Quebec\"      \"Mississippi\"\n\n\nTo fit a logistic regression in R we can use the glm() function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output:\n\nlogmod &lt;- glm(Type ~ uptake,\n              family = binomial(link = logit),\n              data = datasets::CO2)\nsummary(logmod)\n\n\nCall:\nglm(formula = Type ~ uptake, family = binomial(link = logit), \n    data = datasets::CO2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.87192    0.87273   4.437 9.14e-06 ***\nuptake      -0.14130    0.02992  -4.723 2.32e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 116.449  on 83  degrees of freedom\nResidual deviance:  83.673  on 82  degrees of freedom\nAIC: 87.673\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis fitted model results in a linear classification boundary (discontinued line) that splits the predictor variables space in two. Where that line falls depends on the choice of threshold, in this case \\(\\tau=0.5\\) (see where the grey logistic regression line meets the dashed threshold line). You can see it plotted below:\n\nbeta &lt;- coef(logmod)\n\n# reuse previous plot with classification line\ndatasets::CO2 |&gt;\n  ggplot(aes(x = uptake, y = as.numeric(Type)-1, color = Type)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = expression(paste(\"Uptake (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\")),\n       y = \"\") +\n  theme_classic() +\n  \n  # call glm model fit as part of the plotting\n  stat_smooth(method = \"glm\", color = \"grey\", se = FALSE,\n              method.args = list(family = binomial),\n              size = 3) +\n  \n  # manually plot of logit function with fitted coefficients\n  geom_function(fun = function(x) exp(beta[1] + beta[2] * x)/(1 + exp(beta[1] + beta[2] * x)),\n                color = \"black\", size = 0.5) +\n  \n  # visualise threshold\n  geom_vline(xintercept = -beta[1] / beta[2], lty = 2, linetype = \"dotted\") +\n  xlim(0, 60)\n\n\n\n\n\n\n\n\nMost turquoise points fall to one side of the dotted line, representing the threshold \\(\\tau\\), and most reddish points are the other side. This is what we intended. The points that are on the wrong side of the line are mis-classified by the logistic regression model. Our aim is to minimize mis-classification.\nNote that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models.\nModel advantages and concerns\nOne advantage of logistic regression is simplicity. It’s part of the generalized linear regression family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this Wikipedia article.\nFurthermore, logistic regression allows for an interesting interpretation of its model parameters: odds and log-odds. Odds represent how likely it is to find one class versus the other (e.g., if class 1 is twice as likely as class 0, then we have probabilities \\(66\\%\\) vs. \\(33\\%\\)). The odds are defined as the probability of \\(Y\\) belonging to class 1 divided by the probabiity of belonging to class 0, and relates to the model parameters as \\[\\frac{P(Y_i=1)}{P(Y_i=0)} = \\exp(\\beta_0+\\beta_1 X_i).\\] So the log-odds are \\[\\log\\left(\\frac{P(Y_i=1)}{P(Y_i=0)}\\right) = \\beta_0+\\beta_1 X_i.\\] Increases in the values of the predictors affect the odds multiplicatively and the log-odds linearly.\nIt is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3.\nNevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. Whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in Chapter 10 and Chapter 11, can also be used for classification tasks. These offer more flexibility than logistic regression, are not necessarily linear, and don’t need to satisfy strict statistical assumptions.\n\n\n9.4.1.2 Metrics for classification\nMeasuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a confusion matrix:\n\n\n\n\n\\(Y = 1\\)\n\\(Y = 0\\)\n\n\n\n\n\\(\\hat{Y} = 1\\)\nTrue positives (TP)\nFalse positives (FP)\n\n\n\\(\\hat{Y} = 0\\)\nFalse negatives (FN)\nTrue negatives (TN)\n\n\n\nIn a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications.\nImagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives.\nThe following metrics are widely used and highlight different aspects of our modeling goals.\n\nAccuracy is simply the proportion of outputs that were correctly classified: \\[ \\text{Accuracy}=\\frac{\\text{TP} + \\text{TN}}{N},\\] where \\(N\\) is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value \\(\\frac{1}{C}\\) where \\(C\\) is the number of classes.\n\nClassification models are usually compared to randomness: How much better is our model compared to throwing a coin for classification? At random, we would assign each class \\(50\\%\\) of the time. So if we assume that both classes are as likely to appear, that is, they are balanced, the accuracy of a random guess would be around \\(0.5\\). Hence, we want the accuracy to be “better than random”. If there are \\(C\\) different classes and the observations are balanced, we want the accuracy to be above \\(1-1/C\\).\nA challenge is posed by imbalanced classes. For a dataset where \\(90\\%\\) of the observations are from class 1 and \\(10\\%\\) from class 0, always predicting 1 would lead to a accuracy of \\(0.9\\). This value may sound good, but that model is not informative because it doesn’t use any information from predictors. Therefore, be careful when working with imbalanced classes and interpreting your results.\n\nPrecision measures how often our “positive” predictions are correct: \\[\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\]\nThe true positive rate (TPR), also called recall or sensitivity measures the proportion of real “positives” (\\(Y = 1\\)) we are able to capture: \\[ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\\]\nThe false positive rate (FPR) is defined by \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}.\\] and is related to another metric called specificity by \\(\\text{FPR} = 1 - \\text{specificity}\\).\nReceiver operating characteristic (ROC) curve: To evaluate the performance of a binary classification model, it’s common to plot the ROC curve, where the TPR is plotted against the FPR, for varying values of the threshold \\(\\tau\\) used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot:\n\n\n\n\n\n\n\nFigure 9.4: ROC curves and how they compare, from Wikimedia Commons.\n\n\n\n\nAUC: The “area under the curve” is defined as the area betwee the ROC curve and the x-axis. For a random classifier we would have AUC = 0.5 and for the perfect classifier, AUC = 1. The aim is to increase the AUC. Nevertheless, a visual inspection of the ROC curve can say even more.\nF1: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values: \\[\nF1= 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} = \\frac{2 \\text{TP}}{2 \\text{TP} + \\text{FP} + \\text{FN}}.\n\\] This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad.\n\nThese metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives.\nThis was an introduction of the most basic classification metrics. For a more information on the topic, check out Kuhn & Johnson (2019), Chapter 3.3.2.\nImplementation in R\nLet’s take a look at the previous metrics for the logistic regression model we fitted before. The confusionMatrix() function from the {caret} library provides most of the statistics introduced above.\n\n# Make classification predictions\nY &lt;- logmod$data$Type\nx &lt;- as.factor(round(logmod$fitted.values)) # Use 0.5 as threshold\n\n# Change class names\nlevels(Y) &lt;- levels(x) &lt;- c(\"Quebec\", \"Mississippi\")\n\n# plot confusion matrix\nconf_matrix &lt;- caret::confusionMatrix(data = x, reference = Y)\nconf_matrix\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Quebec Mississippi\n  Quebec          32          13\n  Mississippi     10          29\n                                         \n               Accuracy : 0.7262         \n                 95% CI : (0.618, 0.8179)\n    No Information Rate : 0.5            \n    P-Value [Acc &gt; NIR] : 2.039e-05      \n                                         \n                  Kappa : 0.4524         \n                                         \n Mcnemar's Test P-Value : 0.6767         \n                                         \n            Sensitivity : 0.7619         \n            Specificity : 0.6905         \n         Pos Pred Value : 0.7111         \n         Neg Pred Value : 0.7436         \n             Prevalence : 0.5000         \n         Detection Rate : 0.3810         \n   Detection Prevalence : 0.5357         \n      Balanced Accuracy : 0.7262         \n                                         \n       'Positive' Class : Quebec         \n                                         \n\n\nNow we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes.\n\nmosaicplot(conf_matrix$table,\n           main = \"Confusion matrix\")",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#exercises",
    "href": "regression_classification.html#exercises",
    "title": "9  Regression and classification",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\nThere are no exercises with provided solutions for this Chapter.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "regression_classification.html#report-exercise",
    "href": "regression_classification.html#report-exercise",
    "title": "9  Regression and classification",
    "section": "9.6 Report Exercise",
    "text": "9.6 Report Exercise\nAlthough there are a lot of helpful packages and user-friendly functions available, you will often have to write your own data analysis routine. This requires good understanding of statistical knowledge, algorithmic thinking and problem-solving skills. While writing your code, you will face many questions and bugs that you need to solve. And knowing where and how to ask for help properly are crucial parts of this process (see Section 3.2.3 for more on getting help).\nIn this Report Exercise, stepwise forward regression is to be performed for the task of modelling GPP as a function of predictors available in the dataset of half-hourly ecosystem fluxes. To learn the skills mentioned above, you will (attempt to) write your own stepwise forward regression from scratch based on the algorithm description of stepwise forward regression in Section 9.3.3.1. The data of half-hourly fluxes can be downloaded from here.\n\nDeliverables for the report\nThe complete workflow of reading the data, performing the stepwise forward regression, visualising and discussing the results should be implemented in an RMarkdown notebook file and added as file ./vignettes/re_stepwise.Rmd to your git repository. You will then point us to the URL of your repository. We will fork your repository and reproduce the workflow implemented in your notebook. The following aspects should be covered in your notebook:\n\nAn evaluation of all bivariate models (single predictor), implementing just steps 1-3 of the algorithm described in Section 9.3.3.1. This should be complemented by a visualisation and a brief discussion of the results.\nAn implementation of stepwise forward regression, and a visualisation and discussion of its results. If you face unsurmountable challenges and/or errors you didn’t manage to resolve, your notebook may implement a minimum reproducible example of your code that produces the error or of your attempted implementation of stepwise forward regression.\n\n\n\nGuide for your implementation\n\nRemember the structure of the loops implemented for the Exercise Nested loops in Chapter 3.\nIt may be helpful to write a “pseudo-code” that defines the structure of loops and conditionals and defines the points at which certain evaluations and function calls are performed, but cannot be actually run.\nThis Stackoverflow post may be helpful for dynamically creating formulas, given a character vector or variable names,\nWhen discussing results, consider why a certain variable was not chosen in the final model. How do non-included variables relate to variables that are included? How do the model metrics change when adding variables? Can you explain their patterns with your statistical knowledge?\nTo better discuss your results, you might want to look up what each variable stands for. More information can be found here.\nChose an appropriate visualisation for communicating results.\nIf you hand in a minimum reproducible example of an unsuccessful implementation of stepwise regression, follow this guideline. Write a minimum reproducible example as if you were addressing an online audience; describe in detail what you goal is, where you got stuck, what the error message is, provide a code example that is runnable without needing any local files.\nCode errors can break knitting your RMarkdown to HTML. To prevent an entire RMarkdown from failing to render because of erroneous code, you can set the respective code chunk as error = TRUE to avoid render failure while including the error message in the output, or eval = FALSE to avoid running the chunk altogether. More can be found here.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression and classification</span>"
    ]
  },
  {
    "objectID": "supervised_ml_I.html",
    "href": "supervised_ml_I.html",
    "title": "10  Supervised machine learning I",
    "section": "",
    "text": "10.1 Learning objectives\nChapter lead author: Benjamin Stocker\nMachine learning may appear magical. The ability of machine learning algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this and the next (Chapter 11), we will discuss some basics of supervised machine learning and how to achieve best predictive results.\nBasic steps of the implementation of supervised machine learning are introduced, including data splitting, pre-processing, model formulation, and the implementation of these steps using the {caret} and {recipes} R packages. A focus is put on learning the concept of the bias-variance trade-off and overfitting.\nContents of this Chapter are inspired and partly adopted by the excellent book Hands-On Machine Learning in R by Boehmke & Greenwell.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised machine learning I</span>"
    ]
  },
  {
    "objectID": "supervised_ml_I.html#setup",
    "href": "supervised_ml_I.html#setup",
    "title": "10  Supervised machine learning I",
    "section": "10.2 Setup",
    "text": "10.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(recipes)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised machine learning I</span>"
    ]
  },
  {
    "objectID": "supervised_ml_I.html#tutorial",
    "href": "supervised_ml_I.html#tutorial",
    "title": "10  Supervised machine learning I",
    "section": "10.3 Tutorial",
    "text": "10.3 Tutorial\n\n10.3.1 What is supervised machine learning?\nSupervised machine learning is a type of machine learning where the model is trained using labeled data and the goal is to predict the output for new, unseen data. This corresponds to the approach of model fitting that we’ve seen in Chapter 9. In contrast, unsupervised machine learning is a type of machine learning where the algorithms learn from data without being provided with labeled targets. The algorithms aim to identify patterns and relationships in the data without any guidance. Examples include clustering and dimensionality reduction.\nIn supervised machine learning, we use a set of predictors \\(X\\) (also known as features, or independent variables) and observed values of a target variable \\(Y\\) that are recorded in parallel, to find a model \\(f(X) = \\hat{Y}\\) that yields a good match between \\(Y\\) and \\(\\hat{Y}\\) and that can be used for reliably predicting \\(Y\\) for new (“unseen”) data points \\(X_\\text{new}\\) - data that has not been used during model fitting/training. The hat on \\(\\hat{Y}\\) denotes an estimate. Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks).\nFrom above definitions, we can note a few key ingredients of supervised machine learning:\n\nInput data (predictors)\nTarget data recorded in parallel with predictors\nA model that estimates \\(f(X) = \\hat{Y}\\), made of mathematical operations relating \\(X\\) to \\(\\hat{Y}\\) and of model parameters (coefficients) that are calibrated to yield the best match of \\(Y\\) and \\(\\hat{Y}\\)\nA metric measuring how good the match between \\(Y\\) and \\(\\hat{Y}\\) is - the loss function\nAn algorithm (the optimiser) to find the best set of parameters that minimize the loss\n\n\n\n\n\n\n\nFigure 10.1: Supervised machine learning ingredients, adopted from Chollet and Allaire (2018).\n\n\n\nThe type of modelling approach of supervised machine learning is very similar to fitting regression models as we did in Chapter 9. In a sense, supervised machine learning is just another empirical (or statistical) modelling approach. However, you may not want to call linear regression a machine learning algorithm because there is no iterative learning involved. Furthermore, machine learning differs from traditional statistical modelling methods in that it makes no assumptions regarding the data generation process and underlying distributions (Breiman, 2001).\nNevertheless, contrasting a bivariate linear regression model with a complex machine learning algorithm is instructive. Also linear regression provides a prediction \\(\\hat{Y} = f(X)\\), just like other (proper) machine learning algorithms do. The functional form of a bivariate linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters (typically on the order of \\(10^4 - 10^7\\)). You can imagine that their high flexibility allows these types of algorithms to very effectively learn from the data, but also bears the risk of overfitting. What is overfitting?\n\n\n10.3.2 Overfitting\nThis example is based on this example from scikit-learn.\nLet’s assume that there is some true underlying relationship between a single predictor \\(X\\) and the target variable \\(Y\\). We don’t know this relationship and the observations contain a (normally distributed) error. Based on our training data, we fit three polynomial models that differ with respect to their complexity. We fit a polynomial of degree 1, 4, and 15 to the observations. A polynomial of degree \\(N\\) is given by: \\[\ny = \\sum_{n=0}^N a_n x^n\n\\] \\(a_n\\) are the coefficients, i.e., model parameters. The goal of the training is to find the coefficients \\(a_n\\) so that the predicted \\(\\hat{Y}\\) fits observed \\(Y\\) best. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple bivariate linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case.\n\n\n\n\n\n\n\n\n\nWe can use the same fitted models on data that was not used for model fitting - the test data. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points \\(X_\\text{new}\\) and add a new sample of errors on top of the true relationship.\n\n\n\n\n\n\n\n\n\nYou see that, using the test set, we find that “poly4” actually performs best - it has a much lower RMSE than “poly15”. Apparently, “poly15” was overfitted. Apparently, it used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has the implication that, when this model is used for making predictions for data that was not used for training, it will yield misguided predictions that are affected by the errors in the training set. This is the reason why “poly15” performed worse on the test set than the other models.\nFrom the figures above, we can also conclude that “poly1” was underfitted - it performed worse than “poly4” also on the validation set.\nThe out-of-sample performance of “poly15” gets even worse when applying the fitted polynomial models to data that extends beyond the range in \\(X\\) that was used for model training. Here, we’re extending just 20% to the right.\n\n\n\n\n\n\n\n\n\nYou see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fitted the data best when we considered only the training results. This is a fundamental challenge in machine learning - finding the model with the best generalisability. That is, a model that not only fits the training data well, but also performs well on unseen data.\nThe phenomenon of fitting and overfitting as a function of the model complexity is also referred to as the bias-variance trade-off. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In machine learning (as in all statistical modelling), we are challenged to balance this trade-off.\nThis Chapter and Chapter 11 introduce the methods for achieving the best model generalisability and find the sweet spot between high bias and high variance. One of the key steps of the machine learning modelling process is motivated by the example above: the separation of the data into a training and a testing set (data splitting). Only by withholding part of the data from the model training, we have a good basis for testing the model on that unseen data for evaluating its generalisability. Additional steps that may be required or beneficial for effective model training and their implementation in R are introduced in this and the next Chapter. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in \\(f(X)\\) or to quantify the importance of different predictors in our model. This is referred to as model interpretation and is not (currently) included in this book.\nOf course, a plethora of algorithms exist that do the job of \\(Y = f(X)\\). Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger number of machine learning algorithms. For illustration purposes in this chapter, we will use and introduce the K-nearest-Neighbors (KNN) algorithm and compare its performance to a multivariate linear regression for illustration purposes. Chapter 12 introduces Random Forest.\n\n\n10.3.3 Data and the modelling challenge\nWe’re returning to ecosystem flux data that we’ve used in Chapter 4 and Chapter 5. Here, we’re using daily data from the evergreen site in Davos, Switzerland (CH-Dav) to avoid effects of seasonally varying foliage cover for which the data does not contain information. To address such additional effects, we would have to, for example, combine the flux and meteorological data with remotely sensed surface greenness data.\nThe data set FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv contains a time series of the ecosystem gross primary production (GPP) and a range of meteorological variables, measured in parallel. In this chapter, we formulate a model for predicting GPP from a set of covariates (other variables that vary in parallel, here the meteorological variables). This is to say that GPP_NT_VUT_REF is the target variable, and other variables that are available in our dataset are the predictors.\nLet’s read the data, select suitable variables, interpret missing value codes, and select only good-quality data (where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled).\n\ndaily_fluxes &lt;- read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") |&gt;  \n  \n  # select only the variables we are interested in\n  dplyr::select(TIMESTAMP,\n                GPP_NT_VUT_REF,    # the target\n                ends_with(\"_QC\"),  # quality control info\n                ends_with(\"_F\"),   # includes all all meteorological covariates\n                -contains(\"JSB\")   # weird useless variable\n                ) |&gt;\n\n  # convert to a nice date object\n  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt;\n\n  # set all -9999 to NA\n  mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; \n  \n  # retain only data based on &gt;=80% good-quality measurements\n  # overwrite bad data with NA (not dropping rows)\n  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF),\n                TA_F           = ifelse(TA_F_QC        &lt; 0.8, NA, TA_F),\n                SW_IN_F        = ifelse(SW_IN_F_QC     &lt; 0.8, NA, SW_IN_F),\n                LW_IN_F        = ifelse(LW_IN_F_QC     &lt; 0.8, NA, LW_IN_F),\n                VPD_F          = ifelse(VPD_F_QC       &lt; 0.8, NA, VPD_F),\n                PA_F           = ifelse(PA_F_QC        &lt; 0.8, NA, PA_F),\n                P_F            = ifelse(P_F_QC         &lt; 0.8, NA, P_F),\n                WS_F           = ifelse(WS_F_QC        &lt; 0.8, NA, WS_F)) |&gt; \n\n  # drop QC variables (no longer needed)\n  dplyr::select(-ends_with(\"_QC\"))\n\n\nTo reproduce this code chunk, you can download the file FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here.\n\nThe steps above are considered data wrangling and are not part of the modelling process. After completing this tutorial, you will understand this distinction.\n\n\n10.3.4 K-nearest neighbours\nBefore we start with the model training workflow, let’s introduce the K-nearest neighbour (KNN) algorithm. It serves the purpose of demonstrating the bias-variance trade-off. As the name suggests, KNN uses the \\(k\\) observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (for regression) or most frequent value (for classification) and uses it as the prediction of the target value. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (GPP_NT_VUT_REF ~ .), KNN would determine the \\(k\\) days (rows in a data frame) where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points \\(X_a\\) and \\(X_b\\), considering all \\(P\\) predictors \\(j\\).\nEuclidean distance: \\[\n\\sqrt{ \\sum_{j=1}^P (X_{a,j} - X_{b,j})^2  } \\\\\n\\]\nManhattan distance: \\[\n\\sum_{j=1}^P | X_{a,j} - X_{b,j} |\n\\]\nIn two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point \\(a\\) to point \\(b\\) in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. \\(|x|\\) is the absolute value of \\(X\\) ( \\(|-x| = x\\)).\nKNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model “training” has to be done for each prediction step and the computation time of the training increases with \\(x \\times p\\). KNNs are often used, for example, to impute values (fill missing values, see also below) and have the advantage that predicted values are always within the range of observed values of the target variable.\n\n\n10.3.5 Model formulation\nThe aim of supervised machine learning is to find a model \\(\\hat{Y} = f(X)\\) so that \\(\\hat{Y}\\) agrees well with observations \\(Y\\). We typically start with a research question where \\(Y\\) is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or “features”) \\(X\\) are recorded along with \\(Y\\). From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on a set of abiotic factors, mostly meteorological measurements.\n\n10.3.5.1 Formula notation\nIn R, it is common to use the formula notation to specify the target and predictor variables. You have encountered formulas before, e.g., for a linear regression using the lm() function. To specify a linear regression model for GPP_NT_VUT_REF with three predictors SW_IN_F, VPD_F, and TA_F, to be fitted to data daily_fluxes, we write:\n\nlm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes)\n\n\n\n10.3.5.2 The generic train()\nThe way we formulate a model can be understood as being independent of the algorithm, or engine, that takes care of fitting \\(f(X)\\). The R package {caret} provides a unified interface for using different machine learning algorithms implemented in separate packages. In other words, it acts as a wrapper for multiple different model fitting, or machine learning algorithms. This has the advantage that it unifies the interface - the way arguments are provided and outputs are returned. {caret} also provides implementations for a set of commonly used tools for data processing, model training, and evaluation. We’ll use {caret} here for model training with the function train(). Note however, that using a specific algorithm, which is implemented in a specific package outside {caret}, also requires that the respective package be installed and loaded. Using {caret} for specifying the same linear regression model as above, the base-R lm() function, can be done with {caret} in a generalized form as:\n\ncaret::train(\n  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, \n  data = daily_fluxes |&gt; drop_na(),  # drop missing values\n  trControl = caret::trainControl(method = \"none\"),  # no resampling\n  method = \"lm\"\n)\n\nLinear Regression \n\n2729 samples\n   3 predictor\n\nNo pre-processing\nResampling: None \n\n\nNote the argument specified as trControl = trainControl(method = \"none\"). This suppresses the default approach to model fitting in {caret} - to resample using bootstrapping. More on that in Chapter 11. Note also that we dropped all rows that contained at least one missing value - necessary to apply the least squares method for the linear regression model fitting. It’s advisable to apply this data removal step only at the very last point of the data processing and modelling workflow. Alternative algorithms may be able to deal with missing values and we want to avoid losing information along the workflow.\nOf course, it is an overkill to write this as in the code chunk above compared to just writing lm(...). The advantage of the unified interface is that we can simply replace the method argument to use a different model fitting algorithm. For example, to use KNN, we just can write:\n\ncaret::train(\n  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, \n  data = daily_fluxes |&gt; drop_na(), \n  trControl = caret::trainControl(method = \"none\"),\n  method = \"knn\"\n)\n\nk-Nearest Neighbors \n\n2729 samples\n   3 predictor\n\nNo pre-processing\nResampling: None \n\n\n\n\n\n10.3.6 Data splitting\nThe introductory example demonstrated the importance of validating the fitted model with data that was not used for training. Thus, we can test the model’s generalisability to new (“unseen”) data. The essential step that enables us to assess the model’s generalization error is to hold out part of the data from training and set it aside (leaving it absolutely untouched!) for testing.\nThere is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance a trade-off:\n\nSpending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model.\nSpending too much data for validation will leave us with too little data for training. In this case, the machine learning algorithm may not be successful at finding real relationships due to insufficient amounts of training data.\n\nTypical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal, but come at the cost of adding to the already high computational burden of model training.\nIn environmental sciences, the number of predictors is often smaller than the sample size (\\(p &lt; n\\)), because it is typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number \\(p\\) gets large, it is important, and for some algorithms mandatory, to maintain \\(p &lt; n\\) for model training.\nAn important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are well represented in training and testing sets. A particularly challenging case is posed when it is of particular interest that the algorithm learns relationships \\(f(X)\\) under rare conditions \\(X\\), for example meteorological extreme events. If not addressed with particular measures, model training tends to achieve good model performance for the most common conditions. A simple way to put more emphasis for model training on extreme conditions is to compensate by sampling overly proportional from such cases for the training data set.\nSeveral alternative functions for the data splitting step are available from different packages in R. We use the the rsample package here as it allows to additionally make sure that data from the full range of a given variable’s values (VPD_F in the example below) are well covered in both training and testing sets.\n\nset.seed(123)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = \"VPD_F\")\ndaily_fluxes_train &lt;- rsample::training(split)\ndaily_fluxes_test &lt;- rsample::testing(split)\n\nPlot the distribution of values in the training and testing sets.\n\nplot_data &lt;- daily_fluxes_train |&gt; \n  dplyr::mutate(split = \"train\") |&gt; \n  dplyr::bind_rows(daily_fluxes_test |&gt; \n  dplyr::mutate(split = \"test\")) |&gt; \n  tidyr::pivot_longer(cols = 2:9, names_to = \"variable\", values_to = \"value\")\n\nplot_data |&gt; \n  ggplot(aes(x = value, y = ..density.., color = split)) +\n  geom_density() +\n  facet_wrap(~variable, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n10.3.7 Pre-processing\nData pre-processing is aimed at preparing the data for use in a specific model fitting procedure and at improving the effectiveness of model training. The splitting of the data into a training and test set makes sure that no information from the test set is used during or before model training. It is important that absolutely no information from the test set finds its way into the training set (data leakage).\nIn a general sense, pre-processing involve data transformations where the transformation functions use parameters that are determined on the data itself. Consider, for example, the standardization. That is, the linear transformation of a vector of values to have zero mean (data is centered, \\(\\mu = 0\\)) and a standard deviation of 1 (data is scaled to \\(\\sigma = 1\\)). In order to avoid data leakage, the mean and standard deviation have to be determined on the training set only. Then, the normalization of the training and the test sets both use the set of (\\(\\mu, \\sigma\\)) determined on the training set. Data leakage would occur if the (\\(\\mu, \\sigma\\)) would be determined on data containing values from the test set.\nOften, multiple splits of the data are considered during model training. Hence, an even larger number of data transformation parameters (\\(\\mu, \\sigma\\) in the example of normalization) have to be determined and transformations applied to the multiple splits of the data. {caret} deals with this for you and the transformations do not have to be “manually” applied before applying the train() function call. Instead, the data pre-processing is considered an integral step of model training and instructions are specified as part of the train() function call and along with the un-transformed data.\nThe {recipes} package provides an even more powerful way for specifying the formula and pre-processing steps in one go. It is compatible with the train() function of {caret}. For the same formula as above, and an example where the data daily_fluxes_train is to be normalized (centered and scaled), we can specify a “recipe” using the pipe operator as:\n\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\nThe first line with the recipe() function call assigns roles to the different variables. GPP_NT_VUT_REF is an outcome (in “{recipes} speak”). Then, we used selectors to apply the recipe step to several variables at once. The first selector, all_numeric(), selects all variables that are either integers or real values. The second selector, -all_outcomes() removes any outcome (target) variables from this recipe step. The returned object pp does not contain a normalized version of the data frame daily_fluxes_train, but rather the information that allows us to apply a specific set of pre-processing steps also to any other data set.\nThe object pp can then be supplied to train() as its first argument:\n\ncaret::train(\n  pp, \n  data = daily_fluxes_train, \n  method = \"knn\",\n  trControl = caret::trainControl(method = \"none\")\n)\n\nThe example above showed data standardization as a data pre-processing step. Data pre-processing may be done with different aims, as described in sub-sections below.\n\n10.3.7.1 Standardization\nSeveral algorithms explicitly require data to be standardized so that values of all predictors vary within a comparable range. The necessity of this step becomes obvious when considering KNN, where the magnitude of the distance is strongly influenced by the order of magnitude of the predictor values. Here are the ranges and quantiles of the available variables.\n\ndaily_fluxes |&gt; \n  summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |&gt; \n  t() |&gt; \n  as_tibble(rownames = \"variable\") |&gt; \n  setNames(c(\"variable\", \"min\", \"q25\", \"q50\", \"q75\", \"max\"))\n\n# A tibble: 8 × 6\n  variable           min     q25    q50    q75    max\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 GPP_NT_VUT_REF  -4.23    0.773   2.87   5.45  12.3 \n2 TA_F           -21.9    -1.47    3.51   8.72  20.7 \n3 SW_IN_F          3.30   77.8   135.   214.   366.  \n4 LW_IN_F        138.    243.    279.   308.   365.  \n5 VPD_F            0.001   0.959   2.23   4.06  16.6 \n6 PA_F            80.4    83.2    83.7   84.1   85.6 \n7 P_F              0       0       0      1.6   92.1 \n8 WS_F             0.405   1.56    1.93   2.34   6.54\n\n\nWe see for example, that typical values of LW_IN_F are by a factor 100 larger than values of VPD_F. A distance calculated based on these raw values would therefore be strongly dominated by the difference in LW_IN_F values, and differences in VPD_F would hardly affect the distance. Therefore, the data must be standardized before using it with the KNN algorithm (and other algorithms, including Neural Networks). Standardization is done to each variable separately, by centering and scaling each to have \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nThe steps for centering and scaling using the recipes package are described above.\nStandardization can be done not only by centering and scaling (as described above), but also by scaling to within range, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1.\nAs seen above for the feature engineering example, the object pp does not contain a standardized version of the data frame daily_fluxes_train, but rather the information that allows us to apply the same standardization also to other data. In other words, recipe(..., data = daily_fluxes_train) |&gt; step_center(...) |&gt; step_scale(...) doesn’t actually transform daily_fluxes_train. There are two more steps involved to get there. This might seem bothersome at first but their separation is critical in the context of model training and data leakage, and translates the conception of the pre-processing as a “recipe” into the way we write the code.\nTo actually transform the data, we first have to “prepare” the recipe:\n\npp_prep &lt;- recipes::prep(pp, training = daily_fluxes_train) \n\nFinally we can actually transform the data. That is, “juice” the prepared recipe.\n\ndaily_fluxes_juiced &lt;- recipes::juice(pp_prep)\n\nNote, if we are to apply the prepared recipe to new data, we’ll have to bake() it.\n\ndaily_fluxes_baked &lt;- recipes::bake(pp_prep, new_data = daily_fluxes_train)\n\n# confirm that juice and bake return identical objects when given the same data\nall_equal(daily_fluxes_juiced, daily_fluxes_baked)\n\nWarning: `all_equal()` was deprecated in dplyr 1.1.0.\nℹ Please use `all.equal()` instead.\nℹ And manually order the rows/cols as needed\n\n\n[1] TRUE\n\n\nThe effect is of standardization is illustrated by comparing original and transformed variables:\n\n# prepare data for plotting\nplot_data_original &lt;- daily_fluxes_train |&gt; \n  dplyr::select(one_of(c(\"SW_IN_F\", \"VPD_F\", \"TA_F\"))) |&gt; \n  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = \"var\", values_to = \"val\")\n\nplot_data_juiced &lt;- daily_fluxes_juiced |&gt; \n  dplyr::select(one_of(c(\"SW_IN_F\", \"VPD_F\", \"TA_F\"))) |&gt; \n  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = \"var\", values_to = \"val\")\n\n# plot density\nplot_1 &lt;- ggplot(data = plot_data_original, aes(val, ..density..)) +\n  geom_density() +\n  facet_wrap(~var)\n\n# plot density by var\nplot_2 &lt;- ggplot(data = plot_data_juiced, aes(val, ..density..)) +\n  geom_density() +\n  facet_wrap(~var)\n\n# combine both plots\ncowplot::plot_grid(plot_1, plot_2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n10.3.7.2 Handling missing data\nSeveral machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This can lead to severe data loss. In cases where missing values appear predominantly in only a few variables, it may be advantageous to drop the affected variable from the data for modelling. In other cases, it may be advantageous to fill missing values (data imputation, see next section). Although such imputed data is “fake”, it may be preferred to impute values than to drop entire rows and thus get the benefit of being able to use the information contained in available (real) values of affected rows. Whether or not imputation is preferred should be determined based on the model skill for an an out-of-sample test (more on that later).\nVisualizing missing data is the essential first step in making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal).\n\nvisdat::vis_miss(\n  daily_fluxes,\n  cluster = FALSE, \n  warn_large_data = FALSE\n  )\n\n\n\n\n\n\n\n\nHere, the variable LW_IN_F (longwave radiation) if affected by a lot of missing data. Note that we applied a data cleaning step along with the data read-in at the very top of this Chapter. There, we applied a filtering criterion where values are only retained if at least 80% of the underlying half-hourly data is actual measured (and not gap-filled) data. Whether to drop the variable for further modelling should be informed also by our understanding of the data and the processes relevant for the modelling task. Here, the modelling target is GPP and the carbon cycle specialists among the readers may know that longwave radiation is not a known important control on GPP (ecosystem photosynthesis). Therefore, we may consider dropping this variable from the dataset for our modelling task. The remaining variables are affected by less frequent missingness with which we will deal otherwise.\n\n\n10.3.7.3 Imputation\nImputation refers to the replacement of missing values with with a “best guess” value (Boehmke and Greenwell). Different approaches exist for determining that best guess. The most basic approach is to impute missing values with the mean or median of the available values of the same variable, which can be implemented using step_impute_*() from the {recipes} package. For example, to impute the median for all predictors separately:\n\npp |&gt; \n  step_impute_median(all_predictors())\n\nImputing by the mean or median is “uninformative”. We may use information about the co-variation of multiple variables for imputing missing values. For example, for imputing missing VPD values, we may consider the fact that VPD tends to be high when air temperature is high. Therefore, missing VPD values can be modeled as a function of other co-variates (predictors). Several approaches to modelling missing values are available through the {recipes} package (see here). For example, we can use KNN with five neighbors as:\n\npp |&gt; \n  step_impute_knn(all_predictors(), neighbors = 5)\n\n\n\n10.3.7.4 One-hot encoding\nFor machine learning algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common such transformation is one-hot encoding, where a categorical predictor variable that has \\(N\\) levels is replaced by \\(N\\) new variables that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. Because this creates perfect collinearity between these new column, we can also drop one of them. This is referred to as dummy encoding. The example below demonstrates what one-hot encoding does.\n\n# original data frame\ndf &lt;- tibble(id = 1:4, color = c(\"red\", \"red\", \"green\", \"blue\"))\ndf\n\n# A tibble: 4 × 2\n     id color\n  &lt;int&gt; &lt;chr&gt;\n1     1 red  \n2     2 red  \n3     3 green\n4     4 blue \n\n# after one-hot encoding\ndmy &lt;- dummyVars(\"~ .\", data = df, sep = \"_\")\ndata.frame(predict(dmy, newdata = df))\n\n  id colorblue colorgreen colorred\n1  1         0          0        1\n2  2         0          0        1\n3  3         0          1        0\n4  4         1          0        0\n\n\nNote that in a case where color is strictly one of c(\"red\", \"red\", \"green\", \"blue\") (and not, for example, \"yellow\"), then one of the columns added by dummyVars() is obsolete (if it’s neither \"red\", nor \"green\", it must be \"blue\") - columns are collinear. This can be avoided by setting fullRank = FALSE.\nUsing the recipes package, one-hot encoding is implemented by:\n\nrecipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |&gt; \n  step_dummy(all_nominal(), one_hot = TRUE)\n\n\n\n10.3.7.5 Zero-variance predictors\nSometimes, the data generation process yields variables that have the same value in each observation. And sometimes this is due to failure of the measurement device or some other bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “near-zero variance”. That is, variables where only a few unique values occur with a high frequency in the entire data set. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset.\nWe can test for zero-variance or near-zero variance predictors by quantifying the following metrics:\n\nFrequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones.\nPercent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100).\n\nThe function nearZeroVar of the {caret} package flags suspicious variables (zeroVar = TRUE or nzv = TRUE). In our data set, we don’t find any:\n\ncaret::nearZeroVar(daily_fluxes, saveMetrics = TRUE)\n\n               freqRatio percentUnique zeroVar   nzv\nTIMESTAMP       1.000000    100.000000   FALSE FALSE\nGPP_NT_VUT_REF  1.000000     93.732887   FALSE FALSE\nTA_F            1.000000     83.951932   FALSE FALSE\nSW_IN_F         1.500000     95.375723   FALSE FALSE\nLW_IN_F         1.000000     43.170064   FALSE FALSE\nVPD_F           1.142857     60.450259   FALSE FALSE\nPA_F            1.090909     37.906906   FALSE FALSE\nP_F            10.268072      5.978096   FALSE FALSE\nWS_F            1.083333     34.758138   FALSE FALSE\n\n\nUsing the recipes package, we can add a step that removes zero-variance predictors by:\n\npp |&gt; \n  step_zv(all_predictors())\n\n\n\n10.3.7.6 Target engineering\nTarget engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about prediction errors or when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1). A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range are “discounted” in their weight compared to errors in the lower range.\nIn our data set, the variable WS_F (wind speed) is skewed. The target variable that we have considered so far (GPP_NT_VUT_REF) is not skewed. In a case where we would consider WS_F to be our target variable, we would thus consider applying a log-transformation.\n\nplot_1 &lt;- ggplot(data = daily_fluxes, aes(x = WS_F, y = ..density..)) +\n  geom_histogram() +\n  labs(title = \"Original\")\n\nplot_2 &lt;- ggplot(data = daily_fluxes, aes(x = log(WS_F), y = ..density..)) +\n  geom_histogram() +\n  labs(title = \"Log-transformed\")\n\ncowplot::plot_grid(plot_1, plot_2)\n\n\n\n\n\n\n\n\nLog transformation as part of the pre-processing is specified using the step_log() function, here applied to the model target variable (all_outcomes()).\n\nrecipes::recipe(WS_F ~ ., data = daily_fluxes) |&gt;   # it's of course non-sense to model wind speed like this\n  recipes::step_log(all_outcomes())\n\nA log-transformation doesn’t necessarily result in a perfect normal distribution of transformed values. The Box-Cox can get us closer. It can be considered a generalization of the log-transformation. Values are transformed according to the following function:\n\\[\ny(\\lambda) = \\begin{cases}\n\\frac{Y^\\lambda-1}{\\lambda}, &\\; y \\neq 0\\\\\n\\log(Y),                     &\\; y = 0\n\\end{cases}\n\\]\n\\(\\lambda\\) is treated as a parameter that is fitted such that the resulting distribution of values \\(Y\\) approaches the normal distribution. To specify a Box-Cox-transformation as part of the pre-processing, we can use step_BoxCox() from the {recipes} package.\n\npp &lt;- recipes::recipe(WS_F ~ ., data = daily_fluxes_train) |&gt;\n  recipes::step_BoxCox(all_outcomes())\n\nHow do transformed values look like?\n\nprep_pp &lt;- recipes::prep(pp, training = daily_fluxes_train |&gt; drop_na())\ndaily_fluxes_baked &lt;- bake(prep_pp, new_data = daily_fluxes_test |&gt; drop_na())\ndaily_fluxes_baked |&gt;\n  ggplot(aes(x = WS_F, y = ..density..)) +\n  geom_histogram() +\n  labs(title = \"Box-Cox-transformed\")\n\n\n\n\n\n\n\n\nNote that the Box-Cox-transformation can only be applied to values that are strictly positive. In our example, wind speed (WS_F) is. If this is not satisfied, a Yeo-Johnson transformation can be applied.\n\nrecipes::recipe(WS_F ~ ., data = daily_fluxes) |&gt;\n  recipes::step_YeoJohnson(all_outcomes())\n\n\n\n\n10.3.8 Putting it all together (half-way)\nLet’s recap. We have a dataset daily_fluxes and we want to predict ecosystem GPP (GPP_NT_VUT_REF) from a set of predictors - environmental covariates that were measured in parallel to GPP. Let’s compare the performance of a multivariate linear regression and KNN model in terms of its generalisation to data that was not used for model fitting. The following pieces are implemented:\n\nMissing data: We’ve seen that the predictor LW_IN_F has lots of missing values and - given a priori knowledge is not critical for predicting GPP and we’ll drop it.\nData cleaning: Data (daily_fluxes) was cleaned based on quality control information upon reading the data at the beginning of this Chapter. Before modelling, we’re checking the distribution of the target value here again to make sure it is “well-behaved”.\nImputation: We drop rows with missing data for model training, instead of imputing them.\nSome of the predictors are distintively not normally distributed. Let’s Box-Cox transform all predictors as a pre-processing step.\nWe have to standardize the data in order to use it for KNN.\nWe have no variable where zero-variance was detected and we have no categorical variables that have to be transformed by one-hot encoding to be used in KNN.\nWe use a data split, whithholding 30% for testing.\nFit two models: a linear regression model and KNN.\nTake \\(k=10\\) for the KNN model. Other choices are possible and will affect the prediction error on the training and the testing data in different manners. We’ll learn more about the optimal choice of \\(k\\) (hyperparameter tuning) in the next chapter.\nFit models to minimize the root mean square error (RMSE) between predictions and observations. More on the choice of the metric argument in train() (loss function) in the next chapter.\nFor the KNN model, use \\(k=8\\).\n\nThese steps are implemented by the code below.\n\n# Data cleaning: looks ok, no obviously bad data\n# no long tail, therefore no further target engineering\ndaily_fluxes |&gt; \n  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + \n  geom_histogram()\n\n\n\n\n\n\n\n# Data splitting\nset.seed(1982)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = \"VPD_F\")\ndaily_fluxes_train &lt;- rsample::training(split)\ndaily_fluxes_test &lt;- rsample::testing(split)\n\n# Model and pre-processing formulation, use all variables but LW_IN_F\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, \n                      data = daily_fluxes_train |&gt; drop_na()) |&gt; \n  recipes::step_BoxCox(recipes::all_predictors()) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\n# Fit linear regression model\nmod_lm &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train |&gt; drop_na(), \n  method = \"lm\",\n  trControl = caret::trainControl(method = \"none\"),\n  metric = \"RMSE\"\n)\n\n# Fit KNN model\nmod_knn &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train |&gt; drop_na(), \n  method = \"knn\",\n  trControl = caret::trainControl(method = \"none\"),\n  tuneGrid = data.frame(k = 8),\n  metric = \"RMSE\"\n)\n\nWe can use the model objects mod_lm and mod_knn to add the fitted values to the training and the test data, both using the generic function predict(..., newdata = ...). The code below implements the prediction step, the measuring of the prediction skill, and the visualisation of predicted versus observed values on the test and training sets, bundled into one function - eval_model() - which we will re-use for each fitted model object.\n\n# make model evaluation into a function to reuse code\neval_model &lt;- function(mod, df_train, df_test){\n  \n  # add predictions to the data frames\n  df_train &lt;- df_train |&gt; \n    drop_na()\n  df_train$fitted &lt;- predict(mod, newdata = df_train)\n  \n  df_test &lt;- df_test |&gt; \n    drop_na()\n  df_test$fitted &lt;- predict(mod, newdata = df_test)\n  \n  # get metrics tables\n  metrics_train &lt;- df_train |&gt; \n    yardstick::metrics(GPP_NT_VUT_REF, fitted)\n  \n  metrics_test &lt;- df_test |&gt; \n    yardstick::metrics(GPP_NT_VUT_REF, fitted)\n  \n  # extract values from metrics tables\n  rmse_train &lt;- metrics_train |&gt; \n    filter(.metric == \"rmse\") |&gt; \n    pull(.estimate)\n  rsq_train &lt;- metrics_train |&gt; \n    filter(.metric == \"rsq\") |&gt; \n    pull(.estimate)\n  \n  rmse_test &lt;- metrics_test |&gt; \n    filter(.metric == \"rmse\") |&gt; \n    pull(.estimate)\n  rsq_test &lt;- metrics_test |&gt; \n    filter(.metric == \"rsq\") |&gt; \n    pull(.estimate)\n  \n  # visualise as a scatterplot\n  # adding information of metrics as sub-titles\n  plot_1 &lt;- ggplot(data = df_train, aes(GPP_NT_VUT_REF, fitted)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dotted\") +\n    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~\n                            RMSE == .(format(rmse_train, digits = 3))),\n         title = \"Training set\") +\n    theme_classic()\n  \n  plot_2 &lt;- ggplot(data = df_test, aes(GPP_NT_VUT_REF, fitted)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dotted\") +\n    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~\n                            RMSE == .(format(rmse_test, digits = 3))),\n         title = \"Test set\") +\n    theme_classic()\n  \n  out &lt;- cowplot::plot_grid(plot_1, plot_2)\n  \n  return(out)\n}\n\n# linear regression model\neval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)\n\n\n\n\nEvaluation of the linear regression and the KNN models on the training and the test set.\n\n\n\n# KNN\neval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)\n\n\n\n\nEvaluation of the linear regression and the KNN models on the training and the test set.\n\n\n\n\nIt is advisable to keep workflow notebooks (this RMarkdown file) light and legible. Therefore, code chunks should not be excessively long and functions should be kept in a ./R/*.R file, which can be loaded. This also facilitates debugging code inside the function. Here, the function eval_model() is part of the book’s git repository, stored in the sub-directory ./R/, and used also in later chapters.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised machine learning I</span>"
    ]
  },
  {
    "objectID": "supervised_ml_I.html#exercises",
    "href": "supervised_ml_I.html#exercises",
    "title": "10  Supervised machine learning I",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\nThere are no exercises with provided solutions for this Chapter.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised machine learning I</span>"
    ]
  },
  {
    "objectID": "supervised_ml_I.html#report-exercises",
    "href": "supervised_ml_I.html#report-exercises",
    "title": "10  Supervised machine learning I",
    "section": "10.5 Report Exercises",
    "text": "10.5 Report Exercises\n\nComparison of the linear regression and KNN models\nThe figures above show the evaluation of the model performances of the linear regression and the KNN model, evaluated on the training and test set. This exercise is to interpret and understand the observed differences. Implement the following points:\n\nAdopt the code from this Chapter for fitting and evaluating the linear regression model and the KNN into your own RMarkdown file. Name the file ./vignettes/re_ml_01.Rmd. Keep larger functions in a separate file in an appropriate directory and load the function definition as part of the RMarkdown.\nInterpret observed differences in the context of the bias-variance trade-off:\n\n\nWhy is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?\nWhy does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?\nHow would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?\n\n\nVisualise temporal variations of observed and modelled GPP for both models, covering all available dates.\n\n\n\nThe role of k\nLet’s look at the role of \\(k\\) in a KNN. Answer the following questions:\n\nBased on your understanding of KNN (and without running code), state a hypothesis for how the \\(R^2\\) and the MAE evaluated on the test and on the training set would change for \\(k\\) approaching 1 and for \\(k\\) approaching \\(N\\) (the number of observations in the data). Explain your hypothesis, referring to the bias-variance trade-off.\nPut your hypothesis to the test! Write code that splits the data into a training and a test set and repeats model fitting and evaluation for different values for \\(k\\). Visualise results, showing model generalisability as a function of model complexity. Describe how a “region” of overfitting and underfitting can be determined in your visualisation. Write (some of your) code into a function that takes \\(k\\) as an input and and returns the MAE determined on the test set.\nIs there an “optimal” \\(k\\) in terms of model generalisability? Edit your code to determine an optimal \\(k\\).\n\nAdd code and text for addressing this exercise to the file ./vignettes/re_ml_01.Rmd and give the notebook a suitable structure for easy navigation with a table of content (toc) by modifying its YAML header:\n\nImportant: to find an optimal \\(k\\), you will have to use daily data and not half-hourly data! Hint: Do not produce various of the “Training - Test” Figures shown above to find an optimal \\(k\\). Find a suitable plot that shows the optimal \\(k\\) (maybe you can find one in this or another Chapter…).\n\n---\ntitle: \"Report Exercise Chapter 10\"\nauthor: \"Ziggy Stardust\"\noutput:\n  html_document:\n    toc: true\n---\n\n\nDeliverables for the report\nPresent your solutions in a file called re_ml01.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised machine learning I</span>"
    ]
  },
  {
    "objectID": "supervised_ml_II.html",
    "href": "supervised_ml_II.html",
    "title": "11  Supervised machine learning II",
    "section": "",
    "text": "11.1 Learning objectives\nChapter lead author: Benjamin Stocker\nIn the Chapter 10, you learned how the data are pre-processed, the model fitted, and how the model’s generasbility to unseen data is tested. In the exercises of Chapter 10, you learned how the bias-variance trade-off of a model, a KNN, can be controlled and that the choice of model complexity has different implications of the model’s performance on the training and the test sets. A “good” model generalises well. That is, it performs well on unseen data.\nIn this chapter, you will learn more about the process of model training, the concept of the loss, and how we can chose the right level of model complexity for optimal model generalisability as part of the model training step. This completes your set of skills for your first implementations a supervised machine learning workflow.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised machine learning II</span>"
    ]
  },
  {
    "objectID": "supervised_ml_II.html#setup",
    "href": "supervised_ml_II.html#setup",
    "title": "11  Supervised machine learning II",
    "section": "11.2 Setup",
    "text": "11.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(recipes)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised machine learning II</span>"
    ]
  },
  {
    "objectID": "supervised_ml_II.html#tutorial",
    "href": "supervised_ml_II.html#tutorial",
    "title": "11  Supervised machine learning II",
    "section": "11.3 Tutorial",
    "text": "11.3 Tutorial\n\n11.3.1 Data and the modelling challenge\nWe’re using the same data and address the same modelling challenge as in Chapter 10. Let’s load the data, wrangle a bit, specify the same model formulation, and the same pre-processing steps as in the previous Chapter 10.\n\ndaily_fluxes &lt;- readr::read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") |&gt;  \n  \n  # select only the variables we are interested in\n  dplyr::select(TIMESTAMP,\n                GPP_NT_VUT_REF,    # the target\n                ends_with(\"_QC\"),  # quality control info\n                ends_with(\"_F\"),   # includes all all meteorological covariates\n                -contains(\"JSB\")   # weird useless variable\n                ) |&gt;\n\n  # convert to a nice date object\n  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt;\n\n  # set all -9999 to NA\n  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; \n\n  \n  # retain only data based on &gt;=80% good-quality measurements\n  # overwrite bad data with NA (not dropping rows)\n  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF),\n                TA_F           = ifelse(TA_F_QC        &lt; 0.8, NA, TA_F),\n                SW_IN_F        = ifelse(SW_IN_F_QC     &lt; 0.8, NA, SW_IN_F),\n                LW_IN_F        = ifelse(LW_IN_F_QC     &lt; 0.8, NA, LW_IN_F),\n                VPD_F          = ifelse(VPD_F_QC       &lt; 0.8, NA, VPD_F),\n                PA_F           = ifelse(PA_F_QC        &lt; 0.8, NA, PA_F),\n                P_F            = ifelse(P_F_QC         &lt; 0.8, NA, P_F),\n                WS_F           = ifelse(WS_F_QC        &lt; 0.8, NA, WS_F)) |&gt; \n\n  # drop QC variables (no longer needed)\n  dplyr::select(-ends_with(\"_QC\"))\n\n# Data splitting\nset.seed(123)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = \"VPD_F\")\ndaily_fluxes_train &lt;- rsample::training(split)\ndaily_fluxes_test &lt;- rsample::testing(split)\n\n# The same model formulation is in the previous chapter\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, \n                      data = daily_fluxes_train) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\n\nTo reproduce this code chunk, you can download the file FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here.\n\n\n\n11.3.2 Loss function\nModel training in supervised machine learning is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between \\(\\hat{Y}\\) and \\(Y\\). The loss function quantifies this mismatch (\\(L(\\hat{Y}, Y)\\)), and the algorithm (“optimiser” in Fig. 10.1 of Chapter 10) takes care of progressively reducing the loss during model training.\nLet’s say the machine learning model contains two parameters and predictions can be considered a function of the two (\\(\\hat{Y}(w_1, w_2)\\)). \\(Y\\) is actually constant. Thus, the loss function is effectively a function \\(L(w_1, w_2)\\). Therefore, we can consider the model training as a search of the parameter space to find the minimum of the loss. The parameter space spanned by all possible combinations of \\((w_1, w_2)\\). Common loss functions are the root mean square error (RMSE), or the mean square error (MSE), or the mean absolute error (MAE). Loss minimization is a general feature of ML model training.\n\n\n\n\n\n\nFigure 11.1: Visualization of a loss function.\n\n\n\nModel training is implemented in R for different machine learning algorithms in different packages. Some algorithms are even implemented by multiple packages. As described in Chapter 10) the {caret} package provides “wrappers” that handle a large selection of different machine learning model implementations in different packages with a unified interface (see here for an overview of available models). The {caret} function train() is the center piece also in terms of specifying the loss function as the argument metric. It defaults to the RMSE for regression models and the accuracy for classification.\n\n\n11.3.3 Hyperparameters\nPractically all machine learning algorithms have some “knobs” to turn for controlling a model’s complexity and other features of the model training. The optimal choice of these “knobs” is to be found for efficient model performance. Such “knobs” are the hyperparameters. Each algorithm comes with its own, specific hyperparameters.\nFor KNN, k is the (only) hyperparameter. It specifies the number of neighbours to consider for determining distances. There is always an optimum \\(k\\). Obviously, if \\(k = n\\), we consider all observations as neighbours and each prediction is simply the mean of all observed target values \\(Y\\), irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with \\(k = 1\\), the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data.\nHyperparameters usually have to be “tuned”. The optimal setting depends on the data and can therefore not be known a priori. Below is a visualisation of the loss (MAE) on the training and on the test set for different choices of \\(k\\).\n\n\n\n\n\n\n\n\n\n\n\n\nThis illustrates that the model performance on the training set keeps improving as the model variance (as opposed to bias) increases - here as we go towards smaller \\(k\\). However, what counts for measuring out-of-sample model performance is the evaluation on the test set, which deteriorates with increasing model variance beyond a certain point.\nAlthough decisive for the generalisability of the model, we cannot evaluate its performance on the test set during model training. We have set that data aside and must leave it untouched to have a basis for evaluating the model performance on unseen data after training. What can we do to determine the optimal hyperparameter choice during model training, estimating its performance on the test set?\n\n\n11.3.4 Resampling\nThe solution is to split the training data again - now into a training and a validation set. Using the validation set, we can “mimick” out-of-sample predictions during training by determining the loss on the validation set and use that for guiding the model training. However, depending on the volume of data we have access to, evaluation metrics determined on the validation set may not be robust. Results may vary depending on the split into training and validation data. This makes it challenging for reliably estimating out-of-sample performance.\nA solution for this problem is to resample the training data into a number training-validation splits, yielding several pairs of training and validation data. Model training is then guided by minimising the average loss determined on the different resamples. Having multiple resamples (multiple folds of training-validation splits) avoids the loss minimization from being misguided by random peculiarities in the training and/or validation data.\nWhether or not a resampling is applied, depends on the data volume and computational costs of the model training which increase linearly with the number of resamples. For models that take days-weeks to train, resampling is not a realistic option. However, for many machine learning applications in Geography and Environmental Sciences, models are much less costly and resampling is viable and desirable approach to model training.\nThe most important methods of resampling are bootstrapping (not explained here, but see Boehmke and Greenwell (2020)) and k-fold cross validation. An advantage of bootstrapping is that it provides an estimation of the distribution of the training error (without the need for data distribution assumptions because it’s non parametric), which informs not only the magnitude of the training error but also the variance of this estimate. Nevertheless, this statistical approach can become very computationally intensive because it needs many resamples with replacement and model runs. Hence cross validation lends itself more to model training.\nIn k-fold cross validation, the training data is split into k equally sized subsets (folds). (Don’t confuse this k with the k in KNN.) Then, there will be k iterations, where each fold is used for validation once (while the remaining folds are used for training). An extreme case is leave-one-out cross validation, where k corresponds to the number of data points.\n\n\n\n\n\n\nFigure 11.2: K-fold cross validation. From Boehmke and Greenwell (2020).\n\n\n\nTo do a k-fold cross validation during model training in R, we don’t have to implement the loops around folds ourselves. The resampling procedure can be specified in the {caret} function train() with the argument trControl. The object that this argument takes is the output of a function call to trainControl(). This can be implemented in two steps. For example, to do a 10-fold cross-validation, we can write:\n\n\n\nset.seed(1982)\nmod_cv &lt;- caret::train(pp, \n                       data = daily_fluxes_train |&gt; drop_na(), \n                       method = \"knn\",\n                       trControl = caret::trainControl(method = \"cv\", number = 10),\n                       tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),\n                       metric = \"MAE\")\n\n\n\n# generic plot of the caret model object\nggplot(mod_cv)\n\n\n\n\n\n\n\n# generic print\nprint(mod_cv)\n\nk-Nearest Neighbors \n\n1910 samples\n   8 predictor\n\nRecipe steps: center, scale \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1719, 1718, 1718, 1719, 1720, 1718, ... \nResampling results across tuning parameters:\n\n  k    RMSE      Rsquared   MAE     \n    2  1.758390  0.5643266  1.331563\n    5  1.579795  0.6321707  1.199857\n   10  1.539660  0.6480432  1.163468\n   15  1.522924  0.6549458  1.155488\n   20  1.518210  0.6568319  1.152947\n   25  1.516940  0.6574409  1.151223\n   30  1.517698  0.6570899  1.153631\n   35  1.517916  0.6570457  1.153421\n   40  1.517559  0.6573507  1.153832\n   60  1.522451  0.6556145  1.161452\n  100  1.535410  0.6508217  1.177483\n\nMAE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 25.\n\n\nFrom the output of print(mod_cv), we get information about model performance for each hyperparameter choice. The values reported are means of respective metrics determined across the ten folds. Also the optimal choice of \\(k\\) (25) is reported. Does this correspond to the \\(k\\) with the best performance determined on the test set? If resampling was a good approach to estimating out-of-sample model performance, then it should!\n\ndf_mae |&gt;    # see hint\n  filter(set == \"test\") |&gt; \n  filter(.estimate == min(.estimate))\n\n# A tibble: 1 × 6\n    idx .metric .estimator .estimate     k set  \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1     7 mae     standard        1.15    30 test \n\n\n\nThe object df_mae is created above, recording metrics determined on the training and the validation set for differen choices of \\(k\\). The respective code is hidden for the book. Explore the source code here.\n\nThe evaluation on the test set suggests that \\(k=30\\) is optimal, while 10-fold cross-validation yielded an optimal \\(k = 25\\). Apparently, cross-validation suggested a model that is slightly more on the variance side along the bias-variance trade-off than the evaluation on the test set did. This (relatively small) mismatch is probably a result of randomness in the data.\nLet’s look at the results as we did in Chapter 10. The model object mod_cv contains information about the whole hyperparameter search and also about the choice of the best hyperparameter value. When using the object in the predict() function (as used inside eval_model()), it automatically uses the model trained with the optimal \\(k\\).\n\neval_model(mod = mod_cv, df_train = daily_fluxes_train, df_test = daily_fluxes_test)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nRemember that in Chapter 10, we used \\(k=8\\) and got an \\(R^2=0.72\\) on the training set and \\(R^2=0.64\\) on the test set. The results with the optimal choice of \\(k=25\\) yield an \\(R^2=0.69\\) on the training set and \\(R^2=0.65\\) on the test set - poorer than with \\(k=8\\) on the training set but slightly better on the test set.\n\n\n11.3.5 Validation versus testing data\nA source of confusion can be the distinction of validation and testing data. They are different things. The validation data is used during model training. The model fitting and the selection of the optimal hyperparameters is based on comparing predictions with the validation data. Hence, evaluations of true out-of-sample predictions should be done with a portion of the data that has never been used during the model training process (see Figure 11.3).\n\n\n\n\n\n\nFigure 11.3: Figure adopted form Google Machine Learning Crash Course.\n\n\n\n\n\n11.3.6 Modeling with structured data\nA fundamental assumption underlying many machine learning algorithms and their training setup is that the data are independent and identically distributed (iid). This means that each observation is generated by the same process, follows the same distribution, and is independent from its neighboring point or any other data point. This assumption is often made in statistical models to simplify the analysis and to ensure the validity of various mathematical results used in machine learning algorithms. In reality, this is often not satisfied. In Chapter 4, we dealt with structure in the data in the context of formatting and aggregating data frames. Such structures are often also relevant for modelling and what it means for a model to “generalize well”. Remember that structure in data arises from similarity of the subjects generating the data. Such structures and their relation to the modelling task should be considered when choosing the model algorithm, formulating the model, and when implementing the training a testing setup.\nConsider, for example, the time series of ecosystem fluxes and meterological covariates in our dataset daily_fluxes. When using this data to train a KNN or a linear regression model, we implicitly assume that the data is iid. We assumed that there is a true function \\(f(X)\\) that generates the target data \\(Y\\) and that can be used to predict under any novel condition \\(X_\\text{new}\\). However, in reality, this may not be the case. \\(f(X)\\) may change over time. For example, over the course of a season, the physiology of plants changes (think phenology) and may lead to temporally varying relationships between \\(X\\) and \\(Y\\) that are not captured by temporal variations in \\(X\\) itself - the relationships are not stationary.\nWorking with geographic and environmental data, we often deal with temporal dependencies between predictors and the target data. In our data daily_fluxes of ecosystem fluxes and meteorological covariates, this may be, as mentioned, arising from phenological changes in plant physiology and structure, or caused by a lasting effects weather extremes (e.g., a late frost event in spring). In hydrological data, temporal dependencies between weather and streamflow are generated by catchment characteristics and the runoff generation processes. Such temporal dependencies violate the independence assumption. Certain machine learning algorithms (e.g., recurrent neural networks) offer a solution for such cases and are suited for modelling temporal dependencies or, in general, sequential data where the order of the records matters (e.g., language models that consider the order of words in a text). Training-testing and cross-validation splits for sequential data have to preserve the order of the data in the subsets. In this case, the splits have to be done by blocks. That is, model generalisability is to be assessed by training on one block of the time series and testing on the remaining block. Note that the splitting method introduced in Chapter 10 using rsample::initial_split() assumes that the data is iid. In the function, data points are randomly drawn and allocated to either the test or the training subset. It is therefore not applicable for splitting data with temporal dependencies. Modelling temporal dependencies will be dealt in future (not currently available) chapters of this book.\nOther dependencies may arise from the spatial context. For example, a model for classifying an atmospheric pressure field as a high or a low pressure system uses information about the spatial arrangement of the data - in this case raster data. A model predicts one value (‘high pressure system’ or ‘low pressure system’) per pressure field. Such modelling tasks are dealt with yet another class of algorithms (e.g., convolutional neural networks).\nSpatial or group-related structure in the data may arise if, in general, the processes generating the data, cannot be assumed to be identical and lead to identically distributed data across groups. For example, in the data daily_fluxes_allsites_nested_joined from Chapter 4, time series of ecosystem fluxes and meteorological covariates are provided for a set of different sites. There, the group structure is linked to site identity. Similarly, streamflow data may be available for multiple catchments. However, considering the between-catchment variations in soil, terrain, vegetation, and geology, a model may not yield accurate predictions when trained by data from one catchment and applied to a different catchment.\nTo evaluate model generalisability to a new site or catchment (not just a new time step within a single site or catchment), this structure has to be taken into consideration. In this, case, data splits of training and validation or testing subsets are to be separated along blocks, delineated by the similar groups of data points (by sites, or by catchments). That is, training data from a given site (or catchment) should be either in the training set or in the test (or validation) set, but not in both.\nThis illustrates that the data structure and the modelling aim (generalisability in what respect?) have to be accounted for when designing the data split and resampling strategy. The {caret} function groupKFold() offers the solution for such cases, creating folds for cross-validation that respect group delineations. In other cases, such grouping structure may not be evident and may not be reflected by information accessible for modelling. For example, we may be able to separate time series from different sites but we don’t know whether sites are sufficiently independent to be able to consider the test metric to reflect the true uncertainty in predicting to an entirely new location which is neither in the test nor training set. In such cases, creative solutions have to be found and appropriate cross-validations have to be implemented with a view to the data structure and modelling aim.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised machine learning II</span>"
    ]
  },
  {
    "objectID": "supervised_ml_II.html#exercises",
    "href": "supervised_ml_II.html#exercises",
    "title": "11  Supervised machine learning II",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\n\nCross-validation by hand\nIn the tutorial we “built on shoulder of giants” - people that went through the struggle of writing a robust package to implement a cross validation. Although we can and should use such packages, we still have to understand how a cross validation works in detail.\nWrite a function that implements n-fold cross-validation for KNN with \\(k=30\\). (We write ‘n-fold CV’ here to avoid confusion with the k in KNN, but mean the same as described in Section 11.3.4.) The function should take as arguments the training data object, n for specifying the number of folds (use \\(n=60\\)), the name of the target variable, the names of the predictor variables as a character vector, and the \\(k\\) for KNN. The function should return a vector of length n, containing the MAE evaluated on the n folds. To randomize the split of data points into folds, first “re-shuffle” the rows in the training data as part of the function. Centering and scaling of the training and validation data should be applied manually before getting the KNN model object within each fold.\nAs data, use daily ecosystem flux data from here. As target variable, use \"GPP_NT_VUT_REF\". As predictor variables, use c(\"TA_F\", \"SW_IN_F\", \"VPD_F\").\nVisualise the distribution of the validation errors (MAE) across folds.\n\nHint: Center and scale the predictors (but not the target data) within each fold.\n\n\nHint: Use read.csv() to obtain a “nude” data frame (not a tibble) and use the syntax df[,\"variable_name\"] to extract a vector from a data frame. This makes your life easier.\n\n\nHint: To determine the row indices to be allocated to each fold, you may find a solution using the base-R function split() or you may use caret::createFolds().\n\n\nHint: Write two functions: One as asked for above and one that implements steps that are carried out repeatedly for each fold, given row indices of the validation set of the respective fold.\n\n\nHint: Since you are doing the cross-validation “by hand” and perform no hyperparameter tuning, we don’t need the caret::train() function. Instead, use caret::knnreg(x, y, ...) for creating the model object on the training data (with x and y being data frames) and the generic predict() for predicting on the validation set within each fold.\n\n\n\n11.4.1 Cross-validation vs. test error\nNow, you can use the user-friendly caret::train() for KNN with 60-fold cross-validation and tuning the hyperparameter k. Use the MAE as the loss metric. Use the same data as in the Exercise above and withhold 20% of the data for the test set. Visually compare the reported mean MAE from the cross-validation folds with the MAE determined on a test set.\nIn your visual comparison, add a plot layer showing the distribution of validation errors from you manual implementation of cross-validation (Exercise above).\n\nHint: The returned model object from a caret::train() call is a list with fold-wise metrics in its element results.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised machine learning II</span>"
    ]
  },
  {
    "objectID": "supervised_ml_II.html#report-exercise",
    "href": "supervised_ml_II.html#report-exercise",
    "title": "11  Supervised machine learning II",
    "section": "11.5 Report Exercise",
    "text": "11.5 Report Exercise\nHere, we explore the role of structure in the data for model generalisability and how to best estimate a “true” out-of-sample error that corresponds to the prediction task. The task here is to train a KNN-model on ecosystem flux observations from one site and predict to another site. In previous examples and exercises, we always trained and predicted within the same site. How well is a model generalisable to a new site; can we use our model for spatial upscaling? Solve these task and save your reproducible solution in ./vignettes/re_ml02.rmd (don`t forget to upload the data and html):\n\nTo investigate the concept of generalisability, we use flux data from two sites: Davos (CH-Dav) (FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv), and Laegern (CH-Lae) (FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv). Gather information on the characteristics of these sites - how do they differ regarding climate, vegetation, altitude, etc.\nTrain three KNN-models on each dataset: Davos only, Laegern only, both together. For each model: Use 80% of the data to train a KNN-model with an optimal \\(k\\) and set aside the remaining 20% for testing.\nTest your models on each of the three test datasets that you set aside. Create three tables (see example below), one for each model and display the evaluation metrics on all test sets.\n\nExample table:\n\n\n\n\n\n\n\n\nModel trained on data from [insert sitename/s]\n\\(R^2\\)\n\\(RMSE\\)\n\n\nEvaluation against Laegern test set\n\n\n\n\nEvaluation against Davos test set\n\n\n\n\nEvaluation against Laegern & Davos test set\n\n\n\n\n\n\nUse your knowledge on statistical learning and on the different sites to answer the following questions:\n\nWhat are the patterns that you see in the tables?\nHow well do models trained on one site perform on another site? Why is this the case?\n\nTo answer this think of the differences between the sites! Interpret the models’ biases on the out-of-sample prediction with respect to the site’s charactersitics. Perhaps you could identify such biases by comparing the time-series of both models on the same test set?\n\nHow does the model trained on both sites perform on the three test sets? Why is this the case?\nWhen training and testing on both sites, is this a true ‘out-of-sample’ setup? What would you expect if your model was tested against data from a site in Spain?\n\n\n\nHint: To make the models comparable, make sure that you are using the same variables during training. Check your data before you start modelling!\n\n\nHint: Do not produce multiple “Training-Test” plots shown throughout this course. Find a way to convey your message in concise way and pick these plots only when you really need them to support you answer.\n\n\nHint: Consult the RMarkdown cheatsheet or other online resources to learn how to create a Markdown table.\n\n\nDeliverables for the report\nPresent your solutions in a file called re_ml02.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised machine learning II</span>"
    ]
  },
  {
    "objectID": "randomforest.html",
    "href": "randomforest.html",
    "title": "12  Random Forest",
    "section": "",
    "text": "12.1 Learning objectives\nChapter lead author: Benjamin Stocker\nIn this chapter you will learn how to implement a Random Forest model.\nYou will learn:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "randomforest.html#learning-objectives",
    "href": "randomforest.html#learning-objectives",
    "title": "12  Random Forest",
    "section": "",
    "text": "the principles of a decision tree, and purpose of bagging\nhow decision trees make up a Random Forest",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "randomforest.html#setup",
    "href": "randomforest.html#setup",
    "title": "12  Random Forest",
    "section": "12.2 Setup",
    "text": "12.2 Setup\nIn this Chapter, we will need the following libraries\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(recipes)\nlibrary(lubridate)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "randomforest.html#tutorial",
    "href": "randomforest.html#tutorial",
    "title": "12  Random Forest",
    "section": "12.3 Tutorial",
    "text": "12.3 Tutorial\n\n12.3.1 Decision trees\nJust as a forest consists of trees, a Random Forest model consists of decision trees. Therefore, let’s start with a decision tree, also known as CART (classification and regression tree). Consider a similar example as in Chapter 10 where we fitted a function \\(Y = f(X)\\) with polynomials. Instead, here we fit it with a decision tree. The tree grows by successively splitting (branching) the predictor range (\\(X\\)) into binary classes (two regions along \\(X\\)) and predicting a different and constant value \\(c\\) for the target variable on either side of the split. The location of the split is chosen such that the overall error between the observed response (\\(Y_i\\)) and the predicted constant (\\(c_i\\)) is minimized. The error is determined based on whether we’re dealing with a regression or a classification problem.\n\nFor regression problems, the sum of square errors is minimized.\n\n\\[\n    \\text{SSE} = \\sum_i{(c_i-Y_i)^2}\n\\]\n\nFor classification problems, the cross-entropy or the Gini index are typically maximized. Both are measures of the difference between probability distributions (main ideas in this blogpost).\n\n\n\n\n\n\nA (very) shallow decision tree.\n\n\n\n\n\n\n\nA (very) shallow decision tree.\n\n\n\n\nAs a tree grows, splits are recursively added. In our example, only one predictor variable is available. The splits are therefore performed always on the same variable, splitting up \\(X\\) further.\n\n\n\n\n\nA decision tree of depth 3.\n\n\n\n\n\n\n\nA decision tree of depth 3.\n\n\n\n\nIn the visualisation of the decision tree above, the uppermost decision is the root node. From there, two branches connect to internal nodes, and at the bottom of the tree are the leaf nodes. (The nature-aware reader may note that leaves are typically at the top, and roots at the bottom of a tree. Nevermind.)\nTypically, multiple predictor variables are available. For each split, the variable and the location of the split along that variable is determined to satisfy the respective criteria for regression and classification.\nDecision trees are high variance learners. That is, as the maximum tree depth is increased, the variance of predictions increases continuously. In other words, the depth of a tree controls the model complexity and the bias-variance trade-off. With excessive depth, decision trees tend to overfit.\n\n\n\n\n\nAn overfitting decision tree.\n\n\n\n\n\n\n\nAn overfitting decision tree.\n\n\n\n\nAn advantage of decision trees is that they require minimal pre-processing of the data and they are robust to outliers. This is thanks to their approach of converting continuous variables into binary classes for predictions. Hence, they can naturally handle a mix of continuous and categorical predictor variables. Furthermore, predictions are not sensitive to the distance of a predictor variable’s value to a respective variable’s split location. This makes decision trees robust to outliers. It also implies that predictions to unseen data points that lie outside the range of values in the training data (extrapolation) are conservative.\nThe disadvantage is, as demonstrated above, the tendency towards high variance of predictions when models get complex (deep trees). And thus, decision trees tend to be outperformed by other algorithms.\n\n\n12.3.2 Bagging\nThe approach of bagging is to smooth out the high variance of decision tree predictions by averaging over multiple, slightly different trees. Differences between the trees are introduced by bagging, that is, by training each individual tree only on a bootstrapped sample of the full training data. Here, a decision tree has the role of a base learner and bagging takes an ensemble approach. Final predictions are then taken as the average (for regression) or the most frequent class (for classification) across all trees’ predictions.\nBagging is particularly effective when the base learner tends to have a high variance. The variance of predictions is continuously reduced with an increasing number of decision trees over which averages are taken and no tendency to overfit results from increasing the number of trees. However, the computational cost linearly increases with the number of trees and the gain in model performance levels out relatively quickly. Bagging also limits the interpretability. We can no longer visualise the fitted model with an intuitive graphical decision tree as done above.\n\n\n12.3.3 From trees to a forest\nWhile bagging reduces the variance in predictions, limits to predictive performance remain. This is linked to the fact that, although a certain degree of randomness is introduced by sub-sampling the training data for each tree, individual trees often remain relatively similar. This is particularly expressed if variations in the target variable are controlled primarily by variations in a small number of predictor variables. In this case, decision trees tend to be built by splits on the same variable, irrespective of which bootstrapped sample the individual tree is trained with.\nRandom Forest solves this problem by introducing an additional source of randomness: Only a subset of the predictor variables are considered at each split. This strongly reduces the similarity of individual trees (and also reduces computational costs) and leads to improved predictive performance.\nThe number of variables to consider at each split is a hyperparameter of the Random Forest algorithm, commonly named \\(m_\\text{try}\\). In the example below, we use the implementation in the {ranger} package (wrapped with {caret}), where the respective model fitting function has a hyperparameter mtry. Common default values are \\(m_\\text{try} = p/3\\) for regression and \\(m_\\text{try} = \\sqrt{p}\\) for classification, where \\(p\\) is the number of predictors. Model complexity is controlled by the depth of the trees. Depending on the implementation of the Random Forest algorithm, this is governed not by explicitly specifying the tree depth, but by setting the number of observations in the leaf node. In the {ranger} package, the respective hyperparamter is min.node.size. The number of trees is another hyperparameter and affects predictions similarly as described above for bagging.\nA great strength of Random Forest is, inherited by the characteristics of its underlying decision trees, its minimal requirement of data pre-processing, its capability of dealing with continuous and categorical variables, and its robustness to outliers. With the default choices of \\(m_\\text{try}\\), Random Forest provides very good out-of-the-box performance. However, the hyperparameters of Random Forest have interactive effects and should be searched systematically.\n\n# Data loading and cleaning\nsource(\"R/eval_model.R\")\n\n# read daily fluxes\ndaily_fluxes &lt;- readr::read_csv(\n  \"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\"\n  )\n\n# select only the variables we are interested in\n# such as the target (GPP_NT_VUT_REF), the\n# quality control parameters ending in QC,\n# the meteorological covariates ending in F,\n# while removing (-) a weird useless variable JSB\n# \n# Subsequently convert dates to a nice date format\n# set all -9999 to NA and only retain values with\n# QC value of &gt; 0.8\ndaily_fluxes &lt;- daily_fluxes |&gt;  \n  \n  # select only the variables we are interested in\n  dplyr::select(TIMESTAMP,\n                GPP_NT_VUT_REF,    # the target\n                ends_with(\"_QC\"),  # quality control info\n                ends_with(\"_F\"),   # includes all all meteorological covariates\n                -contains(\"JSB\")   # weird useless variable\n                ) |&gt;\n  \n  # convert to a nice date object\n  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt;\n  \n  # set all -9999 to NA\n  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; \n  \n  # retain only data based on &gt;=80% good-quality measurements\n  # overwrite bad data with NA (not dropping rows)\n  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF),\n                TA_F           = ifelse(TA_F_QC        &lt; 0.8, NA, TA_F),\n                SW_IN_F        = ifelse(SW_IN_F_QC     &lt; 0.8, NA, SW_IN_F),\n                LW_IN_F        = ifelse(LW_IN_F_QC     &lt; 0.8, NA, LW_IN_F),\n                VPD_F          = ifelse(VPD_F_QC       &lt; 0.8, NA, VPD_F),\n                PA_F           = ifelse(PA_F_QC        &lt; 0.8, NA, PA_F),\n                P_F            = ifelse(P_F_QC         &lt; 0.8, NA, P_F),\n                WS_F           = ifelse(WS_F_QC        &lt; 0.8, NA, WS_F)) |&gt; \n  # drop QC variables (no longer needed)\n  dplyr::select(-ends_with(\"_QC\"))\n\n# Data splitting\nset.seed(123)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = \"VPD_F\")\nddf_train &lt;- rsample::training(split)\nddf_test &lt;- rsample::testing(split)\n\n# The same model formulation is in the previous chapter\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, \n                      data = ddf_train) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\nmod &lt;- train(\n  pp, \n  data = ddf_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  trControl = trainControl(method = \"cv\", number = 5, savePredictions = \"final\"),\n  tuneGrid = expand.grid( .mtry = floor(6 / 3),\n                          .min.node.size = 5,\n                          .splitrule = \"variance\"),\n  metric = \"RMSE\",\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 2000,           # high number ok since no hperparam tuning\n  seed = 1982                # for reproducibility\n)\n\n\n# generic print\nprint(mod)\n\nRandom Forest \n\n1910 samples\n   8 predictor\n\nRecipe steps: center, scale \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1528, 1528, 1529, 1527, 1528 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.411155  0.7047382  1.070285\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of variance\n\nTuning parameter 'min.node.size' was held constant at a value of 5\n\n\n\neval_model(mod = mod, df_train = ddf_train, df_test = ddf_test)\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n12.3.4 Out-of-bag prediction\nA further advantage of Random Forests is that the bagging, i.e., sub-setting the training data as part of the algorithm, provides a direct way to estimate the out-of-sample (in Random Forest-speak the “out-of-bag”) prediction. Remember that bagging implies the use of only part of the data for building the decision trees. The remainder of the data can be used to estimate that tree’s out-of-sample (“out-of-bag”) prediction. The respective estimate is generated by the {ranger} implementation of Random Forest and can be accessed as part of the fitted model object. If the data is sufficiently large, the out-of-bag prediction should approximate a k-fold cross-validation.\n\n# OOB prediction error of the final model\nsqrt(mod$finalModel$prediction.error)\n\n[1] 1.40572\n\n# RMSE of 5-fold cross-validation\nmod$results$RMSE\n\n[1] 1.411155\n\n\nIn other words, Random Forest has another advantage: Avoiding the need for cross-validation reduces the computational burden of model training - by factor \\(1/k\\) for k-fold cross-validation",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "randomforest.html#exercises",
    "href": "randomforest.html#exercises",
    "title": "12  Random Forest",
    "section": "12.4 Exercises",
    "text": "12.4 Exercises\n\nFitting a Random Forest\nFit a Random Forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (num.trees), each with a minimum number of observations per leaf of 5 (min.node.size). You can consult the respective arguments for the \"ranger\" method typing ?ranger.\nRepeat the fitting with 1000 decision trees and minimum node size of 5, then with 12 decision trees and a minimum node size of 1. Then, discuss the role that the number of decision trees and the minimum number of leaf observations of a tree play in the bias-variance trade-off and in the computation time.\n\n\nHyperparameter tuning\nIn a previous tutorial, you learned how to tune the hyperparameter \\(k\\) in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a Random Forest. The {caret} package allows to vary three hyperparameters:\n\nmtry: The number of variables to consider to make decisions at each node, often taken as \\(p/3\\) for regression, where \\(p\\) is the number of predictors.\nmin.node.size: The number of data points at the “bottom” of each decision tree, i.e. the leaves.\nsplitrule: The function applied to data in each branch of a tree, used for determining the goodness of a decision.\n\nAnswer the following questions, giving a reason for your responses:\n\nCheck the help for the ranger() function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search.\nIn the previous exercise, you have seen how the minimum node size regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when min.node.size = 1 and when min.node.size = n (n being the number of observations)? Note that it’s not necessary to provide the max.depth argument to train() because min.node.size is already limiting the size of the trees in the Random Forests.\nGreedy hyperparameter tuning: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search. Implement the optimization routine yourself, using loops.\n\n\nTip: Keep the number of trees low, otherwise it takes too long to fit each Random Forest model.\n\n\nGrid hyperparameter tuning: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the expand.grid() function to create a data.frame of hyperparameter value combinations. This grid will be passed to train() via the tuneGrid argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of train() and the results of the hyperparameter search.\nCompare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches?\n\n\n\nModel performance\nYou have trained several Random Forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and \\(R^2\\) on the training and the test set, does it show overfitting?",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "interpretable-ml.html",
    "href": "interpretable-ml.html",
    "title": "13  Interpretable Machine Learning",
    "section": "",
    "text": "13.1 Setup\nA great advantage of machine learning models is that they can capture non-linear relationships and interactions between predictors, and that they are effective at making use of large data volumes for learning even faint but relevant patterns thanks to their flexibility (high variance). However, their flexibility, and thus complexity, comes with the trade-off that models are hard to interpret. They are essentially black-box models - we know what goes in and we know what comes out and we can make sure that predictions are reliable (as described in previous chapters). However, we don’t understand what the model learned. In contrast, a linear regression model can be easily interpreted by looking at the fitted coefficients and their statistics.\nThis motivates interpretable machine learning. There are two types of model interpretation methods: model-specific and model-agnostic interpretation. A simple example for a model-specific interpretation method is to compare the t-values of the fitted coefficients in a least squares linear regression model. Here, we will focus on the model-agnostic machine learning model interpretation and cover two types of model interpretations: quantifying variable importance, and determining partial dependencies (functional relationships between the target variable and a single predictor, while all other predictors are held constant).\nWe re-use the Random Forest model object which we created in Chapter 12. As a reminder, we predicted GPP from different environmental variables such as temperature, short-wave radiation, vapor pressure deficit, and others.\nIn this Chapter, we will need the following libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(recipes)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Interpretable Machine Learning</span>"
    ]
  },
  {
    "objectID": "interpretable-ml.html#variable-importance",
    "href": "interpretable-ml.html#variable-importance",
    "title": "13  Interpretable Machine Learning",
    "section": "13.2 Variable importance",
    "text": "13.2 Variable importance\nA model-agnostic way to quantify variable importance is to permute (shuffle) the values of an individual predictor, re-train the model, and measure by how much the skill of the re-trained model has degraded in comparison to the model trained on the un-manipulated data. The metric, or loss function, for quantifying the model degradation can be any suitable metric for the respective model type. For a model predicting a continuous variable, we may use the RMSE. The algorithm works as follows (taken from Boehmke & Greenwell (2019)):\n\n\n1. Compute loss function L for model trained on un-manipulated data\n2. For predictor variable i in {1,...,p} do\n     | Permute values of variable i.\n     | Fit model.\n     | Estimate loss function Li.\n     | Compute variable importance as Ii = Li/L or Ii = Li - L0.\n   End\n3. Sort variables by descending values of Ii.\nThis is implemented by the {vip} package. Note that the {vip} package has model-specific algorithms implemented but also takes model-agnostic arguments as done below.\n\nvip::vip(rf_mod,                        # Fitted model object\n         train = rf_mod$trainingData |&gt; \n           dplyr::select(-TIMESTAMP),   # Training data used in the model\n         method = \"permute\",            # VIP method\n         target = \"GPP_NT_VUT_REF\",     # Target variable\n         nsim = 5,                      # Number of simulations\n         metric = \"RMSE\",               # Metric to assess quantify permutation\n         sample_frac = 0.75,            # Fraction of training data to use\n         pred_wrapper = predict         # Prediction function to use\n         )\n\n\n\n\n\n\n\n\nThis indicates that shortwave radiation (‘SW_IN_F’) is the most important variable for modelling GPP here. I.e., the model performance degrades most (the RMSE increases most) if the information in shortwave radiation is lost. On the other extreme, atmospheric pressure adds practically no information to the model. This variable may therefore well be dropped from the model.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Interpretable Machine Learning</span>"
    ]
  },
  {
    "objectID": "interpretable-ml.html#partial-dependence-plots",
    "href": "interpretable-ml.html#partial-dependence-plots",
    "title": "13  Interpretable Machine Learning",
    "section": "13.3 Partial dependence plots",
    "text": "13.3 Partial dependence plots\nWe may not only want to know how important a certain variable is for modelling, but also how it influences the predictions. Is the relationship positive or negative? Is the sensitivity of predictions equal across the full range of the predictor? Again, model-agnostic approaches exist for determining the functional relationships (or partial dependencies) for predictors in a model. Partial dependence plots (PDP) give insight on the marginal effect of a single predictor variable on the response - all else equal. The algorithm to create PDPs goes as follows (adapted from Boehmke & Greenwell (2019)):\nFor a selected predictor (x)\n1. Construct a grid of N evenly spaced values across the range of x: {x1, x2, ..., xN}\n2. For i in {1,...,N} do\n     | Copy the training data and replace the original values of x with the constant xi\n     | Apply the fitted ML model to obtain vector of predictions for each data point.\n     | Average predictions across all data points.\n   End\n3. Plot the averaged predictions against x1, x2, ..., xj\n\n\n\n\n\n\nFigure 13.1: Visualisation of Partial Dependence Plot algorithm from Boehmke & Greenwell (2019). Here, Gr_Liv_Area is the variable of interest \\(x\\).\n\n\n\nThis algorithm is implemented by the {pdp} package:\n\n# XXX This takes very long\n# The predictor variables are saved in our model's recipe\npreds &lt;- \n  rf_mod$recipe$var_info |&gt; \n  dplyr::filter(role == \"predictor\") |&gt; \n  dplyr::pull(variable)\n\n# The partial() function can take n=3 predictors at max and will try to create\n# a n-dimensional visulaisation to show interactive effects. However, \n# this is computational intensive, so we only look at the simple \n# response-predictor plots\nall_plots &lt;- purrr::map(\n  preds,\n  ~pdp::partial(\n      rf_mod,       # Model to use\n      .,            # Predictor to assess\n      plot = TRUE,  # Whether output should be a plot or dataframe\n      plot.engine = \"ggplot2\"  # to return ggplot objects\n    )\n)\n\npdps &lt;- cowplot::plot_grid(all_plots[[1]], all_plots[[2]], all_plots[[3]], \n                           all_plots[[4]], all_plots[[5]], all_plots[[6]])\n\npdps\n\n\n\n\n\n\n\n\nThese PDPs show that the variables TA_F, SW_IN_F, and LW_IN_F have a strong effect, while VPD_F, P_F, and WS_F have a relatively small marginal effect as indicated by the small range in yhat - in line with the variable importance analysis shown above. In addition to the variable importance analysis, here we also see the direction of the effect and that how the sensitivity varies across the range of the respective predictor. For example, GPP is positively influenced by temperature (TA_F), but the effect really only starts to be expressed for temperatures above about -5\\(^\\circ\\)C, and the positive effect disappears above about 10\\(^\\circ\\)C. The pattern is relatively similar for LW_IN_F, which is sensible because long-wave radiation is highly correlated with temperature. For the short-wave radiation SW_IN_F, we see the saturating effect of light on GPP that we saw in previous chapters.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Interpretable Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Solutions",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#setup",
    "href": "appendix.html#setup",
    "title": "Appendix",
    "section": "Setup",
    "text": "Setup\nFor the solutions we will need the following libraries\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(recipes)\nlibrary(lubridate)",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#getting-started",
    "href": "appendix.html#getting-started",
    "title": "Appendix",
    "section": "Getting Started",
    "text": "Getting Started\n\nDimensions of a circle\n\nGiven the radius of a circle write a few lines of code that calculates its area and its circumference. Run your code with different values assigned of the radius.\n\n\nradius &lt;- 1\narea &lt;- pi * radius^2\ncircum &lt;- 2 * pi * radius\n\n\nPrint the solution as text.\n\n\nprint(paste(\"Radius:\", radius, \"   Circumference:\", circum))\n\n[1] \"Radius: 1    Circumference: 6.28318530717959\"\n\n\n\n\nSequence of numbers\nGenerate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5.\n\nseq(0, pi, length.out = 5)\n\n[1] 0.0000000 0.7853982 1.5707963 2.3561945 3.1415927\n\n\n\n\nGauss sum\nRumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution?\n\nsum(1:100)\n\n[1] 5050\n\n\nGauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R.\n\nvec_a &lt;- 1:50\nvec_b &lt;- 100:51\nvec_c &lt;- vec_a + vec_b\n\n# each element is 101\nvec_c\n\n [1] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n[20] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n[39] 101 101 101 101 101 101 101 101 101 101 101 101\n\n# the length of vectors is fifty. 50 * 101\nsum(vec_c)\n\n[1] 5050\n\n\n\n\nMagic trick algorithm\nDefine a variable named x that contains an integer value and perform the following operations in sequence:\n\nRedefine x by adding 1.\nDouble the resulting number, over-writing x.\nAdd 4 to x and save the result as x.\nRedefine x as half of the previous value of x.\nSubtract the originally chosen arbitrary number from x.\n\nPrint x. Restart the algorithm defined above by choosing a new arbitrary natural number.\n\nx &lt;- -999  # arbitrary integer\nx_save &lt;- x  # save for the last step\nx &lt;- x + 1\nx &lt;- x * 2\nx &lt;- x + 4\nx &lt;- x / 2\nx - x_save\n\n[1] 3\n\n\n\n\nVectors\nPrint the object datasets::rivers and consult the manual of this object.\n\nWhat is the class of the object?\nWhat is the length of the object?\nCalculate the mean, median, minimum, maximum, and the 33%-quantile across all values.\n\n\nclass(datasets::rivers)\n\n[1] \"numeric\"\n\nlength(datasets::rivers)\n\n[1] 141\n\nmean(datasets::rivers)\n\n[1] 591.1844\n\nquantile(datasets::rivers, probs = 0.33)\n\n33% \n342 \n\n\n\n\nData frames\nPrint the object datasets::quakes and consult the manual of this object.\n\nDetermine the dimensions of the data frame using the respective function in R.\nExtract the vector of values in the data frame that contain information about the Richter Magnitude.\nDetermine the value largest value in the vector of event magnitudes.\nDetermine the geographic position of the epicenter of the largest event.\n\n\ndim(datasets::quakes)\n\n[1] 1000    5\n\nvec &lt;- datasets::quakes$mag\nmax(vec)\n\n[1] 6.4\n\nidx &lt;- which.max(vec)  # index of largest value\n\n# geographic positions defined by longitude and latitude (columns long and lat)\ndatasets::quakes$long[idx]\n\n[1] 167.62\n\ndatasets::quakes$lat[idx]\n\n[1] -15.56\n\n\n\n\nWorkspace\nNo solutions provided.",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#programming-primers",
    "href": "appendix.html#programming-primers",
    "title": "Appendix",
    "section": "Programming primers",
    "text": "Programming primers\n\nGauss variations\n\n# for-loop to compute sum from 1 - 100\nsum &lt;- 0\nfor (i in 1:100){\n  sum &lt;- sum + i # for-loop iterating from 1 to 100 \n}\nprint(sum)\n\n[1] 5050\n\n\n\n# while-loop to compute sum from 1 - 100\nloop_status &lt;- TRUE\ncounter &lt;- 0\nsum &lt;- 0\nwhile (loop_status) { # while-loop is repeated as long as loop_status is true\n  counter &lt;- counter + 1\n  sum &lt;- sum + counter\n  if (counter == 100) loop_status &lt;- FALSE\n}\nprint(sum)\n\n[1] 5050\n\n\n\n# Initiate sum variable\nsum &lt;- 0\n\n# Go through loop from 1 to 100\nfor (i in seq(100)) {\n  \n  # Check if the current number a muliple of three and seven\n  # The modulo operator '%%' returns the remainder of a division\n  if (i %% 3 == 0 && i %% 7 == 0 ) {\n    sum &lt;- sum + i\n  } \n} \nprint(paste0(\"The sum of multiples of 3 and 7 within 1-100 is: \", sum))\n\n[1] \"The sum of multiples of 3 and 7 within 1-100 is: 210\"\n\n\n\n\nNested loops\n\nmymat &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, \n              NA, 9, 12, 6, 11, NA, 3, \n              9, 4, 7, 3, 21, NA, 6, \n              rep(NA, 7)),\n            nrow = 4, byrow = TRUE)\nmyvec &lt;- c(8, 4, 12, 9, 15, 6)\n\n\n# Loop over the rows in `mymat`.\nfor (i in 1:nrow(mymat)){\n  \n  # Loop over the columns in `mymat`.\n  for (j in 1:ncol(mymat)){\n    \n    # Check if current value is missing, if so overwrite with max in 'myvec'\n    if (is.na(mymat[i,j])){\n      mymat[i,j] &lt;- max(myvec)\n    }\n  }\n  myvec &lt;- myvec[-which.max(myvec)] # update the vector removing the maximum value\n}\nmymat\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    6    7    3   15   15    6    7\n[2,]   12    9   12    6   11   12    3\n[3,]    9    4    7    3   21    9    6\n[4,]    8    8    8    8    8    8    8\n\n\n\n\nInterpolation\n\n# Set up vector as required in the exercise\nvec &lt;- rep(NA, 100) # initialize vector of length 100 with NA\nvec[1:25] &lt;- 6      # populate first 25 elements of 'vec' with 6. \nvec[66:100] &lt;- -20  # populate elements 66:100 with -20.\n\n# Determine index of last non-missing value before gap\nlast_non_na &lt;- 1\nwhile (!is.na(vec[last_non_na+1])) last_non_na &lt;- last_non_na + 1\n\n# determine index of first non-missing value after gap\nfirst_non_na &lt;- last_non_na + 1\nwhile (is.na(vec[first_non_na])) first_non_na &lt;- first_non_na + 1 \n\n# Get the increment that is needed for interpolation\nlast_value  &lt;- vec[last_non_na]  # Last non-NA value\nfirst_value &lt;- vec[first_non_na] # First non-NA value\ndelta &lt;- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x\n\n# fill missing values incrementally\nfor (i in 2:length(vec)){\n  if (is.na(vec[i])) vec[i] &lt;- vec[i-1] + delta\n}\n\nplot(vec)\n\n\n\n\n\n\n\n# or short using the approx() function:\nvec &lt;- rep(NA, 100) # initialize vector of length 100 with NA\nvec[1:25] &lt;- 6      # populate first 25 elements of 'vec' with 6. \nvec[66:100] &lt;- -20  # populate elements 66:100 with -20.\n\nvec &lt;- approx(1:100, vec, xout = 1:100)\n\nplot(vec)",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#data-wrangling",
    "href": "appendix.html#data-wrangling",
    "title": "Appendix",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nStar wars\n{dplyr} comes with a toy dataset dplyr::starwars (just type it into the console to see its content). Have a look at the dataset with View(). Play around with the dataset to get familiar with the {tidyverse} coding style. Use (possibly among others) the functions dplyr::filter(), dplyr::arrange(), dplyr::pull(), dplyr::select(), dplyr::desc() and dplyr::slice() to answer the following question:\n\nHow many pale characters come from the planets Ryloth or Naboo?\n\n\ndplyr::starwars |&gt; \n  dplyr::filter(\n    skin_color == \"pale\" & \n      (homeworld == \"Naboo\" | homeworld == \"Ryloth\")\n    ) |&gt; \n  nrow()   \n\n[1] 2\n\n\n\nWho is the oldest among the tallest thirty characters?\n\n\ndplyr::starwars |&gt; \n  arrange(desc(height)) |&gt; \n  slice(1:30) |&gt; \n  arrange(birth_year) |&gt; \n  slice(1) |&gt; \n  pull(name)\n\n[1] \"IG-88\"\n\n\n\nWhat is the name of the shortest character and their starship in “Return of the Jedi”?\n\n\ndplyr::starwars |&gt; \n  unnest(films) |&gt; \n  filter(films == \"Return of the Jedi\") |&gt; \n  unnest(starships) |&gt; \n  arrange(height) |&gt; \n  slice(1) |&gt; \n  select(name, starships)\n\n# A tibble: 1 × 2\n  name      starships        \n  &lt;chr&gt;     &lt;chr&gt;            \n1 Nien Nunb Millennium Falcon\n\n\n\n\nAggregating\nYou have learned about aggregating in the {tidyverse}. Let’s put it in practice.\n\nReuse the code in the tutorial to read, reduce, and aggregate the half_hourly_fluxes dataset to the daily scale, calculating the following metrics across half-hourly VPD_F values within each day: mean, 25% quantile, and 75% quantile.\n\n\n# read half hourly fluxes\nhalf_hourly_fluxes &lt;- readr::read_csv(\n  \"data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv\"\n  ) \n\n# Select only variables that we are interested in\nhalf_hourly_fluxes &lt;- half_hourly_fluxes |&gt;\n  dplyr::select(\n    starts_with(\"TIMESTAMP\"),\n    ends_with(\"_F\"),\n    GPP_NT_VUT_REF,\n    NEE_VUT_REF_QC,\n    -starts_with(\"SWC_F_MDS_\"),\n    -contains(\"JSB\")\n  )\n\n# Clean the datetime objects\n# and aggregate to daily scale\ndaily_fluxes &lt;- half_hourly_fluxes |&gt;  \n  dplyr::mutate(\n    date_time = lubridate::ymd_hm(TIMESTAMP_START),\n    date = lubridate::date(date_time)) |&gt;\n  dplyr::group_by(date) |&gt; \n  dplyr::summarise(\n    mean = mean(VPD_F),\n    q25  = quantile(VPD_F, probs = 0.25),\n    q75  = quantile(VPD_F, probs = 0.75)\n  )\n\n\nRetain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles.\n\n\n# In two steps. First, get thresholds of the quantiles\nthresholds &lt;- quantile(daily_fluxes$mean, probs = c(0.1, 0.9))\n\n# Then, filter data to be above/below the upper/lower quantiles and combine\ndaily_fluxes_sub &lt;- daily_fluxes |&gt; \n  \n  # in lower 10% quantile\n  filter(mean &lt; thresholds[1]) |&gt; \n  mutate(qq = \"lower\") |&gt;   # add label\n  \n  # combine\n  bind_rows(\n    \n    daily_fluxes |&gt; \n      # in upper 90% quantile\n      filter(mean &gt; thresholds[2]) |&gt; \n      mutate(qq = \"upper\")\n    )\n\n\nCalculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD.\n\n\ndaily_fluxes_sub |&gt; \n  group_by(qq) |&gt; \n  summarise(\n    q25_mean = mean(q25),\n    q75_mean = mean(q75)\n  )\n\n# A tibble: 2 × 3\n  qq    q25_mean q75_mean\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lower   0.0989    0.149\n2 upper   6.56     15.2  \n\n\n\n\nPatterns in data quality\nThe uncleaned dataset FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv holds half-hourly data that is sometimes of poor quality. Investigate whether NEE data quality is randomly spread across hours in a day by calculating the proportion of (i) actually measured data, (ii) good quality gap-filled data, (iii) medium quality data, and (iv) poor quality data within each hour-of-day (24 hours per day).\n\n# using half_hourly_fluxes read above\ndaily_fluxes &lt;- half_hourly_fluxes |&gt;\n  mutate(TIMESTAMP_START = lubridate::ymd_hm(TIMESTAMP_START)) |&gt; \n  mutate(hour_of_day = lubridate::hour(TIMESTAMP_START)) |&gt; \n  group_by(hour_of_day) |&gt; \n  summarise(n_measured = sum(NEE_VUT_REF_QC == 0),\n            n_good     = sum(NEE_VUT_REF_QC == 1),\n            n_medium   = sum(NEE_VUT_REF_QC == 2),\n            n_poor     = sum(NEE_VUT_REF_QC == 3),\n            n_total    = n()\n            ) |&gt; \n  mutate(f_measured = n_measured / n_total,\n         f_good     = n_good     / n_total,\n         f_medium   = n_medium   / n_total,\n         f_poor     = n_poor     / n_total,\n         )\n\nInterpret your findings: Are the proportions evenly spread across hours in a day?\n\n# this is not asked for but interesting. More on data visualisation in Chapter 5\n# you can also just look at values of df$f_measured over the course of a day (hod)  \n\ndaily_fluxes |&gt; \n  pivot_longer(c(f_measured, f_good, f_medium, f_poor), \n               names_to = \"quality\", \n               values_to = \"fraction\") |&gt; \n  ggplot(aes(x = hour_of_day, \n             y = fraction * 100,     # *100 to get percentages \n             color = quality)) +\n  geom_line(linewidth = 1.5) +       # make lines bit bigger\n  theme_classic() +                  # Pick a nice theme\n  scale_color_brewer(                # Pick a nice color palette\n    \"Quality\",                       # Give legend a title\n    labels = c(\"Good gap filled data\", \"Measured data\", \"Medium gap filled data\", \"Poor gap filled data\"), # Give legend levels a label\n    palette = 3,                     # Pick color palette\n    direction = -1                   # Inverse order of color palette\n  ) +\n  labs(\n    title = \"Temporal pattern of GPP quality\",\n    y     = \"Fraction of total GPP entries [%]\",\n    x     = \"Hour of Day\"\n  )\n\n\n\n\n\n\n\n\nPerform an aggregation of the half-hourly GPP data (variable GPP_NT_VUT_REF) to daily means of the unmodified data read from file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv, and from cleaned data where only measured (not gap-filled) half-hourly data is kept and aggregated. This yields two data frames with daily GPP data. Calculate the overall mean GPP for the two data frames (across all days in the data frame). Are the overall mean GPP values equal? If not, why?\n\ndaily_fluxes_all &lt;- half_hourly_fluxes |&gt;\n  dplyr::mutate(\n    date_time = lubridate::ymd_hm(TIMESTAMP_START),\n    date = lubridate::date(date_time)\n  ) |&gt;\n  dplyr::group_by(date) |&gt; \n  dplyr::summarise(\n    GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE)\n  )\n\ndaily_fluxes_cleaned &lt;- half_hourly_fluxes |&gt;\n  dplyr::mutate(\n    date_time = lubridate::ymd_hm(TIMESTAMP_START),\n    date = lubridate::date(date_time)\n  ) |&gt;\n  dplyr::mutate(\n    GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC == 0, GPP_NT_VUT_REF, NA)\n    ) |&gt; \n  dplyr::group_by(date) |&gt; \n  dplyr::summarise(\n    GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE)\n  )\n\n# overall means\ndaily_fluxes_all |&gt; \n  summarise(\n    GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 1\n  GPP_NT_VUT_REF\n           &lt;dbl&gt;\n1           4.20\n\ndaily_fluxes_cleaned |&gt; \n  summarise(\n    GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 1\n  GPP_NT_VUT_REF\n           &lt;dbl&gt;\n1           7.07",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#data-visualisation",
    "href": "appendix.html#data-visualisation",
    "title": "Appendix",
    "section": "Data Visualisation",
    "text": "Data Visualisation\n\nSpurious data\nIn Section 4.3.10.2 Bad data, we discovered that certain values of GPP_NT_VUT_REF in the half-hourly data half_hourly_fluxes (to be read from file data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv) are repeated with a spuriously high frequency. Determine all values of GPP_NT_VUT_REF that appear more than once in half_hourly_fluxes and label them as being “spurious”. Visualise the time series of the first two years of half-hourly GPP, mapping the information whether the data is spurious or not to the color aesthetic.\n\n# Read and wrangle data\nhalf_hourly_fluxes &lt;- readr::read_csv(\"data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv\") |&gt; \n  \n  # set all -9999 to NA\n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \n                              ~dplyr::na_if(., -9999))) |&gt; \n  \n    # interpret all variables starting with TIMESTAMP as a date-time object\n    dplyr::mutate_at(vars(starts_with(\"TIMESTAMP_\")), lubridate::ymd_hm)\n\nRows: 52608 Columns: 235\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (235): TIMESTAMP_START, TIMESTAMP_END, TA_F_MDS, TA_F_MDS_QC, TA_ERA, TA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# determine spurious GPP_NT_VUT_REF values as those that are duplicated\n# this creates a logical vector specifying whether the respective row has a \n# duplicate\nvec_spurious &lt;- half_hourly_fluxes |&gt; \n  \n  # by keeping only one column, duplicated() determines duplications in \n  # that variable only \n  select(GPP_NT_VUT_REF) |&gt;\n  duplicated()\n\n# label spurious half-hourly data\nhalf_hourly_fluxes &lt;- half_hourly_fluxes |&gt; \n  mutate(spurious = vec_spurious)\n\n# visualise\nggplot(\n    data = half_hourly_fluxes |&gt; slice(1:(48*365)),\n    aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +\n  geom_line() +\n  geom_point(aes(color = spurious), size = 0.9) +\n  labs(x = \"Time\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  scale_color_viridis_d() + # inverse color scale is more intuitive here\n  theme_classic()\n\n\n\n\n\n\n\n\nThen aggregate half-hourly to daily data, taking the mean of GPP_NT_VUT_REF and recording the proportion of underlying half-hourly data points that are “spurious”. Visualise the time series of daily GPP_NT_VUT_REF with the color scale indicating the proportion of spurious half-hourly data that was used for determining the respective date’s mean GPP.\n\n# aggregate\ndaily_fluxes &lt;- half_hourly_fluxes |&gt; \n  mutate(date = lubridate::date(TIMESTAMP_START)) |&gt; \n  group_by(date) |&gt; \n  summarise(frac_spurious = sum(spurious)/48,\n            GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF))\n\n# visualise\nggplot(\n    data = daily_fluxes,\n    aes(x = date, y = GPP_NT_VUT_REF)) +\n  geom_line() +\n  geom_point(aes(color = frac_spurious), size = 0.9) +\n  labs(x = \"Time\", \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nIdentifying Outliers\nA key part of data cleaning is to detect and understand outliers. Visualisations can help. Your task here is to find outliers in GPP_NT_VUT_REF.\nFirst, using the half-hourly fluxes data, determine “outliers” as those values of GPP_NT_VUT_REF that fall outside \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red.\n\nHint: Use boxplot.stats() to return a list containing a vector of the data points which lie beyond the extremes of the whiskers of the boxplot.\n\n\nHint: Use scale_color_manual() to mannually define the color scale.\n\n\nvec_outliers &lt;- boxplot.stats(half_hourly_fluxes$GPP_NT_VUT_REF)$out\n\nplot_data &lt;- half_hourly_fluxes |&gt; \n  mutate(outlier = GPP_NT_VUT_REF %in% vec_outliers)\n\nplot_data |&gt; \n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = outlier)) + \n  geom_point() + \n  scale_color_manual(\"Outlier?\",                    # Set title of legend\n                     values = c(\"black\", \"red\"),    # Highlight in red\n                     labels = c(\"No\", \"Yes\")        # Add labels to the legend\n                     ) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNow, we want to “control” for the influence of shortwave radiation on GPP and define outliers with respect to the distribution of residuals of the linear regression between the two variables. Relax the definition of what is considered an outlier by setting adjusting their definition to falling outside \\(( Q_1 - 5 (Q_3 - Q_1)\\) to \\(Q_3 + 5 (Q_3 - Q_1)\\). Again, plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red.\n\nHint: Fit the linear regression model as lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes) and obtain the residuals from the object returned by the lm() function (see ‘Value’ in its help page).\n\n\nHint: The output of boxplot.stats(x) is a list, containing an element out. out is a named vector of the oulier values with names referring to the row numbers of x. Use as.integer(names(boxplot.stats(x)$out)) to get row numbers.\n\n\nresiduals &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes)$residuals\n\n# # unclear why this doesn't work:\n# vec_outliers &lt;- boxplot.stats(residuals, coef = 5)$out\n# plot_data &lt;- half_hourly_fluxes |&gt; \n#   mutate(outlier = GPP_NT_VUT_REF %in% vec_outliers)\n\n# this works:\nrowindex_outliers &lt;- as.integer(names(boxplot.stats(residuals, coef = 5)$out))\nplot_data &lt;- half_hourly_fluxes |&gt; \n  mutate(rowindex = dplyr::row_number()) |&gt;\n  mutate(outlier = rowindex %in% rowindex_outliers)\n\nplot_data |&gt; \n  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = outlier)) + \n  geom_point() + \n  scale_color_manual(\"Outlier?\",                    # Set title of legend\n                     values = c(\"black\", \"red\"),    # Highlight in red\n                     labels = c(\"No\", \"Yes\")        # Add labels to the legend\n                     ) +\n  labs(x = expression(paste(\"Shortwave radiation (W m\"^-2, \")\")), \n       y = expression(paste(\"GPP (\", mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1, \")\"))) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWhat do we see in this plot? We see that the red points, the outliers, fall outside the main point cloud of green points. The distribution of these outliers seems without systematic pattern or deviation. Nonetheless, it is good practice to go a step further and look at these data points in detail to find out whether they should be removed or not in your analysis. In later Chapters you will learn more on what disproportionate role outliers can play and how they may affect your statistical model and analysis.\n\n\nVisualising diurnal and seasonal cycles of GPP\nAs explored in the previous Chapter’s exercises, GPP varies over diurnal and seasonal cycles. Create a publication-ready figure that visualises the mean diurnal cycle of GPP for each day-of-year (mean across multiple years). Make sure that the figure is properly labelled, and legible for readers with a color vision deficiency.\n\nHint: To get the diurnal and seasonal cycles, summarise the half-hourly data by the hour of the day and the day of the year simultaneously using multiple grouping variables within group_by() and calculate mean values for GPP for each group.\n\n\nHint: Chose an appropriate visualisation that maps the hour-of-day to the x-axis and the day-of-year to the y-axis.\n\n\n# Aggregate to hours-in-day for each day-in-year\nfluxes_per_hod_doy &lt;- half_hourly_fluxes |&gt;          # df from previous exercise\n  dplyr::mutate(\n    hour_day = lubridate::hour(TIMESTAMP_START),     # hour of the day\n    day_year = lubridate::yday(TIMESTAMP_START)) |&gt;  # day of the year\n  dplyr::group_by(hour_day, day_year) |&gt;             # multiple grouping\n  dplyr::summarise(gpp = mean(GPP_NT_VUT_REF))\n\n# Publication-ready raster plot\nfluxes_per_hod_doy |&gt; \n  \n  # Specify aesthetics\n  ggplot(aes(x = hour_day, \n             y = day_year, \n             fill = gpp)) +  # fill color of the raster\n  \n  geom_raster() +\n  \n  # Use a color scale that works also for color-blind people\n  scale_fill_viridis_c(option = \"magma\") +\n\n  # adjust the aspect ratio of the plotting region\n  coord_fixed(ratio = 0.18) +\n  \n  # labels of each mapping axis, \\n is a line break\n  labs(title = \"Gross primary production\",\n       subtitle = \"Diurnal and seasonal cycle\",\n       x = \"Hour of day\", \n       y = \"Day of year\", \n       fill = expression(paste(mu,\"mol CO\"[2], \" m\"^-2, \"s\"^-1))) + \n                                                               \n  \n  # avoid having a small padding from the lowest values to the end of axes\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n\n\n\n\n\n\n\n\nBeautiful, isn’t it? We nicely see that on practically all days of the year we have the diurnal cycle of GPP which follows the sun’s cycle. And throughout a year, we see a rapid increase in GPP in the spring when all trees put out their leaves at once. After a highly productive summer, temperatures drop, sensescence kicks in and GPP gradually drops into its winter low.\n\n\nTrend in carbon dioxide concentrations\nThis exercise explores the longest available atmospheric CO\\(_2\\) record, obtained at the Mauna Loa observatory in Hawaii. Atmospheric CO\\(_2\\) in the northern hemisphere is characterised by seasonal swings, caused by the seasonal course of CO\\(_2\\) uptake and release by the terrestrial biosphere. We’ve explored the seasonality of the CO\\(_2\\) uptake measured at one site (in Switzerland) extensively in this an previous chapters. Your task here is to calculate and visualise the long-term trend of CO\\(_2\\). Follow these steps:\n\nDownload and read the monthly mean CO2\\(_2\\) data as a CSV file from here and read it into R.\n\n\n# download the file\ndownload.file(\n  \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\",\n  \"data/co2_mm_mlo.csv\"\n  )\n\n# read in the data\nml_co2 &lt;- readr::read_csv(\n  \"data/co2_mm_mlo.csv\",\n  skip = 40\n  )\n\n# wrangle the data\n# interpret missing values\n# rename to avoid white space in column name\nml_co2 &lt;- ml_co2 |&gt;\n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \n                              ~dplyr::na_if(., -9.99))) |&gt; \n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \n                              ~dplyr::na_if(., -0.99))) |&gt; \n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \n                              ~dplyr::na_if(., -1))) |&gt; \n  dplyr::rename(year_dec = `decimal date`)\n\n\nMake a simple graph for visualizing the monthly CO\\(_2\\) time series.\n\n\nml_co2 |&gt; \n  ggplot2::ggplot() + \n  ggplot2::geom_line(aes(year_dec, average))\n\n\n\n\n\n\n\n\n\nWrite a function that computes a 12-month running mean of the CO\\(_2\\) time series. The running mean for month \\(m\\) should consider values of \\(m-5\\) to \\(m+6\\). Define arguments for the function that let the user specify the width of the running mean “box” (i.e., setting the \\(5\\) and \\(6\\) to any other integer of choice)\n\n\n# Write the running mean function\n# variables are defined as:\n# Input vector (vec)\n# Number of elements to the left (left)\n# Number of elements to the right (right)\nrunning_mean &lt;- function(\n    vec,\n    left,\n    right\n) {\n  \n  # Create an empty vector of the same length as the input data\n  vec_out &lt;- rep(NA, length(vec))\n  \n  # Loop over each position in the vector\n  for (idx in (left+1):(length(vec)-right)){\n    \n    # Define start and end of the box to average over\n    startbox &lt;- idx - left\n    endbox   &lt;- idx + right\n    \n    vec_out[idx] &lt;- mean(vec[startbox:endbox], na.rm = TRUE)\n  }\n  \n  return(vec_out)\n}\n\nml_co2 &lt;- ml_co2 |&gt; \n  mutate(\n    average_12m = running_mean(average, left = 5, right = 6)\n    )\n\n\nMake a publication-ready figure that shows the monthly and the 12-month running mean time series of the CO\\(_2\\) record.\n\n\nHint: To automatically render the time axis with ggplot, you can create a time object by combining the year and month columns: lubridate::ymd(paste(as.character(year), \"-\", as.character(month), \"-15\"))\n\n\n# create a date object for nice plotting\nplot_data &lt;- ml_co2 |&gt; \n  mutate(\n    date = lubridate::ymd(\n      paste(as.character(year),\n            \"-\",\n            as.character(month), \"-15\")  # centering monthly mean on the 15th of each month\n      )\n    )\n\nplot_data |&gt;\n  ggplot() +\n  \n  # monthly means\n  geom_line(\n    aes(\n      date,\n      average,\n      color = \"Monthly mean\"\n      )\n  ) +\n  \n  # running mean\n  geom_line(\n    aes(\n      date,\n      average_12m,\n      color = \"12-month running mean\"\n      )\n  ) +\n  \n  # Style the plot\n  theme_classic() +\n  theme(\n    legend.position = c(0.25, 0.75) # Move legend into the plot\n    ) + \n  scale_color_manual(\n    \"\", # Omit legend title\n    values = c(\"tomato\", \"black\"),\n    labels = c(\"12-month running mean\", \"Monthly mean\")\n  ) +\n  labs(\n    title = expression(\n      paste(\"Atmospheric CO\"[2],\n            \" concentrations on Manua Lao, Hawaii\")\n    ),\n    y     = expression(paste(\"CO\"[2], \" (ppm)\")),\n    x     = \"Year\"\n  )\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#data-variety",
    "href": "appendix.html#data-variety",
    "title": "Appendix",
    "section": "Data Variety",
    "text": "Data Variety\n\nFiles and file formats\n\nReading and writing human readable files\nWhile not leaving your R session, download and open the files at the following locations:\n\nThe below code shows how to read in the different demo data sets (CSV files). You will note that they all need separate settings, and that a given file extension isn’t necessarily a reflection of the content the file. Inspection of your read in data is therefore key.\n\n\n# read in the first demo\ndemo_01 &lt;- read.table(\n  \"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_1.csv\",\n  sep = \",\",\n  header = TRUE\n  )\n\n# read in second demo\ndemo_02 &lt;- read.table(\n  \"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_2.csv\",\n  sep = \" \",\n  header = TRUE\n  )\n\ndemo_03 &lt;- read.table(\n  \"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_3.csv\",\n  sep = \";\",\n  comment.char = \"|\",\n  header = TRUE,\n  )\n\nAll the demo data sets are equal, except for their formatting. We can test if the content is identical by using the identical() function in R.\n\n# compare 1 with 2\nidentical(demo_01, demo_02)\n\n[1] TRUE\n\n# compare 2 with 3\nidentical(demo_02, demo_03)\n\n[1] TRUE\n\n# Given transitive properties, demo_01 is identical to demo_03\n\nOnce loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory.\n\nYou can combine the three datasets using the {dplyr} bind_rows() function.\n\n\n# combining all demo datasets\ndemo_all &lt;- dplyr::bind_rows(demo_01, demo_02, demo_03)\n\n# writing the data to a temporary CSV file\nwrite.table(\n  demo_all, \n  file = file.path(tempdir(), \"tmp_csv_file.csv\"),\n  col.names = TRUE,\n  row.names = FALSE,\n  sep = \",\"\n)\n\n# or...\nwrite.csv(\n  demo_all,\n  file.path(tempdir(), \"tmp_csv_file.csv\"),\n  row.names = FALSE\n)\n\n# read in the previous CSV file\ndemo_all_new &lt;-read.table(\n  file.path(tempdir(), \"tmp_csv_file.csv\"),\n  header = TRUE,\n  sep = \",\"\n)\n\n# writing the data to a JSON file\njsonlite::write_json(demo_all_new, path = \"./my_json_file.json\")\n\n\n\nReading and writing binary files\nDownload and open the following file:\nhttps://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc\n\nWhat file format are we dealing with?\n\nIt is a NetCDF file (ending .nc, see Table in Chapter 6  Data variety)\n\nWhat library would you use to read this kind of data?\n\nDifferent libraries are available, including {terra}, {raster} (the predecessor of {terra}), and {ncdf4}, see Table in 6  Data variety.\n\nWhat does this file contain?\n\nIn R:\n\n# read unknown netcdf file using the {terra} library\nlibrary(terra)\nunknown_netcdf &lt;- terra::rast(\n  \"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.nc\"\n  )\n\n# print the meta-data by calling the variable\nunknown_netcdf\n\n# visually plot the data\nterra::plot(unknown_netcdf)\n\nFrom your terminal (when the file is located in the current working directory:\nncdump -h demo_data.nc\nWhen printing the object in R, we get: varname     : t2m (2 metre temperature).\n\nWrite this file to disk in a different geospatial format you desire (use the R documentation of the library used to read the file and the chapter information).\n\n\n# write the data as a geotiff (other options are possible as well in writeRaster)\nterra::writeRaster(\n  unknown_netcdf,\n  filename = \"./test.tif\",\n  overwrite = TRUE\n  )\n\n\nDownload and open the following file: https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.tif. Does this data seem familiar, and how can you tell? What are your conclusions?\n\n\n# read unknown tif file using the {terra} library\nlibrary(terra)\nunknown_tif &lt;- terra::rast(\n  \"https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/demo_data.tif\"\n  )\n\n# print the meta-data by calling the variable\nunknown_tif\n\n# visually plot the data\nterra::plot(unknown_tif)\n\n# Are they exactly the same\nterra::plot(unknown_tif - unknown_netcdf)\n\n# or...\nidentical(unknown_netcdf, unknown_tif)\n\nLooks similar to the NetCDF data, but temperature appears to be given in Celsius in the GeoTIFF file and in Kelvin in the NetCDF file.\n\n\n\nAPI Use\n\nGet\n\nWe can get the total sand content using the tutorial using new coodinates outlining Switzerland.\n\n\n# set API URL endpoint\n# for the total sand content\nurl &lt;- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4\"\n\n# formulate query to pass to httr\nquery &lt;- list(\n  \"var\" = \"T_SAND\",\n  \"south\" = 45.5,\n  \"west\" =  5.9,\n  \"east\" =  10.7,\n  \"north\" = 48,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SAND.nc\"),\n    overwrite = TRUE\n  )\n)\n\n# to visualize the data\n# we need to load the {terra}\n# library\nsand &lt;- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nterra::plot(sand)\n\n\n\n\n\n\n\n\n\nConsulting the original data pages or the package help files one can determine that the parameter “T_SAND” needs to be replaced by “T_SILT” in both the URL and the query.\n\n\n# set API URL endpoint\n# for the total sand content\nurl &lt;- \"https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SILT.nc4\"\n\n# formulate query to pass to httr\nquery &lt;- list(\n  \"var\" = \"T_SILT\",\n  \"south\" = 45.5,\n  \"west\" =  5.9,\n  \"east\" =  10.7,\n  \"north\" = 48,\n  \"disableProjSubset\" = \"on\",\n  \"horizStride\" = 1,\n  \"accept\" = \"netcdf4\"\n)\n\n# download data using the\n# API endpoint and query data\nstatus &lt;- httr::GET(\n  url = url,\n  query = query,\n  httr::write_disk(\n    path = file.path(tempdir(), \"T_SILT.nc\"),\n    overwrite = TRUE\n  )\n)\n\n# to visualize the data\n# we need to load the {terra}\n# library\nsilt &lt;- terra::rast(file.path(tempdir(), \"T_SILT.nc\"))\nterra::plot(silt)\n\n\n\n\n\n\n\n\n\n\nDedicated libraries\n\nUsing the {hwsdr} package this simplifies to:\n\n\n# Download a soil fraction map\n# of sand for a given bounding box\nhwsdr::ws_subset(\n  location = c(45.5, 5.9, 48, 10.7),\n  param = \"T_SAND\",\n  path = tempdir()\n)\n\nclass       : SpatRaster \ndimensions  : 97, 51, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : 45.5, 48.05, 5.9, 10.75  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : T_SAND.nc \nname        : T_SAND \nunit        :      % \n\n# to visualize the data\n# we need to load the {terra}\n# library\nsand &lt;- terra::rast(file.path(tempdir(), \"T_SAND.nc\"))\nterra::plot(sand)\n\n\n\n\n\n\n\n\n\nYou can easily list all {MODISTools} products using:\n\n\n# list all products\nproducts &lt;- MODISTools::mt_products()\n\n# We count\nnrow(products)\n\n[1] 46\n\n\n\nYou can use the {MODISTools} package to easily query the land cover data. Use the MODISTools::mt_products() and MODISTools::mt_bands() functions to determine products to use (i.e. MCD12Q1). Note that MODISTools does not use a bounding box but km left/right and top/bottom - an approximation is therefore good enough.\n\n\n# download land cover data (for a single year to speed things up)\nland_cover &lt;- MODISTools::mt_subset(\n  product = \"MCD12Q1\",\n  site_name = \"Swiss\",\n  lat = 46.6756,\n  lon = 7.85480,\n  band = \"LC_Type1\",\n  start = \"2012-01-01\",\n  end = \"2012-12-31\",\n  km_lr = 50,\n  km_ab = 50,\n  internal = TRUE,\n  progress = TRUE\n)\n\nDownloading chunks:\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# convert to a raster map for plotting\nland_cover &lt;- MODISTools::mt_to_terra(land_cover)\nterra::plot(land_cover)",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#open-science",
    "href": "appendix.html#open-science",
    "title": "Appendix",
    "section": "Open Science",
    "text": "Open Science\n\nExternal data\nThe project data is stored in one folder without folders to sort data from code to give it structure.\nThe project can be re-organized using a simple project structure as such:\n~/project/\n├─ data/\n     ├─ 00_convert_data.R\n     ├─ survey.xlsx # the original\n     ├─ survey.csv # from (xls conversion (copy 1).csv)\n├─ R/\n     ├─ my_functions.R\n├─ analysis/\n     ├─ 00_model_fits.R # from Model-test-final.R\n     ├─ 01_model_plots.R # from Plots.R\n├─ vignettes/\n     ├─ Report.Rmd\n├─ manuscript/\n     ├─ Report.html\n     ├─ Figure 1.png\nNote that duplicate files are removed, code to cleanup data is numbered and stored with the data, functions which are accessible for analysis are stored in the ./R/ folder, Rmarkdown files are stored in the vignettes folder and the results of the full analysis is stored in a manuscript folder. Some variations on naming is possible.\n\n\nA new project\nThis exercise trains your ability to access and wrangle data yourself in a reproducible way. The best solution to test whether you successfully did so is by letting a friend run all of your code on their machine. Resolving the errors you may encounter helps you to improve your workflow and ensures a streamlined submission of your final report.\n\n\nTracking the state of your project\nFor your new project created above run {renv} by following the tutorial outline.\nIn short, in the main project run all project code (or load all require libraries) and execute:\n\n# Initiate a local index of used libraries\nrenv::init()\n\n# Take a snapshot of all used libraries\nrenv::snapshot()\n\nYou should now find an renv folder in your project as well as an renv.lock file.",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#code-management",
    "href": "appendix.html#code-management",
    "title": "Appendix",
    "section": "Code Management",
    "text": "Code Management\n\nLocation based code management\nNo solutions provided.",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#regression-and-classification",
    "href": "appendix.html#regression-and-classification",
    "title": "Appendix",
    "section": "Regression and Classification",
    "text": "Regression and Classification",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#supervised-ml-i",
    "href": "appendix.html#supervised-ml-i",
    "title": "Appendix",
    "section": "Supervised ML I",
    "text": "Supervised ML I\nNo solutions provided",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#supervised-ml-ii",
    "href": "appendix.html#supervised-ml-ii",
    "title": "Appendix",
    "section": "Supervised ML II",
    "text": "Supervised ML II\n\nCross-validation by hand\nIn the tutorial we “built on shoulder of giants” - people that went through the struggle of writing a robust package to implement a cross validation. Although we can and should use such packages, we still have to understand how a cross validation works in detail.\nWrite a function that implements n-fold cross-validation for KNN with \\(k=30\\). (We write ‘n-fold CV’ here to avoid confusion with the k in KNN, but mean the same as described in 11.3.4 Resampling.) The function should take as arguments the training data object, n for specifying the number of folds (use \\(n=60\\)), the name of the target variable, the names of the predictor variables as a character vector, and the \\(k\\) for KNN. The function should return a vector of length n, containing the MAE evaluated on the n folds. To randomize the split of data points into folds, first “re-shuffle” the rows in the training data as part of the function. Centering and scaling of the training and validation data should be applied manually before getting the KNN model object within each fold.\nAs data, use daily ecosystem flux data from here. As target variable, use \"GPP_NT_VUT_REF\". As predictor variables, use c(\"TA_F\", \"SW_IN_F\", \"VPD_F\").\nVisualise the distribution of the validation errors (MAE) across folds.\n\n# read data provided for this exercise\ndaily_fluxes &lt;- read.csv(\"data/df_daily_exercise_supervisedmlii.csv\")\n\nnam_target &lt;- \"GPP_NT_VUT_REF\"\nnams_predictors &lt;- c(\"TA_F\", \"SW_IN_F\", \"VPD_F\")\n\n# function returning the MAE for each fold, given the indices of the \n# rows to be used as the validation set.\nget_mae_byfold &lt;- function(df,              \n                           idx_fold,        # row indices for validation set\n                           nam_target,      # character string for target variables\n                           nams_predictors, # vector of character strings for predictors\n                           use_k            # k for KNN\n                           ){\n  \n  # validation set\n  df_valid &lt;- df[idx_fold, c(nam_target, nams_predictors)]\n  \n  # remaining training set\n  df_train &lt;- df[-idx_fold, c(nam_target, nams_predictors)]\n  \n  # center and scale based on training data parameters\n  mean_byvar &lt;- c()\n  sd_byvar &lt;- c()\n  df_train_cs &lt;- df_train * NA\n  df_valid_cs &lt;- df_valid * NA\n  \n  # center and scale each predictor\n  for (ivar in nams_predictors){\n    \n    # determine mean and sd for centering and scaling\n    mean_byvar[ivar]   &lt;- mean(df_train[,ivar], na.rm = TRUE)\n    sd_byvar[ivar]     &lt;- sd(df_train[,ivar], na.rm = TRUE)\n    \n    # center and scale training data\n    df_train_cs[,ivar] &lt;- df_train[,ivar] - mean_byvar[ivar]\n    df_train_cs[,ivar] &lt;- df_train_cs[,ivar] / sd_byvar[ivar]\n    \n    # center and scale validation data \n    # important: use parameters (mean and sd) determined on training data\n    df_valid_cs[,ivar] &lt;- df_valid[,ivar] - mean_byvar[ivar]\n    df_valid_cs[,ivar] &lt;- df_valid_cs[,ivar] / sd_byvar[ivar]\n  }\n  \n  # add unmodified target variable\n  df_valid_cs[,nam_target] &lt;- df_valid[,nam_target] \n  df_train_cs[,nam_target] &lt;- df_train[,nam_target] \n  \n  # train using the scaled training data\n  mod &lt;- caret::knnreg(df_train_cs[,nams_predictors], \n                       df_train_cs[,nam_target], \n                       k = use_k\n                       )\n  \n  # predict using the scaled validation data\n  df_valid_cs$pred &lt;- predict(mod, newdata = df_valid_cs[,nams_predictors])\n  \n  # calculate MAE on validation data\n  out &lt;- mean(abs(df_valid_cs$pred - df_valid_cs[,nam_target]))\n  \n  return(out)\n  \n}\n\n# function reshuffling data, creating folds (list of row indices), and \n# calling function to calculate MAE on each fold. Returns vector of MAE for each fold.\nget_mae_cv &lt;- function(df, \n                       nam_target,       # character string for target variables\n                       nams_predictors,  # vector of character strings for predictors\n                       n_folds,          # number of folds for cross-validation\n                       use_k             # k for KNN\n                       ){\n\n  # re-shuffle rows in data frame\n  df &lt;- df[sample(nrow(df)),]\n\n  # determine row indices to be allocated to each fold\n  # each fold takes in 1/n_folds of the total number of rows\n  nrows_per_fold &lt;- ceiling(nrow(df) / n_folds)\n  idx &lt;- rep(seq(1:n_folds), each = nrows_per_fold)\n  folds_1 &lt;- split(1:nrow(df), idx[1:nrow(df)])\n  \n  # alternative option\n  n_folds &lt;- 5\n  idx &lt;-  (1:nrow(df) - 1) %/% (nrow(df) / n_folds)\n  folds_2 &lt;- split(1:nrow(df), idx)\n  \n  # using caret built-in function\n  folds_3 &lt;- caret::createFolds(1:nrow(df), k = n_folds)\n  \n  # loop over folds and get MAE determined on each validation set\n  mae_list &lt;- purrr::map(\n    folds_1,\n    ~get_mae_byfold(df, ., nam_target, nams_predictors, use_k)\n  )\n  \n  # return a vector of MAE in each fold\n  return(unlist(mae_list))\n}\n\n# get MAE for each fold of the cross-validation\nvec_mae_byhand &lt;- get_mae_cv(daily_fluxes, \n                             nam_target, \n                             nams_predictors, \n                             n_folds = 60, \n                             use_k = 30\n                             )\n\n\ntibble(mae = vec_mae_byhand) |&gt; \n  ggplot(aes(mae, ..count..)) +\n  geom_histogram(fill = \"grey70\", color = \"black\", bins = 10) +\n  theme_classic()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\n\nCross-validation vs. test error\nNow, you can use the user-friendly caret::train() for KNN with 60-fold cross-validation and tuning the hyperparameter k. Use the MAE as the loss metric. Use the same data as in the Exercise above and withhold 20% of the data for the test set. Visually compare the reported mean MAE from the cross-validation folds with the MAE determined on a test set.\nIn your visual comparison, add a plot layer showing the distribution of validation errors from you manual implementation of cross-validation (Exercise above).\n\ndaily_fluxes &lt;- read.csv(\"data/df_daily_exercise_supervisedmlii.csv\")\n\nset.seed(1982)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.8)\ndaily_fluxes_train &lt;- rsample::training(split)\ndaily_fluxes_test &lt;- rsample::testing(split)\n\n# The same model formulation is in the previous chapter\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, \n                      data = daily_fluxes_train) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\nmod &lt;- caret::train(pp, \n                    data = daily_fluxes_train |&gt; drop_na(), \n                    method = \"knn\",\n                    trControl = caret::trainControl(method = \"cv\", \n                                                    number = 60\n                                                    ),\n                    tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),\n                    metric = \"MAE\"\n                    )\n\n# data frame containing metrics on validation set by fold\nmetrics_byfold &lt;- mod$resample\n\n# MAE on test set\ndaily_fluxes_test &lt;- daily_fluxes_test |&gt; \n  drop_na() %&gt;%  # use magrittr-pipe here for the dot evaluation\n  mutate(fitted =  predict(mod, newdata = .))  \n\nmae_test &lt;- mean(abs(daily_fluxes_test$fitted - daily_fluxes_test$GPP_NT_VUT_REF))\n\n\nplot_data &lt;- metrics_byfold |&gt; \n  select(caret = MAE) |&gt; \n  bind_cols(\n    tibble(byhand = vec_mae_byhand)\n  ) |&gt; \n  pivot_longer(cols = c(caret, byhand), \n               names_to = \"implementation\", \n               values_to = \"MAE\")\n\nplot_data |&gt; \n  ggplot(aes(x = MAE, y = ..count.., fill = implementation)) +\n  geom_density(alpha = 0.5) +\n  \n  # test error\n  geom_vline(xintercept = mae_test, color = \"red\", size = 2) +\n  \n  scale_fill_manual(values = c(\"darkgoldenrod\", \"royalblue\")) +\n  theme_classic()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#random-forest",
    "href": "appendix.html#random-forest",
    "title": "Appendix",
    "section": "Random Forest",
    "text": "Random Forest\n\nFitting a Random Forest\nFit a Random Forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (num.trees), each with a minimum number of observations per leaf of 5 (min.node.size). You can consult the respective arguments for the \"ranger\" method typing ?ranger.\n\n# Data loading and cleaning\ndaily_fluxes &lt;- read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") |&gt;\n    \n  # select only the variables we are interested in\n  dplyr::select(TIMESTAMP,\n                GPP_NT_VUT_REF,    # the target\n                ends_with(\"_QC\"),  # quality control info\n                ends_with(\"_F\"),   # includes all all meteorological covariates\n                -contains(\"JSB\")   # weird useless variable\n  ) |&gt;\n  \n  # convert to a nice date object\n  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt;\n  \n  # set all -9999 to NA\n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -9999))) |&gt; \n  # dplyr::na_if(-9999) |&gt; xxxxx\n  \n  # retain only data based on &gt;=80% good-quality measurements\n  # overwrite bad data with NA (not dropping rows)\n  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF),\n                TA_F           = ifelse(TA_F_QC        &lt; 0.8, NA, TA_F),\n                SW_IN_F        = ifelse(SW_IN_F_QC     &lt; 0.8, NA, SW_IN_F),\n                LW_IN_F        = ifelse(LW_IN_F_QC     &lt; 0.8, NA, LW_IN_F),\n                VPD_F          = ifelse(VPD_F_QC       &lt; 0.8, NA, VPD_F),\n                PA_F           = ifelse(PA_F_QC        &lt; 0.8, NA, PA_F),\n                P_F            = ifelse(P_F_QC         &lt; 0.8, NA, P_F),\n                WS_F           = ifelse(WS_F_QC        &lt; 0.8, NA, WS_F)) |&gt; \n  \n  # drop QC variables (no longer needed)\n  dplyr::select(-ends_with(\"_QC\"))\n\n# Data splitting\nset.seed(123)  # for reproducibility\nsplit &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = \"VPD_F\")\ndaily_fluxes_train &lt;- rsample::training(split)\ndaily_fluxes_test &lt;- rsample::testing(split)\n\n# The same model formulation is in the previous chapter\npp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, \n                      data = daily_fluxes_train) |&gt; \n  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt;\n  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())\n\n\n# Fit the first model of 12 trees and maximum depth 4\nrf_12_5 &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    savePredictions = \"final\"\n    ),\n  tuneGrid = expand.grid(\n    .mtry = floor(6 / 3),        # default p/3\n    .min.node.size = 5,          # set to 5\n    .splitrule = \"variance\"      # default \"variance\"\n  ),\n  # arguments specific to \"ranger\" method\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 12,       \n  seed = 1982                                          # for reproducibility\n)\n\n# generic print\nprint(rf_12_5)\n\nRandom Forest \n\n1910 samples\n   8 predictor\n\nRecipe steps: center, scale \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1528, 1528, 1529, 1527, 1528 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.468118  0.6796483  1.116388\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of variance\n\nTuning parameter 'min.node.size' was held constant at a value of 5\n\n\nRepeat the fitting with 1000 decision trees and minimum node size of 5, then with 12 decision trees and a minimum node size of 1. Then, discuss the role that the number of decision trees and the minimum number of leaf observations of a tree play in the bias-variance trade-off and in the computation time.\n\n# Directly fit the model again, with same data and model formulation\n# Now train with 1000 trees and maximum depth 4\nrf_1000_5 &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    savePredictions = \"final\"\n    ),\n  tuneGrid = expand.grid(\n    .mtry = floor(6 / 3),       # default p/3\n    .min.node.size = 5,         # set to 5\n    .splitrule = \"variance\"     # default variance\n    ),\n  # arguments specific to \"ranger\" method\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 1000,       \n  seed = 1982\n)\n\n# generic print\nprint(rf_1000_5)\n\nRandom Forest \n\n1910 samples\n   8 predictor\n\nRecipe steps: center, scale \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1527, 1528, 1529, 1529, 1527 \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  1.415667  0.701975  1.078454\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of variance\n\nTuning parameter 'min.node.size' was held constant at a value of 5\n\n\n\n# Repeat model fit with 12 trees and maximum depth 6\nrf_12_1 &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5, \n    savePredictions = \"final\"\n    ),\n  tuneGrid = expand.grid(\n    .mtry = floor(6 / 3),        # default p/3\n    .min.node.size = 1,          # set to 1\n    .splitrule = \"variance\"      # default \"variance\"\n  ),\n  # arguments specific to \"ranger\" method\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 12,       \n  seed = 1982\n)\n\n# generic print\nprint(rf_12_1)\n\nRandom Forest \n\n1910 samples\n   8 predictor\n\nRecipe steps: center, scale \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1527, 1528, 1529, 1528, 1528 \nResampling results:\n\n  RMSE      Rsquared  MAE    \n  1.470344  0.678954  1.11868\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of variance\n\nTuning parameter 'min.node.size' was held constant at a value of 1\n\n\nInterpretation of results\n\n# check results\nrbind(\n  rf_12_5$results |&gt; mutate(Setup = \"12 Trees with at least 5 obs per leaf\") |&gt; relocate(Setup),\n  rf_1000_5$results |&gt; mutate(Setup = \"1000 Trees with at least 5 obs per leaf\") |&gt; relocate(Setup),\n  rf_12_1$results |&gt; mutate(Setup = \"12 Trees with at least 1 obs per leaf\") |&gt; relocate(Setup)\n)[,- c(2:4)] |&gt;\n  knitr::kable(caption = \"Comparison of cross-validated metrics across Random Forest setups.\")\n\n\nComparison of cross-validated metrics across Random Forest setups.\n\n\n\n\n\n\n\n\n\n\n\nSetup\nRMSE\nRsquared\nMAE\nRMSESD\nRsquaredSD\nMAESD\n\n\n\n\n12 Trees with at least 5 obs per leaf\n1.468118\n0.6796483\n1.116388\n0.0631198\n0.0243411\n0.0436952\n\n\n1000 Trees with at least 5 obs per leaf\n1.415667\n0.7019750\n1.078454\n0.0661862\n0.0228750\n0.0308386\n\n\n12 Trees with at least 1 obs per leaf\n1.470344\n0.6789540\n1.118680\n0.0766487\n0.0244763\n0.0588226\n\n\n\n\n\nIncreasing the number of decision trees leads to a decrease in error (e.g. RMSE) and in error variance in the random forest results, making the model more accurate, robust and generalisable. This can be seen by the smaller values of practically all the metrics above. In the bias-variance trade-off, higher num.trees shifts the balance towards both lower bias and lower variance.\nDecreasing the number of observations required to go to each split allows for a better fit of the data, but makes the model less generalisable. Actually, it would lead to a perfect fit of the training data. Since the error metrics are calcualted using cross-validation, the lack of generalisability is captured together with the model fit. This is seen by an increase in both the error metrics and their estimated standard deviation. Hence, using low values for min.node.size may lead to overfitting.\n\n\nHyperparameter tuning\nIn a previous tutorial, you learned how to tune the hyperparameter \\(k\\) in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a random forest. The {caret} package allows to vary three hyperparameters:\n\nmtry: The number of variables to consider to make decisions at each node, often taken as \\(p/3\\) for regression, where \\(p\\) is the number of predictors.\nmin.node.size: The number of data points at the “bottom” of each decision tree, i.e. the leaves.\nsplitrule: The function applied to data in each branch of a tree, used for determining the goodness of a decision.\n\nAnswer the following questions, giving a reason for your responses:\n\nCheck the help for the ranger() function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search.\n\n\nmtry_values &lt;- c(2, 4, 6)\nmin.node.size_values &lt;- c(2, 5, 10, 20)\nsplitrule_values &lt;- c(\"variance\", \"extratrees\", \"maxstat\")\n\n\nIn the previous exercise, you have seen how the minimum node size regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when min.node.size = 1 and when min.node.size = n (n being the number of observations)? Note that it’s not necessary to provide the max.depth argument to train() because min.node.size is already limiting the size of the trees in the Random Forests.\n\nSolution: The minimum node size is inversely related to the tree depth. The more nodes are required to be at the leaves, the shorter the tree will be, because not so many splits can be done. If we allowed each leave to correspond to a single observation, we would have a very large tree, such that each branch corresponds to one observation. If we force each leave to have at least \\(n\\) observations, then the tree will not “grow”, that is, it will never even split.\n\nGreedy hyperparameter tuning: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search. Implement the optimization routine yourself, using loops. &gt; Tip: Keep the number of trees low, otherwise it takes too long to fit each Random Forest model.\n\n\n# Use data and model formulation created before\n\nresults &lt;- c()   # initialise results\nset.seed(1997)   # for reproducibility\n\n# Train models in a loop, save metrics\nfor (mtry_value in mtry_values){\n  mod &lt;- caret::train(\n    pp, \n    data = daily_fluxes_train %&gt;% \n      drop_na(), \n    method = \"ranger\",\n    metric = \"RMSE\",\n    trControl = trainControl(\n      method = \"cv\",\n      number = 5,\n      savePredictions = \"final\"\n      ),\n    tuneGrid = expand.grid(\n      .mtry = mtry_value,           # modify mtry\n      .min.node.size = 5,           # default 5\n      .splitrule = \"variance\"       # default \"variance\"\n      ),\n    # arguments specific to \"ranger\" method\n    replace = FALSE,\n    sample.fraction = 0.5,\n    num.trees = 100,\n    seed = 1982\n  )\n  \n  results &lt;- rbind(results, mod$results)\n}\n\nresults\n\n  mtry min.node.size splitrule     RMSE  Rsquared      MAE     RMSESD\n1    2             5  variance 1.408401 0.7048393 1.079003 0.06273482\n2    4             5  variance 1.415472 0.7036540 1.078367 0.02791004\n3    6             5  variance 1.433248 0.6946220 1.094301 0.11539229\n  RsquaredSD      MAESD\n1 0.01596849 0.04919243\n2 0.01083131 0.03353947\n3 0.03200823 0.07272992\n\n\nBased on the first round of hyperparameter tuning, we should take mtry = 2 because it leads to the smallest RMSE and MAE, and the highest Rsquared. That is, the best fit. Nevertheless, the difference between taking mtry = 2 or 4 is very small and the second actually leads to smaller variance in the metric estimates. Hence, the decision is not so clear.\n\n# Take the previous best model and tune next hyperparameter\n\nresults &lt;- c()   # initialise results\nset.seed(1997)   # for reproducibility\n\n# Train models in a loop, save metrics\nfor (min.node.size_value in min.node.size_values){\n  mod &lt;- caret::train(\n    pp, \n    data = daily_fluxes_train %&gt;% \n      drop_na(), \n    method = \"ranger\",\n    metric = \"RMSE\",\n    trControl = trainControl(\n      method = \"cv\",\n      number = 5,\n      savePredictions = \"final\"\n      ),\n    tuneGrid = expand.grid(\n      .mtry = 2,      # best mtry\n      .min.node.size = min.node.size_value,   # modify\n      .splitrule = \"variance\"       # default \"variance\"\n      ),\n    # arguments specific to \"ranger\" method\n    # keep num.trees keep small for computation\n    # for reproducibility set the seed value\n    sample.fraction = 0.5,\n    num.trees = 100,\n    seed = 1982\n  )\n  \n  results &lt;- rbind(results, mod$results)\n}\n\nresults\n\n  mtry min.node.size splitrule     RMSE  Rsquared      MAE     RMSESD\n1    2             2  variance 1.414459 0.7021521 1.080694 0.06568570\n2    2             5  variance 1.416445 0.7033030 1.078525 0.03266685\n3    2            10  variance 1.429133 0.6971880 1.090143 0.12087515\n4    2            20  variance 1.426698 0.6983137 1.085333 0.05662594\n   RsquaredSD      MAESD\n1 0.016183122 0.05218533\n2 0.005366239 0.03448577\n3 0.034472910 0.07423081\n4 0.015027167 0.03858145\n\n\nThe second hyperparameter should be either min.node.size = 2 or 5 because they lead to the best metrics overall. Again, the differences are very small and each metric would lead to a different decision. By increasing min.node.size, we get more generalisability, but if it’s too high we will lose fit quality. If you change the random seed, you’ll see that the tuning results are not robust, so whichever hyperparameter we choose won’t make a big difference in the model fit (at least within the ranges searched). Let’s take min.node.size = 2 for the next loop.\n\n# Take the previous best models and tune last hyperparameter\nresults &lt;- c()\n\n# Train models in a loop, save metrics\nfor (splitrule_value in splitrule_values){\n  mod &lt;- caret::train(\n    pp, \n    data = daily_fluxes_train %&gt;% \n      drop_na(), \n    method = \"ranger\",\n    metric = \"RMSE\",\n    trControl = trainControl(\n      method = \"cv\",\n      number = 5,\n      savePredictions = \"final\"\n      ),\n    tuneGrid = expand.grid(\n      .mtry = 2,                    # best mtry\n      .min.node.size = 2,             # best min.node.size\n      .splitrule = splitrule_value  # modify\n    ),\n    # arguments specific to \"ranger\" method\n    # keep num.trees keep small for computation\n    # for reproducibility set the seed value\n    replace = FALSE,\n    sample.fraction = 0.5,\n    num.trees = 100,\n    seed = 1982\n  )\n  \n  results &lt;- rbind(results, mod$results)\n}\n\nresults\n\n  mtry min.node.size  splitrule     RMSE  Rsquared      MAE     RMSESD\n1    2             2   variance 1.430041 0.6960092 1.089296 0.07545037\n2    2             2 extratrees 1.406141 0.7084745 1.067863 0.08168255\n3    2             2    maxstat 1.450796 0.6908389 1.109260 0.06738257\n  RsquaredSD      MAESD\n1 0.02332409 0.05434451\n2 0.01930827 0.04337092\n3 0.01935027 0.04709398\n\n\nAccording to the last round of tuning, we should use splitrule = \"extratrees\". With that, we found the best model so far.\n\nGrid hyperparameter tuning: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the expand.grid() function to create a data.frame of hyperparameter value combinations. This grid will be passed to train() via the tuneGrid argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of train() and the results of the hyperparameter search.\n\n\nset.seed(1403)    # for reproducibility\nmod &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    savePredictions = \"final\"\n  ),\n  # expand grid of tunable hyperparameters\n  tuneGrid = expand.grid(\n    .mtry = mtry_values,             \n    .min.node.size = min.node.size_values,    \n    .splitrule = splitrule_values\n    ),\n  # arguments specific to \"ranger\" method\n  # keep num.trees keep small for computation\n  # for reproducibility set the seed value\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 100,\n  seed = 1982\n)\n\n\nplot(mod, metric = \"RMSE\")\n\n\n\n\n\n\n\nplot(mod, metric = \"Rsquared\")\n\n\n\n\n\n\n\n\n\nCompare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches?\n\nThe best model according to the grid search, with lowest RMSE and highest \\(R^2\\), is the one with mtry = 6, min.node.size = 10 and splitrule = \"extratrees\" . This is not the “best model” we found with the greedy approach (with mtry = 2 and min.node.size = 2) but actually this model is overall the second best model. The metric used for tuning matters, and looking at several of them at the same can help make decisions, if different metrics agree. All these hyperparameter tuning approaches agree that the best splitrule is \"extratrees\" (and if you change the random seed, this result is consistent). The best mtry value is different for each split rule used, so having started with \"variance\" in the greedy search lead the tuning in the wrong direction, moving us towards a local optimum rather than a global optimum. This highlights that hyperparameter values interact with each other and optimizing over grids is preferred (although it takes more time).\n\n\nModel performance\nYou have trained several random forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and \\(R^2\\) on the training and the test set, does it show overfitting?\n\n# Train best model\nmod_best &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    savePredictions = \"final\"\n    ),\n  # expand grid of tunable hyperparameters\n  tuneGrid = expand.grid(\n    .mtry = 6,             \n    .min.node.size = 10,    \n    .splitrule = \"extratrees\"\n  ),  \n  # arguments specific to \"ranger\" method\n  # keep num.trees keep small for computation\n  # for reproducibility set the seed value\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 100,\n  seed = 1982\n)\n\n# Get predictions\n\n# Train worst model\nmod_worst &lt;- caret::train(\n  pp, \n  data = daily_fluxes_train %&gt;% \n    drop_na(), \n  method = \"ranger\",\n  metric = \"RMSE\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    savePredictions = \"final\"\n    ),\n  # expand grid of tunable hyperparameters\n  tuneGrid = expand.grid(\n    .mtry = 2,             \n    .min.node.size = 20,    \n    .splitrule = \"maxstat\"\n  ),  \n  # arguments specific to \"ranger\" method\n  # keep num.trees keep small for computation\n  # for reproducibility set the seed value\n  replace = FALSE,\n  sample.fraction = 0.5,\n  num.trees = 100,\n  seed = 1982\n)\n\nsource(\"R/eval_model.R\")\neval_model(\n  mod_best,\n  daily_fluxes_train,\n  daily_fluxes_test\n  )\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\neval_model(\n  mod_worst,\n  daily_fluxes_train,\n  daily_fluxes_test\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe performance on the test set for the best model is still close to the performance on the training set, so the model doesn’t seem to overfit. The same goes for the worse model, which leads to worse \\(R^2\\) and RMSE and visually the fit is slightly worse.",
    "crumbs": [
      "Home",
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "14  References",
    "section": "",
    "text": "System information and package list\nThis book was compiled with the following environment:\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       UTC\n date     2025-02-24\n pandoc   3.1.11 @ /opt/hostedtoolcache/pandoc/3.1.11/x64/ (via rmarkdown)\n quarto   1.6.42 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P cli           3.6.4   2025-02-13 [?] RSPM (R 4.4.0)\n P digest        0.6.37  2024-08-19 [?] RSPM (R 4.4.0)\n P evaluate      1.0.3   2025-01-10 [?] RSPM (R 4.4.0)\n P fastmap       1.2.0   2024-05-15 [?] RSPM (R 4.4.0)\n P htmltools     0.5.8.1 2024-04-04 [?] RSPM (R 4.4.0)\n P jsonlite      1.9.0   2025-02-19 [?] RSPM (R 4.4.0)\n P knitr         1.49    2024-11-08 [?] RSPM (R 4.4.0)\n P renv          1.1.1   2025-02-07 [?] RSPM\n P rlang         1.1.5   2025-01-17 [?] RSPM (R 4.4.0)\n P rmarkdown     2.29    2024-11-04 [?] RSPM (R 4.4.0)\n P sessioninfo   1.2.3   2025-02-05 [?] RSPM (R 4.4.0)\n P xfun          0.51    2025-02-19 [?] RSPM (R 4.4.0)\n P yaml          2.3.10  2024-07-26 [?] RSPM (R 4.4.0)\n\n [1] /home/runner/work/agds_book/agds_book/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/runner/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/3df92652\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>References</span>"
    ]
  }
]