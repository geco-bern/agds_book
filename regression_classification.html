<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Regression and classification – Applied Geodata Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./supervised_ml_I.html" rel="next">
<link href="./code_management.html" rel="prev">
<link href="././figures/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-f2e526a75abc17d413c930221ffb7d38.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-ed7e8abda2d2788c0893c9caeea0eb1f.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="9&nbsp; Regression and classification – Applied Geodata Science">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="regression_classification_files/figure-html/unnamed-chunk-2-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title"></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/stineb/agds_book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_classification.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Regression and classification</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting started</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./programming_primers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Programming primers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data wrangling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_vis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data visualisation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_variety.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data variety</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./open_science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Open science practices</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./code_management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Code management</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression_classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Regression and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_ml_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised machine learning I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_ml_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Supervised machine learning II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./randomforest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Random Forest</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interpretable-ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Interpretable Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">9.1</span> Learning objectives</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">9.2</span> Setup</a></li>
  <li><a href="#tutorial" id="toc-tutorial" class="nav-link" data-scroll-target="#tutorial"><span class="header-section-number">9.3</span> Tutorial</a>
  <ul class="collapse">
  <li><a href="#types-of-models" id="toc-types-of-models" class="nav-link" data-scroll-target="#types-of-models"><span class="header-section-number">9.3.1</span> Types of models</a></li>
  <li><a href="#regression" id="toc-regression" class="nav-link" data-scroll-target="#regression"><span class="header-section-number">9.3.2</span> Regression</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">9.3.3</span> Model selection</a></li>
  <li><a href="#outlier-detection" id="toc-outlier-detection" class="nav-link" data-scroll-target="#outlier-detection"><span class="header-section-number">9.3.4</span> Outlier detection</a></li>
  </ul></li>
  <li><a href="#extra-material" id="toc-extra-material" class="nav-link" data-scroll-target="#extra-material"><span class="header-section-number">9.4</span> Extra material</a>
  <ul class="collapse">
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification"><span class="header-section-number">9.4.1</span> Classification</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">9.5</span> Exercises</a></li>
  <li><a href="#report-exercise" id="toc-report-exercise" class="nav-link" data-scroll-target="#report-exercise"><span class="header-section-number">9.6</span> Report Exercise</a>
  <ul class="collapse">
  <li><a href="#deliverables-for-the-report" id="toc-deliverables-for-the-report" class="nav-link" data-scroll-target="#deliverables-for-the-report">Deliverables for the report</a></li>
  <li><a href="#guide-for-your-implementation" id="toc-guide-for-your-implementation" class="nav-link" data-scroll-target="#guide-for-your-implementation">Guide for your implementation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-regressionclassification" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Regression and classification</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Chapter lead author: Pepa Aran</strong></p>
<section id="learning-objectives" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">9.1</span> Learning objectives</h2>
<p>After completing this tutorial, you will be able to:</p>
<ul>
<li>Understand the basics of regression and classification models.</li>
<li>Fit linear and logistic regression models in R.</li>
<li>Choose and calculate relevant model performance metrics.</li>
<li>Evaluate and compare regression models.</li>
<li>Detect data outliers.</li>
<li>Select best predictive variables.</li>
</ul>
<p>Contents of this Chapter are inspired and partly adopted by the excellent book by <a href="https://bradleyboehmke.github.io/HOML/">Boehmke and Greenwell</a>.</p>
</section>
<section id="setup" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="setup"><span class="header-section-number">9.2</span> Setup</h2>
<p>In this Chapter, we will need the following libraries</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tutorial" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="tutorial"><span class="header-section-number">9.3</span> Tutorial</h2>
<section id="types-of-models" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="types-of-models"><span class="header-section-number">9.3.1</span> Types of models</h3>
<p>Models try to explain relationships between variables through a mathematical formulation, particularly to predict a given <em>target</em> variable using other <em>explanatory variables</em>, also called <em>predictors.</em> Generally, we say that the target variable <span class="math inline">\(Y\)</span> is a function (denoted <span class="math inline">\(f\)</span>) of a set of explanatory variables <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> and some model parameters <span class="math inline">\(\beta\)</span>. Models can be represented as: <span class="math display">\[Y \sim f(X_1, X_2, \dots, X_p, \beta)\]</span></p>
<p>This is a very general notation and depending on the structure of these components, we get to different modelling approaches.</p>
<p>The first distinction comes from the type of target variable. Whenever <span class="math inline">\(Y\)</span> is a continuous variable, we are facing a <em>regression</em> problem. If <span class="math inline">\(Y\)</span> is categorical, we speak of <em>classification</em>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regression</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Target variable</td>
<td>Continuous</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>Common models</td>
<td>Linear regression, polynomial regression, KNN, tree-based regression</td>
<td>Logistic regression, KNN, SVM, tree classifiers</td>
</tr>
<tr class="odd">
<td>Metrics</td>
<td>RMSE, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, BIC</td>
<td>Accuracy, precision, AUC, F1</td>
</tr>
</tbody>
</table>
</section>
<section id="regression" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="regression"><span class="header-section-number">9.3.2</span> Regression</h3>
<p>In this section, we will introduce the most basic regression model - linear regression. We will explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented.</p>
<section id="linear-regression" class="level4" data-number="9.3.2.1">
<h4 data-number="9.3.2.1" class="anchored" data-anchor-id="linear-regression"><span class="header-section-number">9.3.2.1</span> Linear regression</h4>
<p><strong>Theory</strong></p>
<p>Let’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective. Here, we will take a data-fitting approach.</p>
<p>For example, we can try to explain the relationship between GPP and short wave radiation, like in <a href="data_vis.html" class="quarto-xref"><span>Chapter 5</span></a>. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read and format data from Ch 3</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="st">"./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plot_1 <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">2000</span>) <span class="sc">|&gt;</span>  <span class="co"># to reduce the dataset</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>)), </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP (gC m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>segment_points <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x0 =</span> <span class="dv">332</span>, <span class="at">y0 =</span> <span class="fl">3.65</span>, <span class="at">y_regr =</span> <span class="fl">8.77</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plot_1 <span class="sc">+</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x0, <span class="at">y =</span> y0, <span class="at">xend =</span> x0, <span class="at">yend =</span> y_regr), </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> segment_points,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="fl">1.2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>To reproduce this code chunk, you can download the file <code>FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN</code> from <a href="https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv">here</a> and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored <a href="https://github.com/geco-bern/agds/tree/main/data">here</a>.</p>
</blockquote>
<p>We want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable <span class="math inline">\(X\)</span> and our target <span class="math inline">\(Y\)</span>: <span class="math display">\[
Y_i \sim \beta_0 + \beta_1 X_i, \;\;\; i = 1, 2, ...n \;,
\]</span> where <span class="math inline">\(Y_i\)</span> is the i-th observation of the target variable, and <span class="math inline">\(X_i\)</span> is the i-th value of the (single) predictor variable. <span class="math inline">\(n\)</span> is the number of observations we have and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are constant coefficients (model parameters). We call <span class="math inline">\(\beta_0\)</span> the intercept and <span class="math inline">\(\beta_1\)</span> the slope of the regression line. Generally, <span class="math inline">\(\hat{Y}\)</span> denotes the model prediction.</p>
<p>Fitting a linear regression is finding the values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that, on average over all points, the distance between the line at <span class="math inline">\(X_i\)</span>, that is <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> (blue segment in the plot above), and the observed value <span class="math inline">\(Y_i\)</span>, is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is: <span class="math display">\[
\min_{\beta_0, \beta_1} \sum_i (Y_i - \beta_0 - \beta_1 X_i)^2 .
\]</span> This linear model can be used to make predictions on new data, which are obtained by <span class="math inline">\(\hat{Y}_\text{new} = \beta_0 + \beta_1 X_\text{new}\)</span>. When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction.</p>
<p>It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of <span class="math inline">\(p\)</span> predictor variables: <span class="math display">\[Y \sim \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \; ... \; + \beta_p X_p \;.\]</span> Note that here, <span class="math inline">\(X_1, \dots, X_p\)</span> and <span class="math inline">\(Y\)</span> are vectors of length corresponding to the number of observations in our data set (<span class="math inline">\(n\)</span> - as above). Analogously, calibrating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2, ..., \beta_p\)</span> is to minimize the sum of square errors <span class="math inline">\(\min_{\beta} \sum_i (Y_i - \hat{Y}_i)^2\)</span>.</p>
<p>While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions.</p>
<p><strong>Implementation in R</strong></p>
<p>To fit a univariate linear regression model in R, we can use the <code>lm()</code> function. Already in <a href="data_wrangling.html" class="quarto-xref"><span>Chapter 4</span></a>, we created linear models by doing:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numerical variables only, remove NA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">starts_with</span>(<span class="st">"TIMESTAMP"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit univariate linear regression</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, <span class="at">data =</span> df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <code>GPP_NT_VUT_REF</code> is <span class="math inline">\(Y\)</span>, and <code>SW_IN_F</code> is <span class="math inline">\(X\)</span>. The <em>formula</em> notation <code>GPP_NT_VUT_REF ~ SW_IN_F</code> is common in R and can be used for a most functions in different packages. The (single) variable to the left of the <code>~</code> is the target variable (<span class="math inline">\(Y\)</span>). The variable to its right is the predictor. Of course, we can include multiple predictors for a multivariate regression, for example as:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit multivariate linear regression</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>linmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The shorthand for including all predictors that are available in the data (here, all columns other than <code>GPP_NT_VUT_REF</code> in <code>df</code>), we can write:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>linmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>linmod*</code> is now a model object of class <code>"lm"</code>. It is a list containing the following components:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span>(linmod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "assign"        "call"          "coefficients"  "df.residual"  
 [5] "effects"       "fitted.values" "model"         "qr"           
 [9] "rank"          "residuals"     "terms"         "xlevels"      </code></pre>
</div>
</div>
<p>Enter <code>?lm</code> in the console for a complete documentation of these components and other details of the linear model implementation.</p>
<p>R offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the <em>residuals</em> are the difference between the observed target values and the predicted values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-38.699  -2.092  -0.406   1.893  35.153 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.8732273  0.0285896   30.54   &lt;2e-16 ***
SW_IN_F     0.0255041  0.0001129  225.82   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.007 on 41299 degrees of freedom
Multiple R-squared:  0.5525,    Adjusted R-squared:  0.5525 
F-statistic: 5.099e+04 on 1 and 41299 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can also extract coefficients <span class="math inline">\(\beta\)</span> with</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)     SW_IN_F 
 0.87322728  0.02550413 </code></pre>
</div>
</div>
<p>and the residual sum of squares (which we wanted to minimize) with</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">residuals</span>(linmod1)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1035309</code></pre>
</div>
</div>
<p>Although <code>summary()</code> provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using <code>tidy()</code> from the <code>broom</code> package:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(linmod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 5
  term        estimate std.error statistic   p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)   0.873   0.0286        30.5 1.25e-202
2 SW_IN_F       0.0255  0.000113     226.  0        </code></pre>
</div>
</div>
<p><strong>Model advantages and concerns</strong></p>
<p>An advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that <code>GPP_NT_VUT_REF</code> increases by 0.0255 for a unit increase in <code>SW_IN_F</code>. Of course, the units of the coefficients depend on the units of <code>GPP_NT_VUT_REF</code> and <code>SW_IN_F</code>. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether <code>GPP_NT_VUT_REF</code> is given in (g C m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span>) or in (t C km<span class="math inline">\(^{-2}\)</span> year<span class="math inline">\(^{-1}\)</span>).</p>
<p>Another advantage of linear regression is that it’s much less prone to <em>overfit</em> than other algorithms. You’ll learn more about the concept of <em>overfitting</em> in <a href="supervised_ml_I.html" class="quarto-xref"><span>Chapter 10</span></a>. Not being prone to overfitting can also be a disadvantage: linear regression models are often not flexible enough to be effectively fit to the data. They are also not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, they often fit the data less well and generate poorer predictions than more complex models.</p>
<p>A further limitation is that least squares regression requires <span class="math inline">\(n&gt;p\)</span>. In words, the number of observations must be greater than the number of predictors. If this is not given, one can resort to stepwise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. We will encounter stepwise regression in the exercises.</p>
<p>When multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in Geography and Environmental Sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that in <a href="supervised_ml_II.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
<p>An alternative strategy is to use <em>dimension reduction</em> methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R <a href="https://bradleyboehmke.github.io/HOML/linear-regression.html#PCR">here</a>.</p>
</section>
<section id="regression-on-categorical-variables" class="level4" data-number="9.3.2.2">
<h4 data-number="9.3.2.2" class="anchored" data-anchor-id="regression-on-categorical-variables"><span class="header-section-number">9.3.2.2</span> Regression on categorical variables</h4>
<p>In the <a href="data_vis.html#vis_regr_cat">regression within categories</a> section of <a href="data_vis.html" class="quarto-xref"><span>Chapter 5</span></a>, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better.</p>
<p>So far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class <code>numeric</code>, otherwise the <code>lm()</code> function treats them as continuous variables. For example, although the variable <code>NIGHT</code> is categorical with values <code>0</code> and <code>1</code>, the model <code>linmod3</code> treats it as a number. We must make sure that categorical variables have class <code>character</code> or, even better, <code>factor</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create month category</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(TIMESTAMP_START)) <span class="sc">|&gt;</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>() <span class="sc">|&gt;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(MONTH, GPP_NT_VUT_REF, SW_IN_F)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># fix class of categorical variables</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> <span class="fu">as.factor</span>(MONTH))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can fit the linear model again:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>linmod_cat <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_cat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat)

Residuals:
    Min      1Q  Median      3Q     Max 
-36.212  -2.346  -0.223   2.200  34.416 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.6146109  0.0893693  18.067  &lt; 2e-16 ***
MONTH2      -1.8105447  0.1294675 -13.985  &lt; 2e-16 ***
MONTH3      -2.8800172  0.1264177 -22.782  &lt; 2e-16 ***
MONTH4      -2.5667281  0.1278097 -20.082  &lt; 2e-16 ***
MONTH5      -0.0288745  0.1273491  -0.227 0.820631    
MONTH6       0.4614556  0.1298069   3.555 0.000378 ***
MONTH7       0.1697514  0.1283830   1.322 0.186100    
MONTH8       1.2942463  0.1231252  10.512  &lt; 2e-16 ***
MONTH9       0.5140562  0.1165474   4.411 1.03e-05 ***
MONTH10     -0.4807082  0.1152536  -4.171 3.04e-05 ***
MONTH11     -1.3370277  0.1159059 -11.535  &lt; 2e-16 ***
MONTH12     -1.2634451  0.1151530 -10.972  &lt; 2e-16 ***
SW_IN_F      0.0246420  0.0001169 210.810  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.865 on 41288 degrees of freedom
Multiple R-squared:  0.5776,    Adjusted R-squared:  0.5775 
F-statistic:  4704 on 12 and 41288 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>In the fit summary, you can observe that, there are <code>MONTH2</code> to <code>MONTH12</code> parameters. <code>MONTH</code> is a factor which can take 12 different values: <code>1</code> to <code>12</code>. <code>lm()</code> uses one of the factor level as the reference, in this case <code>1</code>, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH_NAME =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(<span class="fu">as.integer</span>(MONTH), <span class="at">label =</span> <span class="cn">TRUE</span>)) <span class="sc">|&gt;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x <span class="sc">+</span> <span class="dv">0</span>, <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"SW"</span>, <span class="at">y =</span> <span class="st">"GPP"</span>) <span class="sc">+</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>MONTH_NAME) <span class="sc">+</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an <em>interaction</em> term <code>MONTH:SW_IN_F</code> in the regression formula, like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>linmod_inter <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F <span class="sc">+</span> MONTH<span class="sc">:</span>SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_inter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, 
    data = df_cat)

Residuals:
    Min      1Q  Median      3Q     Max 
-28.891  -2.113  -0.420   1.892  34.029 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      2.0449603  0.0944991  21.640  &lt; 2e-16 ***
MONTH2          -1.5386938  0.1369424 -11.236  &lt; 2e-16 ***
MONTH3          -1.5249304  0.1365863 -11.165  &lt; 2e-16 ***
MONTH4          -1.0050639  0.1396023  -7.199 6.15e-13 ***
MONTH5          -0.4502367  0.1412720  -3.187  0.00144 ** 
MONTH6          -1.2559057  0.1474257  -8.519  &lt; 2e-16 ***
MONTH7          -0.8440097  0.1446838  -5.833 5.47e-09 ***
MONTH8          -0.2188300  0.1346734  -1.625  0.10419    
MONTH9          -1.3407190  0.1269387 -10.562  &lt; 2e-16 ***
MONTH10         -0.9991456  0.1235627  -8.086 6.32e-16 ***
MONTH11         -1.2124373  0.1230946  -9.850  &lt; 2e-16 ***
MONTH12         -1.0724209  0.1210819  -8.857  &lt; 2e-16 ***
SW_IN_F          0.0158600  0.0008758  18.110  &lt; 2e-16 ***
MONTH2:SW_IN_F  -0.0030373  0.0011518  -2.637  0.00837 ** 
MONTH3:SW_IN_F  -0.0058229  0.0009713  -5.995 2.05e-09 ***
MONTH4:SW_IN_F  -0.0038333  0.0009469  -4.048 5.17e-05 ***
MONTH5:SW_IN_F   0.0087370  0.0009305   9.389  &lt; 2e-16 ***
MONTH6:SW_IN_F   0.0135219  0.0009172  14.743  &lt; 2e-16 ***
MONTH7:SW_IN_F   0.0110791  0.0009182  12.066  &lt; 2e-16 ***
MONTH8:SW_IN_F   0.0151014  0.0009317  16.209  &lt; 2e-16 ***
MONTH9:SW_IN_F   0.0180496  0.0009297  19.415  &lt; 2e-16 ***
MONTH10:SW_IN_F  0.0097277  0.0009761   9.966  &lt; 2e-16 ***
MONTH11:SW_IN_F -0.0011415  0.0010932  -1.044  0.29640    
MONTH12:SW_IN_F -0.0099745  0.0012972  -7.689 1.52e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.593 on 41277 degrees of freedom
Multiple R-squared:  0.6237,    Adjusted R-squared:  0.6234 
F-statistic:  2974 on 23 and 41277 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="polynomial-regression" class="level4" data-number="9.3.2.3">
<h4 data-number="9.3.2.3" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">9.3.2.3</span> Polynomial regression</h4>
<p>Furthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess">LOESS</a> (local polynomial regression fitting), etc.</p>
<p>Let’s fit a simple quadratic regression model, just for the month of August. For this we use the <code>poly()</code> function which constructs orthogonal polynomials of a given degree. Here, a second-order polynomial (a parabola) is fitted:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>quadmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> <span class="fu">poly</span>(SW_IN_F, <span class="dv">2</span>), </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(quadmod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, 
    MONTH == 8))

Residuals:
    Min      1Q  Median      3Q     Max 
-26.367  -2.055  -0.253   1.801  32.375 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          7.13084    0.07944   89.77   &lt;2e-16 ***
poly(SW_IN_F, 2)1  447.25113    4.61907   96.83   &lt;2e-16 ***
poly(SW_IN_F, 2)2 -151.08797    4.61907  -32.71   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.619 on 3378 degrees of freedom
Multiple R-squared:  0.7556,    Adjusted R-squared:  0.7555 
F-statistic:  5223 on 2 and 3378 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>In the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>) <span class="sc">|&gt;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">method =</span> <span class="st">"lm"</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"lm"</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), <span class="at">method =</span> <span class="st">"lm"</span>, </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"poly2"</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">3</span>), <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"poly3"</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"SW"</span>, <span class="at">y =</span> <span class="st">"GPP"</span>, <span class="at">color =</span> <span class="st">"Regression"</span>) <span class="sc">+</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="metrics-for-regression-evaluation" class="level4" data-number="9.3.2.4">
<h4 data-number="9.3.2.4" class="anchored" data-anchor-id="metrics-for-regression-evaluation"><span class="header-section-number">9.3.2.4</span> Metrics for regression evaluation</h4>
<p>Metrics measure the quality of fit between predicted and observed values. Metrics are essential to model fitting, model selection, and for describing and quantifying patterns in the data. Metrics are also key for guiding the training of machine learning models, as you will learn in <a href="supervised_ml_II.html" class="quarto-xref"><span>Chapter 11</span></a>. Different metrics measure different aspects of the model-data agreement. In other words, a single metric never captures all aspects and patterns of the model-data agreement. Therefore, a visual inspection of the model fit is always a good start of the model evaluation.</p>
<p>To get an intuitive understanding of the different abilities of different metrics, compare the scatterplots in <a href="#fig-correlationplots" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> and how different aspects of the model-data agreement are measured by different metrics. The observed target values <span class="math inline">\(Y\)</span> are plotted against the predicted values <span class="math inline">\(\hat{Y}\)</span> from a regression model, and the dashed line represents the ideal fit: predictions matching the data perfectly. Definitions of the metrics displayed and other metrics are given below.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-correlationplots" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-correlationplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_classification_files/figure-html/fig-correlationplots-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-correlationplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Correlation plots between observed and fitted target values.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Common metrics used for evaluating regression fits are:</p>
<p><strong>MSE</strong></p>
<p>The mean squared error is defined, as its name suggests, as: <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2
\]</span>It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in <a href="supervised_ml_I.html" class="quarto-xref"><span>Chapter 10</span></a>, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is sensitive to large errors in single points, including outliers.</p>
<p><strong>RMSE</strong></p>
<p>The root mean squared error is, as its name suggests, the root of the MSE: <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2}
\]</span>Like the MSE, the RMSE also measures the magnitude of the errors and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data <span class="math inline">\(Y\)</span> and is less sensitive to outliers than the MSE.</p>
<p><strong>MAE</strong></p>
<p>The mean absolute error is similarly defined: <span class="math display">\[
\text{MAE} = \frac{1}{n} \sum_{i = 1}^{n} |Y_i - \hat{Y_i}|
\]</span> Measuring the discrepancies between predictions and observations using absolute errors, instead of squared errors, gives less importance to errors of large magnitude and more importance to small errors than the MSE would. Hence, this measures is more stable in the presence of outliers.</p>
<p><strong><span class="math inline">\(R^2\)</span> - coefficient of determination</strong></p>
<p>describes the proportion of variation in <span class="math inline">\(Y\)</span> that is captured by modelled values <span class="math inline">\(\hat{Y}\)</span>. It tells us how much better our fitted values <span class="math inline">\(\hat{Y}\)</span> are than just taking the average of the target <span class="math inline">\(\bar{Y}\)</span> as predictions. In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, <span class="math inline">\(R^2\)</span> measures <em>consistency</em>, or <em>correlation</em>, or <em>goodness of fit</em>. It is defined as: <span class="math display">\[
R^2 = 1 - \frac{\sum_i (\hat{Y}_i - Y_i)^2}{\sum_i (Y_i - \bar{Y})^2}\\
\]</span> When the regression model is fitted by minimizing the MSE, the <span class="math inline">\(R^2\)</span> takes values between 0 and 1. A perfect fit is quantified by <span class="math inline">\(R^2 = 1\)</span>. There is no generally valid threshold of <span class="math inline">\(R^2\)</span> for a model to be considered “good”. It depends on the application and the nature of the data and the data-generating process. Note that the above equation can also be written as <span class="math inline">\(R^2 = 1 - \text{MSE}/var(Y)\)</span>.</p>
<p><strong><span class="math inline">\(r\)</span> - Pearson’s correlation</strong></p>
<p>The linear association between two variables (here <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span>) is measured by the <em>Pearson’s correlation coefficient</em> <span class="math inline">\(r\)</span>. <span class="math display">\[
r = \frac{\sum_i (Y_i - \bar{Y}) (\hat{Y_i} - \bar{\hat{Y}}) }{\sqrt{ \sum_i(Y_i-\bar{Y})^2 \; (\hat{Y_i}-\bar{\hat{Y}})^2 } }
\]</span></p>
<p>The correlation calculated between the target <span class="math inline">\(Y\)</span> and a predictor <span class="math inline">\(X\)</span> can tell us about the predictive power of a regression model (the higher the correlation, the more powerful). We can also compute the correlation between the target <span class="math inline">\(Y\)</span> and the predicted values <span class="math inline">\(\hat{Y}\)</span> by a model (multivariate, or even not linear) to assess the adequacy of the model chosen. See Figure <a href="#fig-correlationplots" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> as an example. It is noteworthy to mention that correlation is location and scale invariant, hence it will not detect model deviations like the ones in the middle row plots.</p>
<p>The squared value of the Pearson’s <em>r</em> is often reported as “<span class="math inline">\(R^2\)</span>” but is not equivalent to the definition of the coefficient of determination given above. However, the square of the Pearson’s <em>r</em> is closely related to the coefficient of determination <span class="math inline">\(R^2\)</span>. For a linear regression, fitted by minimizing the MSE, they are identical (see proof <a href="https://statproofbook.github.io/P/slr-rsq">here</a>). In subsequent chapters, we will use “<span class="math inline">\(R^2\)</span>” to refer to the square of the Pearson’s <em>r</em> between the observed <span class="math inline">\(Y\)</span> and predicted <span class="math inline">\(\hat{Y}\)</span> values.</p>
<blockquote class="blockquote">
<p>Note the implementations in R.The <span class="math inline">\(R^2\)</span> reported by the generic <code>summary()</code> function corresponds to the base-R function <code>cor()^2</code> , to <code>yardstick::rsq()</code>, and to the definition of the square of the <em>Pearson’s</em> <span class="math inline">\(r\)</span> given above. The <code>yardstick::rsq_trad()</code> returns the coefficient of determination as traditionally defined and is not equal to the values above, unless computed on the predicted values <span class="math inline">\(\hat{Y}\)</span>.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate correlated random data </span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1982</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">|&gt;</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_fitted =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)<span class="sc">$</span>fitted.values)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations using Pearson's correlation</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> df))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6186521</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>y, df<span class="sc">$</span>x)<span class="sc">^</span><span class="dv">2</span> <span class="co"># remember: location and scale invariant</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6186521</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq</span>(df, y, x) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6186521</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>x))<span class="sc">*</span>(df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>x))<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6186521</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations using coefficient of determination definition</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> df<span class="sc">$</span>y)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>) <span class="co"># should be \hat{y}, not x</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5993324</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df, y, x) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate) <span class="co"># incorrect</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5993324</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df, y, y_fitted) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate) <span class="co"># correct</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6186521</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>An “<span class="math inline">\(R^2\)</span>” is commonly reported when evaluating the <em>agreement</em> between observed and predicted values of a given model. When the <em>correlation</em> between two different variables in a sample is quantified, <span class="math inline">\(r\)</span> is commonly used to reflect also whether the correlation is positive or negative (<span class="math inline">\(r\)</span> can attain positive or negative values in the interval <span class="math inline">\([-1, 1]\)</span>). The coefficient of determination can return negative values when comparing observed and predicted values for uninformative estimates (worse than just using the average of <span class="math inline">\(Y\)</span>) and is thus not actually bound between 0 and 1. Therefore, be careful with the interpreration of “<span class="math inline">\(R^2\)</span>” and think on which variables it was computed and with which method.</p>
</blockquote>
<blockquote class="blockquote">
<p>Sometimes, the Person’s version is computed between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span>, and it leads to the same number due to its “location and scale invariant” property. Nevertheless, this is conceptually wrong, as we should look at the predictions, not the predictors: We are not predicting <span class="math inline">\(Y\)</span> by just giving the values of <span class="math inline">\(x\)</span> instead. Hence, especially when using {yardstick} functions, make sure you compute the values on <span class="math inline">\(\hat{Y}\)</span>. When we have several predictors, it’s already clear that we should compare <span class="math inline">\(Y\)</span> to <span class="math inline">\(\hat{Y}\)</span> instead of <span class="math inline">\(Y\)</span> to each predictor separately.</p>
</blockquote>
<p><strong>Bias</strong></p>
<p>The bias is simply the mean error: <span class="math display">\[
\text{bias} = \frac{1}{n} \sum_i^n{(\hat{Y}_i - Y_i)}
\]</span></p>
<p><strong>Slope</strong></p>
<p>The slope refers to the slope of the linear regression line between predicted and observed values. It is returned as the second element of the vector returned by <code>coef(lm(..))</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> y_fitted, <span class="at">data =</span> df))[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>y_fitted 
       1 </code></pre>
</div>
</div>
</section>
<section id="metrics-for-regression-model-comparison" class="level4" data-number="9.3.2.5">
<h4 data-number="9.3.2.5" class="anchored" data-anchor-id="metrics-for-regression-model-comparison"><span class="header-section-number">9.3.2.5</span> Metrics for regression model comparison</h4>
<p>In general, the aim of regression modelling is to find a model that best explains the data - but not the random errors in the data. More complex models tend to <em>overfit</em> more than simpler models. The implication of overfitting is that the model fits the data used for model fitting well, but doesn’t fit well when evaluating the predictions of the same model to new data (data not used for model fitting). In such a case, the model’s <em>generalisability</em> is poor. We’ll learn more about overfitting and generalisability in the context of supervised machine learning in later chapters. Often, simpler models generalise better than more complex model. The challenge is to strike a balance between complexity and generalisability. But how to find the “sweet spot” of this trade-off?</p>
<p>In this context it should be noted that the <span class="math inline">\(R^2\)</span> <em>always</em> increases when predictors are added to a model. Therefore, the <span class="math inline">\(R^2\)</span> is not a suitable metric for comparing models that differ with respect to their number of predictors - a factor controlling model complexity. <em>Cross-validation</em> can be regarded as the “gold-standard” for measuring model generalisability if the data is plentiful. It will be introduced in the context of supervised machine learning in <a href="supervised_ml_II.html" class="quarto-xref"><span>Chapter 11</span></a>. However, when the data size is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors <span class="math inline">\(p\)</span>. Different metrics are available:</p>
<p><strong>Adjusted</strong> <span class="math inline">\(R^2\)</span></p>
<p>The adjusted <span class="math inline">\(R^2\)</span> discounts values of <span class="math inline">\(R^2\)</span> by the number of predictors. It is defined as <span class="math display">\[
{R}^2_{adj} = 1 - (1-R^2) \; \frac{n-1}{n-p-1} \;,
\]</span> where <span class="math inline">\(n\)</span> (as before) is the number of observations, <span class="math inline">\(p\)</span> the number of parameters and <span class="math inline">\(R^2\)</span> the usual coefficient of determination. Same as for <span class="math inline">\(R^2\)</span>, the goal is to maximize <span class="math inline">\(R^2_{adj}\)</span>.</p>
<p><strong>AIC</strong></p>
<p>The Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as: <span class="math display">\[
\text{AIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + 2(p+2)
\]</span> where <span class="math inline">\(n\)</span> is the number of observations used for estimation, <span class="math inline">\(p\)</span> is the number of explanatory variables in the model and SSE is the sum of squared errors (SSE<span class="math inline">\(= \sum_i (Y_i-\hat{Y_i})^2\)</span>). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for generalisations to new data. Since it penalizes having many parameters, it will favor less complex models.</p>
<p><strong>AIC</strong><span class="math inline">\(_c\)</span></p>
<p>For small values of <span class="math inline">\(n\)</span> the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: <span class="math display">\[
\text{AIC}_c = \text{AIC} + \frac{2(p + 2)(p + 3)}{n-p-3}
\]</span> Also AIC<span class="math inline">\(_c\)</span> is minimized for an optimal predictive model.</p>
<p><strong>BIC</strong></p>
<p>The Schwarz’s Bayesian Information Criterion is defined as <span class="math display">\[
\text{BIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + (p+2)  \log(n)
\]</span> Also for BIC, the goal is to minimize it. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC.</p>
<p><strong>Implementation in R</strong></p>
<p>Let’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{adj}\)</span> are given by the <code>summary()</code> function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>compute_regr_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(mod){</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">length</span>(mod<span class="sc">$</span>coefficients)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(mod<span class="sc">$</span>residuals)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse =</span> <span class="fu">mean</span>(mod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2 =</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared,</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2_adj =</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>adj.r.squared,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC =</span> <span class="fu">extractAIC</span>(mod)[<span class="dv">2</span>],</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC_adj =</span> <span class="fu">extractAIC</span>(mod)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>(p<span class="sc">+</span><span class="dv">2</span>)<span class="sc">*</span>(p<span class="sc">+</span><span class="dv">3</span>)<span class="sc">/</span>(n<span class="sc">-</span>p<span class="dv">-3</span>),</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">BIC =</span> <span class="fu">BIC</span>(mod) <span class="co"># this implementation is based on log-likelihood</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>list_metrics <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map</span>(</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(linmod1, linmod2, linmod_cat, quadmod), </span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span><span class="fu">compute_regr_metrics</span>(.))</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(list_metrics) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Linear model"</span>, </span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"Linear model 2"</span>, </span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"Linear + categories"</span>,</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"Quadratic model"</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(list_metrics, <span class="at">.id =</span> <span class="st">"type"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 7
  type                  mse    R2 R2_adj     AIC AIC_adj     BIC
  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
1 Linear model         25.1 0.553  0.553 133058. 133058. 250293.
2 Linear model 2       24.8 0.558  0.558 132590. 132590. 249842.
3 Linear + categories  23.7 0.578  0.577 130700. 130700. 248030.
4 Quadratic model      21.3 0.756  0.755  10350.  10350.  19972.</code></pre>
</div>
</div>
</section>
</section>
<section id="model-selection" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">9.3.3</span> Model selection</h3>
<p>Model selection refers to several techniques that help you compare models and select the one that best explains the data. Here, concepts will be explained using regression as an example, but are directly translated to classification problems.</p>
<p>First, be systematic with model comparisons. Three key ideas in model selection are:</p>
<ul>
<li>Comparisons should be hierarchical: compare a model to another that “contains it”, i.e.&nbsp;compare <code>y ~ x1</code> to <code>y ~ x1 + x2</code>, and not <code>y ~ x1</code> to <code>y ~ x2 + x3</code>.</li>
<li>Complexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors.</li>
<li>Choose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g., recall for a classification where you don’t want to miss any positives).</li>
</ul>
<p>If you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best KNN non-parametric regression model and a random forest. Then compare those three final models and choose the one that answers your research question the best.</p>
<p>One must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. <a href="supervised_ml_I.html" class="quarto-xref"><span>Chapter 10</span></a> introduces the concept of <em>overfitting</em>, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g., by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships).</p>
<section id="sec-variableselection" class="level4" data-number="9.3.3.1">
<h4 data-number="9.3.3.1" class="anchored" data-anchor-id="sec-variableselection"><span class="header-section-number">9.3.3.1</span> Variable selection</h4>
<p>Let’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of all available predictors, calculate a metric (e.g., AIC) and choose the best one (lowest AIC). The problem is, if you have, say, eight predictors, you would fit 40320 different regression models. This can be very computationally expensive.</p>
<p>Instead, take a hierarchical, or “greedy”, approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called <em>stepwise forward regression</em>. The algorithm goes as follows:</p>
<ol type="1">
<li>Set the number of predictors to be considered to <span class="math inline">\(p=1\)</span>.</li>
<li>Fit all regression models with <span class="math inline">\(p\)</span> predictors and compute their <span class="math inline">\(R^2\)</span>.</li>
<li>Select the model with <span class="math inline">\(p\)</span> predictors that achieves the highest <span class="math inline">\(R^2\)</span> (best fitting model) and compute its AIC.</li>
<li>Increment to <span class="math inline">\(p+1\)</span>. Fit all regression models with <span class="math inline">\(p+1\)</span> predictors that include the predictor selected at the previous step and compute their <span class="math inline">\(R^2\)</span>. Select the best fitting model and compute its AIC.</li>
<li>If the AIC of the model with <span class="math inline">\(p+1\)</span> predictors is poorer than the AIC of the model with <span class="math inline">\(p\)</span> predictors, retain the model with <span class="math inline">\(p\)</span> predictors and quit. You have found the (presumably) optimal model. Otherwise, continue with with step 4.</li>
</ol>
<p>Instead of the AIC, you may also use the BIC for model comparison. Note that algorithm doesn’t consider all possible combinations of predictors and it is possible that the globally optimal model is thus not found. The function <code>stats::step()</code> implements the stepwise algorithm in R.</p>
<p>This stepwise approach can also be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression whereby the aim si to increase the <em>degree</em> of the polynomials - step by step. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”.</p>
<p><strong>Multicollinearity</strong> exists when there is a correlation between multiple predictors in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the correlated variables unstable. Since correlated variables contain similar information, it doesn’t matter whether we include one or the other in the model or even if we include both of them. The performance metrics will be similar. Hence, it becomes difficult to conclude which variables actually influence the target.</p>
<p>The <strong>variance inflation factor (VIF)</strong> is a score from economics that measures the amount of multicollinearity in regression based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as <span class="math display">\[\text{VIF}_j = \frac{1}{1 - R^2_j},\]</span> where <span class="math inline">\(R^2_j\)</span> is the coefficient of determination for regressing the <span class="math inline">\(j^{th}\)</span> predictor on the <span class="math inline">\(p-1\)</span> remaining predictors. More specifically, <span class="math display">\[R^2_j = 1 - \frac{\sum_i (X_{j, i} - \hat{X}_{j, i})^2}{\sum_i (X_{j, i} - \bar{X}_j)^2},\]</span> where <span class="math inline">\(\hat{X}_{j, i}\)</span> is the fitted value corresponding to the regression <span class="math inline">\(X_j \sim X_1 + ... X_{j-1} + X_{j+1} + ... + X_p\)</span> for the <span class="math inline">\(i^{th}\)</span> observation. A VIF<span class="math inline">\(_j\)</span> is computed for each of the <span class="math inline">\(p\)</span> predictors in the multivariate regression model we are evaluating, and their values interpreted to detect multicollinearity. Meaning: if <span class="math inline">\(\text{VIF}_j = 1\)</span> variables are not correlated; if <span class="math inline">\(1 &lt; \text{VIF}_j &lt; 5\)</span> there is moderate collinearity between <span class="math inline">\(X_j\)</span> and the rest of predictors; and if <span class="math inline">\(\text{VIF}_j \geq 5\)</span> they are highly correlated. Because variable <span class="math inline">\(X_{j}\)</span> can be almost fully explained by all the other predictors (high <span class="math inline">\(R^2_j\)</span>), this variable is redundant in our final model.</p>
<p>To remedy collinearity, you may choose to use only one or two of those correlated variables. Another option would be to use <em>Principal Component Analysis</em> (PCA), which you may read more about <a href="https://www.r-bloggers.com/2021/05/principal-component-analysis-pca-in-r/">here</a>. What PCA does is to map the space of predictors into another space of smaller dimension, leading to a smaller set of predictor variables <span class="math inline">\(\{Z_1, ... , Z_q\}\)</span>, each of them being a linear combination of all the initial available predictors, that is <span class="math inline">\(Z_1 = \alpha^1_0 + \alpha^1_1 X_1 + ... + \alpha^1_p X_p\)</span>, etc. If you have collinearity, those highly correlated variables would be summarized into one single new variable, called <em>principal component</em>.</p>
<p>When we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check <a href="https://www.r-bloggers.com/2020/06/understanding-lasso-and-ridge-regression/">this post</a> if you’re curious).</p>
</section>
</section>
<section id="outlier-detection" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="outlier-detection"><span class="header-section-number">9.3.4</span> Outlier detection</h3>
<p>Detecting outliers is important, because they can affect the fit of a model and render the model fitting not robust. When the data is large, individual points have less influence on the model fitting. Therefore, only outliers that are very far from normal values will affect the model fit (see below). Outliers are particularly problematic in the context of small data, because every value has a big influence on the fitted model.</p>
<p>Take a look at the two linear regressions below and how one single point can strongly influence the fit. the added point (red circle around black dot) lies clearly outside the “cloud” of remaining points and doesn’t seem to follow the same pattern in the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes_small <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">100</span>) <span class="sc">|&gt;</span> <span class="co"># reduce dataset</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(SW_IN_F, GPP_NT_VUT_REF)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plot_3 <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>)), </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP ("</span>, mu,<span class="st">"mol CO"</span>[<span class="dv">2</span>], <span class="st">" m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1100</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>plot_4 <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">1100</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>)), </span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP ("</span>, mu,<span class="st">"mol CO"</span>[<span class="dv">2</span>], <span class="st">" m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">1100</span>, <span class="at">y =</span> <span class="sc">-</span><span class="dv">20</span>), <span class="at">colour =</span> <span class="st">'red'</span>, <span class="at">shape =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1100</span>)</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_3, plot_4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'
`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-outlier" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-outlier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_classification_files/figure-html/fig-outlier-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-outlier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: The influence of an outlier on a regression fit.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first step to identifying outliers is usually to look at the data, one variable at a time. Plot a histogram to check the distribution of a variable. The shape of the distribution is very informative for what can be considered an <em>outlier</em>. In <a href="data_wrangling.html" class="quarto-xref"><span>Chapter 4</span></a> and <a href="data_vis.html" class="quarto-xref"><span>Chapter 5</span></a> it was introduced how to identify values that fall “outside” a distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an outlier for demonstration purposes</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes_outlier <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">1100</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Various ways to identify the outlier using graphs</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>plot_5 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> GPP_NT_VUT_REF, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">"grey70"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">'red'</span>)<span class="sc">+</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Histogram, density and boxplot'</span>, </span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP (gC m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>plot_6 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">""</span>, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">"grey70"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP (gC m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>)))</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>plot_7 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">"grey70"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">'red'</span>)<span class="sc">+</span></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Histogram, density and boxplot'</span>, </span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>plot_8 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">""</span>, <span class="at">y =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">"grey70"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>)))</span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_5, plot_7, plot_6, plot_8,</span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>                   <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">rel_heights =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>),</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>                   <span class="at">align =</span> <span class="st">'v'</span>, <span class="at">axis =</span> <span class="st">'lr'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>A <strong>Q-Q Plot</strong> depicts the sample quantiles of a variable against the theoretical quantiles of a distribution that is assumed to uderly the data. In the histograms above, GPP looks somewhat Gaussian (normally distributed) but with fatter tails and slightly skewed to the right, while shortwave radiation is clearly skewed to the right, resembling an exponential distribution. The Q-Q plots below reveal that both variables are clearly not normally distributed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>plot_9 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">sample =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"GPP (gC m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>)),</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Theoretical normal quantiles"</span>) <span class="sc">+</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>plot_10 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Shortwave radiation (W m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">")"</span>)),</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Theoretical normal quantiles"</span>) <span class="sc">+</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_9, plot_10, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>QQ-plots serve to investigate whether the data follows an assumed theoretical distribution. For linear and logistic regression, we would like predictor variables to look as normal, i.e.&nbsp;Gaussian, as possible. You’ve probably learned some of the reasons for this in the <em>Quantitative Methods</em> course (or a similar classical Statistics course), and we will not discuss this further here.</p>
<p>Note however, that neither the histograms, nor the boxplots, or the QQ-plots revealed any evidently suspicious pattern of the added point (shortwave radiation = 1100 W m<span class="math inline">\(^{-2}\)</span> s<span class="math inline">\(^{-1}\)</span>, and GPP = -20 <span class="math inline">\(\mu\)</span>mol CO<span class="math inline">\(_2\)</span> m<span class="math inline">\(^{-2}\)</span> s<span class="math inline">\(^{-1}\)</span>) compared to the <em>individual</em> distributions of the two variables. Yet, in <a href="#fig-outlier" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>, the influence of the outlying point on the fit was clear. Because the suspicious observation is off the multivariate pattern in the remaining data, it becomes very influential. That is, it has a big <em>leverage</em>. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs.&nbsp;Leverage” plot:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit regression with outlier</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>linmod_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> <span class="fu">add_row</span>(half_hourly_fluxes_small, </span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">SW_IN_F =</span> <span class="dv">1100</span>, </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>))</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(linmod_outlier, <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This plot shows the leverage (see the mathematical definition <a href="https://en.wikipedia.org/wiki/Leverage_(statistics)">here</a>) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e., points that are far from the center of the predictor distribution, and large residuals, i.e., points that are far from the fitted regression line, are very influential. The <strong>Cook’s distance</strong> (definition <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">here</a>) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are drawn with a dashed line.</p>
<p>Finally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data.</p>
</section>
</section>
<section id="extra-material" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="extra-material"><span class="header-section-number">9.4</span> Extra material</h2>
<section id="classification" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="classification"><span class="header-section-number">9.4.1</span> Classification</h3>
<p>Classification models predict a categorical target variable. Note that predictors of a classification model may still be, and often are, continuous. We will introduce a classification problem with a binary target, since it’s straightforward to generalize to categorical variables with more than two classes. As an example, we use the <code>CO2</code> dataset from the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/zCO2.html">{datasets} package</a>, which contains data from an experiment on the cold tolerance of the grass species <em>Echinochloa crus-galli</em>.</p>
<div id="fig-grass" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig.align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://swbiodiversity.org/imglib/h_seinet/seinet/genfield/palexander/set003/Echinochloa_c-g_19Aug06_1595.jpg" class="img-fluid figure-img" style="width:60.0%" data-fig.align="center">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: <em>Echinochloa crus-galli</em>, image from <a href="https://swbiodiversity.org/seinet/taxa/index.php?taxon=2915">swbiodiversity.org</a>
</figcaption>
</figure>
</div>
<p>We will try to classify the origin of each plant (categorical variable <code>Type</code> with values <code>Quebec</code> or <code>Mississippi</code>) depending on the carbon dioxide uptake rate of the plant (continuous variable <code>uptake</code> measured in <span class="math inline">\(\mu\)</span>mol m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span>). More information on the dataset can be obtained by typing <code>?datasets::CO2</code> in the console.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>datasets<span class="sc">::</span>CO2 <span class="sc">|&gt;</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> uptake, <span class="at">y =</span> Type, <span class="at">color =</span> Type)) <span class="sc">+</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Uptake ("</span>, mu,<span class="st">"mol CO"</span>[<span class="dv">2</span>], <span class="st">" m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>At first sight, it’s easy to see that the carbon uptake is lower for the <em>Mississippi</em> type. Note that other predictors can be included in the model, but we’ll focus on a single predictor. Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification.</p>
<section id="logistic-regression" class="level4" data-number="9.4.1.1">
<h4 data-number="9.4.1.1" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">9.4.1.1</span> Logistic regression</h4>
<p><strong>Theory</strong></p>
<p>A classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of <code>GPP_NT_VUT_REF ~ SW_IN_F</code> was <code>GPP_NT_VUT_REF</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>SW_IN_F</code>. With the classification model <code>Type ~ uptake</code>, we cannot just write <code>Type</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>uptake</code> because <code>Type</code> is not a number. Hence, the categorical variable must be encoded, in this case 0 represents <code>Quebec</code> and 1 represents <code>Mississippi</code>.</p>
<p>The next issue is that a linear model makes continuous predictions in the entire real numbers space <span class="math inline">\((-\inf, \inf)\)</span>, but we want the predictions to be either 0 or 1. We can transform these values to be in the interval <span class="math inline">\([0,1]\)</span> with a <em>link</em> function. For a binary response, it’s common to use a <em>logit</em> link function: <span class="math display">\[\text{logit}(z) = \frac{\exp(z)}{1+\exp(z)}.\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">exp</span>(x)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(x))) <span class="sc">+</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"logit(x)"</span>, <span class="at">x =</span> <span class="st">"x"</span>) <span class="sc">+</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
</div>
</div>
<p>Combining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive at the <strong>logistic regression</strong> model:</p>
<p><span class="math display">\[f(X, \beta) = \text{logit}(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p) = \frac{\exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}{1 + \exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}.
\]</span> This predicted value can be understood as the probability of belonging to class 1 (in our example, <code>Mississippi</code>). A classification rule is defined such that an observation <span class="math inline">\(X_{new}\)</span> with a predicted probability of belonging to class 1 higher than a given <em>threshold</em> <span class="math inline">\(\tau\)</span> (i.e.&nbsp;<span class="math inline">\(f(X_{new}, \beta) &gt; \tau\)</span>) will be classified as 1. If the predicted probability is smaller than the threshold <span class="math inline">\(\tau\)</span>, it will be classified as 0.</p>
<p>A logistic regression model results in a linear classification rule. This means that the <span class="math inline">\(p\)</span>-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with carbon uptake as predictor, the classification boundary would be a point dividing the real line. If we include a second predictor, we would obtain a line dividing the 2-dimensional plane in two.</p>
<p>Furthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator of <span class="math inline">\(\beta\)</span> with an iterative algorithm. We will learn more about iterative model fitting, i.e.&nbsp;parameter optimization, in the context of supervised machine learning (<a href="supervised_ml_II.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
<p><strong>Implementation in R</strong></p>
<p>First, let’s see how the target variable is encoded. R directly loads the dataframe with <code>Type</code> as a factor and <code>Quebec</code> as the reference level. R factors work such that each factor level (here <code>Quebec</code> and <code>Mississippi</code>) corresponds to an integer value (its position given by <code>levels()</code>, here <code>1</code> and <code>2</code> respectively). We can fit a logistic model in R with this encoding.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(datasets<span class="sc">::</span>CO2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:
 $ Plant    : Ord.factor w/ 12 levels "Qn1"&lt;"Qn2"&lt;"Qn3"&lt;..: 1 1 1 1 1 1 1 2 2 2 ...
 $ Type     : Factor w/ 2 levels "Quebec","Mississippi": 1 1 1 1 1 1 1 1 1 1 ...
 $ Treatment: Factor w/ 2 levels "nonchilled","chilled": 1 1 1 1 1 1 1 1 1 1 ...
 $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...
 $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...
 - attr(*, "formula")=Class 'formula'  language uptake ~ conc | Plant
  .. ..- attr(*, ".Environment")=&lt;environment: R_EmptyEnv&gt; 
 - attr(*, "outer")=Class 'formula'  language ~Treatment * Type
  .. ..- attr(*, ".Environment")=&lt;environment: R_EmptyEnv&gt; 
 - attr(*, "labels")=List of 2
  ..$ x: chr "Ambient carbon dioxide concentration"
  ..$ y: chr "CO2 uptake rate"
 - attr(*, "units")=List of 2
  ..$ x: chr "(uL/L)"
  ..$ y: chr "(umol/m^2 s)"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(datasets<span class="sc">::</span>CO2<span class="sc">$</span>Type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Quebec"      "Mississippi"</code></pre>
</div>
</div>
<p>To fit a logistic regression in R we can use the <code>glm()</code> function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>logmod <span class="ot">&lt;-</span> <span class="fu">glm</span>(Type <span class="sc">~</span> uptake,</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit),</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> datasets<span class="sc">::</span>CO2)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logmod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Type ~ uptake, family = binomial(link = logit), 
    data = datasets::CO2)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  3.87192    0.87273   4.437 9.14e-06 ***
uptake      -0.14130    0.02992  -4.723 2.32e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 116.449  on 83  degrees of freedom
Residual deviance:  83.673  on 82  degrees of freedom
AIC: 87.673

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>This fitted model results in a linear classification boundary (discontinued line) that splits the predictor variables space in two. Where that line falls depends on the choice of threshold, in this case <span class="math inline">\(\tau=0.5\)</span> (see where the grey logistic regression line meets the dashed threshold line). You can see it plotted below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">coef</span>(logmod)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># reuse previous plot with classification line</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>datasets<span class="sc">::</span>CO2 <span class="sc">|&gt;</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> uptake, <span class="at">y =</span> <span class="fu">as.numeric</span>(Type)<span class="sc">-</span><span class="dv">1</span>, <span class="at">color =</span> Type)) <span class="sc">+</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Uptake ("</span>, mu,<span class="st">"mol CO"</span>[<span class="dv">2</span>], <span class="st">" m"</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">"s"</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">")"</span>)),</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># call glm model fit as part of the plotting</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method =</span> <span class="st">"glm"</span>, <span class="at">color =</span> <span class="st">"grey"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>,</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family =</span> binomial),</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># manually plot of logit function with fitted coefficients</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">exp</span>(beta[<span class="dv">1</span>] <span class="sc">+</span> beta[<span class="dv">2</span>] <span class="sc">*</span> x)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(beta[<span class="dv">1</span>] <span class="sc">+</span> beta[<span class="dv">2</span>] <span class="sc">*</span> x)),</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># visualise threshold</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="sc">-</span>beta[<span class="dv">1</span>] <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">linetype =</span> <span class="st">"dotted"</span>) <span class="sc">+</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Most turquoise points fall to one side of the dotted line, representing the threshold <span class="math inline">\(\tau\)</span>, and most reddish points are the other side. This is what we intended. The points that are on the wrong side of the line are <em>mis-classified</em> by the logistic regression model. Our aim is to minimize mis-classification.</p>
<p>Note that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models.</p>
<p><strong>Model advantages and concerns</strong></p>
<p>One advantage of logistic regression is simplicity. It’s part of the <em>generalized linear regression</em> family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Wikipedia article</a>.</p>
<p>Furthermore, logistic regression allows for an interesting interpretation of its model parameters: <em>odds</em> and <em>log-odds</em>. Odds represent how likely it is to find one class versus the other (e.g., if class 1 is twice as likely as class 0, then we have probabilities <span class="math inline">\(66\%\)</span> vs.&nbsp;<span class="math inline">\(33\%\)</span>). The <em>odds</em> are defined as the probability of <span class="math inline">\(Y\)</span> belonging to class 1 divided by the probabiity of belonging to class 0, and relates to the model parameters as <span class="math display">\[\frac{P(Y_i=1)}{P(Y_i=0)} = \exp(\beta_0+\beta_1 X_i).\]</span> So the log-odds are <span class="math display">\[\log\left(\frac{P(Y_i=1)}{P(Y_i=0)}\right) = \beta_0+\beta_1 X_i.\]</span> Increases in the values of the predictors affect the odds multiplicatively and the log-odds linearly.</p>
<p>It is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3.</p>
<p>Nevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. Whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in <a href="supervised_ml_I.html" class="quarto-xref"><span>Chapter 10</span></a> and <a href="supervised_ml_II.html" class="quarto-xref"><span>Chapter 11</span></a>, can also be used for classification tasks. These offer more flexibility than logistic regression, are not necessarily linear, and don’t need to satisfy strict statistical assumptions.</p>
</section>
<section id="metrics-for-classification" class="level4" data-number="9.4.1.2">
<h4 data-number="9.4.1.2" class="anchored" data-anchor-id="metrics-for-classification"><span class="header-section-number">9.4.1.2</span> Metrics for classification</h4>
<p>Measuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a <em>confusion matrix</em>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(Y = 1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Y = 0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{Y} = 1\)</span></td>
<td style="text-align: center;">True positives (TP)</td>
<td style="text-align: center;">False positives (FP)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y} = 0\)</span></td>
<td style="text-align: center;">False negatives (FN)</td>
<td style="text-align: center;">True negatives (TN)</td>
</tr>
</tbody>
</table>
<p>In a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications.</p>
<p>Imagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives.</p>
<p>The following metrics are widely used and highlight different aspects of our modeling goals.</p>
<ul>
<li><strong>Accuracy</strong> is simply the proportion of outputs that were correctly classified: <span class="math display">\[ \text{Accuracy}=\frac{\text{TP} + \text{TN}}{N},\]</span> where <span class="math inline">\(N\)</span> is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value <span class="math inline">\(\frac{1}{C}\)</span> where <span class="math inline">\(C\)</span> is the number of classes.</li>
</ul>
<p>Classification models are usually compared to randomness: How much better is our model compared to throwing a coin for classification? At random, we would assign each class <span class="math inline">\(50\%\)</span> of the time. So if we assume that both classes are as likely to appear, that is, they are <em>balanced</em>, the accuracy of a random guess would be around <span class="math inline">\(0.5\)</span>. Hence, we want the accuracy to be “better than random”. If there are <span class="math inline">\(C\)</span> different classes and the observations are balanced, we want the accuracy to be above <span class="math inline">\(1-1/C\)</span>.</p>
<p>A challenge is posed by <em>imbalanced</em> classes. For a dataset where <span class="math inline">\(90\%\)</span> of the observations are from class 1 and <span class="math inline">\(10\%\)</span> from class 0, always predicting 1 would lead to a accuracy of <span class="math inline">\(0.9\)</span>. This value may sound good, but that model is not informative because it doesn’t use any information from predictors. Therefore, be careful when working with imbalanced classes and interpreting your results.</p>
<ul>
<li><p><strong>Precision</strong> measures how often our “positive” predictions are correct: <span class="math display">\[\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.\]</span></p></li>
<li><p>The <strong>true positive rate</strong> (TPR), also called <strong>recall</strong> or <strong>sensitivity</strong> measures the proportion of real “positives” (<span class="math inline">\(Y = 1\)</span>) we are able to capture: <span class="math display">\[ \text{TPR} = \frac{\text{TP}}{\text{TP}+\text{FN}}.\]</span></p></li>
<li><p>The <strong>false positive rate</strong> (FPR) is defined by <span class="math display">\[\text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}}.\]</span> and is related to another metric called <strong>specificity</strong> by <span class="math inline">\(\text{FPR} = 1 - \text{specificity}\)</span>.</p></li>
<li><p><strong>Receiver operating characteristic (ROC) curve</strong>: To evaluate the performance of a binary classification model, it’s common to plot the <em>ROC curve</em>, where the TPR is plotted against the FPR, for varying values of the threshold <span class="math inline">\(\tau\)</span> used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot:</p></li>
</ul>
<div id="fig-roccurve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig.align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roccurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./figures/Roc_curve.png" class="img-fluid figure-img" style="width:60.0%" data-fig.align="center">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roccurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: ROC curves and how they compare, from <a href="https://commons.wikimedia.org/wiki/File:Roc_curve.svg">Wikimedia Commons</a>.
</figcaption>
</figure>
</div>
<ul>
<li><p><strong>AUC</strong>: The “area under the curve” is defined as the area betwee the ROC curve and the x-axis. For a random classifier we would have AUC = 0.5 and for the perfect classifier, AUC = 1. The aim is to increase the AUC. Nevertheless, a visual inspection of the ROC curve can say even more.</p></li>
<li><p><strong>F1</strong>: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values: <span class="math display">\[
F1= 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}} = \frac{2 \text{TP}}{2 \text{TP} + \text{FP} + \text{FN}}.
\]</span> This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad.</p></li>
</ul>
<p>These metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives.</p>
<p>This was an introduction of the most basic classification metrics. For a more information on the topic, check out <a href="https://bookdown.org/max/FES/measuring-performance.html#class-metrics">Kuhn &amp; Johnson (2019), Chapter 3.3.2</a>.</p>
<p><strong>Implementation in R</strong></p>
<p>Let’s take a look at the previous metrics for the logistic regression model we fitted before. The <code>confusionMatrix()</code> function from the {caret} library provides most of the statistics introduced above.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make classification predictions</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> logmod<span class="sc">$</span>data<span class="sc">$</span>Type</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(logmod<span class="sc">$</span>fitted.values)) <span class="co"># Use 0.5 as threshold</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Change class names</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(Y) <span class="ot">&lt;-</span> <span class="fu">levels</span>(x) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Quebec"</span>, <span class="st">"Mississippi"</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot confusion matrix</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="at">data =</span> x, <span class="at">reference =</span> Y)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>conf_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

             Reference
Prediction    Quebec Mississippi
  Quebec          32          13
  Mississippi     10          29
                                         
               Accuracy : 0.7262         
                 95% CI : (0.618, 0.8179)
    No Information Rate : 0.5            
    P-Value [Acc &gt; NIR] : 2.039e-05      
                                         
                  Kappa : 0.4524         
                                         
 Mcnemar's Test P-Value : 0.6767         
                                         
            Sensitivity : 0.7619         
            Specificity : 0.6905         
         Pos Pred Value : 0.7111         
         Neg Pred Value : 0.7436         
             Prevalence : 0.5000         
         Detection Rate : 0.3810         
   Detection Prevalence : 0.5357         
      Balanced Accuracy : 0.7262         
                                         
       'Positive' Class : Quebec         
                                         </code></pre>
</div>
</div>
<p>Now we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mosaicplot</span>(conf_matrix<span class="sc">$</span>table,</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Confusion matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_classification_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="exercises" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="exercises"><span class="header-section-number">9.5</span> Exercises</h2>
<p>There are no exercises with provided solutions for this Chapter.</p>
<!-- -   Implement the formula for RMSE using simple "low-level" functions -->
<!--     like `sqrt()` and `mean()`. Confirm that the function `rmse()` from -->
<!--     the yardstick package computes the RMSE the same way. -->
<!--     ```{r} -->
<!--     ## generate random data  -->
<!--     df <- tibble(x = rnorm(100)) |>  -->
<!--       mutate(y = x + rnorm(100), -->
<!--              x = x) -->
<!--     yardstick::rmse(df, y, x) |> pull(.estimate) -->
<!--     sqrt(mean((df$x - df$y)^2)) -->
<!--     ``` -->
</section>
<section id="report-exercise" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="report-exercise"><span class="header-section-number">9.6</span> Report Exercise</h2>
<p>Although there are a lot of helpful packages and user-friendly functions available, you will often have to write your own data analysis routine. This requires good understanding of statistical knowledge, algorithmic thinking and problem-solving skills. While writing your code, you will face many questions and bugs that you need to solve. And knowing where and how to ask for help properly are crucial parts of this process (see <a href="programming_primers.html#sec-findinghelp" class="quarto-xref"><span>Section 3.2.3</span></a> for more on getting help).</p>
<p>In this Report Exercise, stepwise forward regression is to be performed for the task of modelling GPP as a function of predictors available in the dataset of half-hourly ecosystem fluxes. To learn the skills mentioned above, you will (attempt to) write your own stepwise forward regression from scratch based on the algorithm description of stepwise forward regression in <a href="#sec-variableselection" class="quarto-xref"><span>Section 9.3.3.1</span></a>. The data of half-hourly fluxes can be downloaded from <a href="https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/df_for_stepwise_regression.csv">here</a>.</p>
<section id="deliverables-for-the-report" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="deliverables-for-the-report">Deliverables for the report</h3>
<p>The complete workflow of reading the data, performing the stepwise forward regression, visualising and discussing the results should be implemented in an RMarkdown notebook file and added as file <code>./vignettes/re_stepwise.Rmd</code> to your <em>git</em> repository. You will then point us to the URL of your repository. We will fork your repository and reproduce the workflow implemented in your notebook. The following aspects should be covered in your notebook:</p>
<ol type="1">
<li><p>An evaluation of all bivariate models (single predictor), implementing just steps 1-3 of the algorithm described in <a href="#sec-variableselection" class="quarto-xref"><span>Section 9.3.3.1</span></a>. This should be complemented by a visualisation and a brief discussion of the results.</p></li>
<li><p>An implementation of stepwise forward regression, and a visualisation and discussion of its results. If you face unsurmountable challenges and/or errors you didn’t manage to resolve, your notebook may implement a minimum reproducible example of your code that produces the error or of your <em>attempted</em> implementation of stepwise forward regression.</p></li>
</ol>
</section>
<section id="guide-for-your-implementation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="guide-for-your-implementation">Guide for your implementation</h3>
<ul>
<li>Remember the structure of the loops implemented for the Exercise <em>Nested loops</em> in <a href="programming_primers.html" class="quarto-xref"><span>Chapter 3</span></a>.</li>
<li>It may be helpful to write a “pseudo-code” that defines the structure of loops and conditionals and defines the points at which certain evaluations and function calls are performed, but cannot be actually run.</li>
<li><a href="https://stackoverflow.com/questions/4951442/formula-with-dynamic-number-of-variables">This Stackoverflow post</a> may be helpful for dynamically creating formulas, given a character vector or variable names,</li>
<li>When discussing results, consider why a certain variable was not chosen in the final model. How do non-included variables relate to variables that are included? How do the model metrics change when adding variables? Can you explain their patterns with your statistical knowledge?</li>
<li>To better discuss your results, you might want to look up what each variable stands for. More information can be found <a href="https://fluxnet.org/data/fluxnet2015-dataset/fullset-data-product/">here</a>.</li>
<li>Chose an appropriate visualisation for communicating results.</li>
<li>If you hand in a minimum reproducible example of an unsuccessful implementation of stepwise regression, follow <a href="https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example">this guideline</a>. Write a minimum reproducible example as if you were addressing an online audience; describe in detail what you goal is, where you got stuck, what the error message is, provide a code example that is runnable without needing any local files.</li>
<li>Code errors can break knitting your RMarkdown to HTML. To prevent an entire RMarkdown from failing to render because of erroneous code, you can set the respective code chunk as <code>error = TRUE</code> to avoid render failure while including the error message in the output, or <code>eval = FALSE</code> to avoid running the chunk altogether. More can be found <a href="https://bookdown.org/yihui/rmarkdown-cookbook/opts-error.html">here</a>.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./code_management.html" class="pagination-link" aria-label="Code management">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Code management</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./supervised_ml_I.html" class="pagination-link" aria-label="Supervised machine learning I">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised machine learning I</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>